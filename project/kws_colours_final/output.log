Configuring device: MAX78000, simulate=False.
Log file for this run: /mnt/c/Users/Daniel/Github/ai8x-training/logs/2024.01.15-120305/2024.01.15-120305.log
{'start_epoch': 10, 'weight_bits': 8}
Optimizer Type: <class 'torch.optim.adam.Adam'>
Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False}
No key `stretch` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]

Processing train...
train set: 105843 elements
validation set: 5136 elements
Class red (# 25): 2793 elements
Class green (# 13): 2697 elements
Class blue (# 3): 3054 elements
Class on (# 23): 8058 elements
Class off (# 22): 8031 elements
Class go (# 12): 8097 elements
Class stop (# 30): 8022 elements
Class UNKNOWN: 70227 elements

Processing test...
test set: 5631 elements
Class red (# 25): 300 elements
Class green (# 13): 300 elements
Class blue (# 3): 300 elements
Class on (# 23): 942 elements
Class off (# 22): 969 elements
Class go (# 12): 903 elements
Class stop (# 30): 978 elements
Class UNKNOWN: 939 elements
Dataset sizes:
	training=105843
	validation=5136
	test=5631
Reading compression schedule from: policies/schedule_kws20-200ep.yaml


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [0][   10/  207]    Overall Loss 2.062907    Objective Loss 2.062907                                        LR 0.001000    Time 0.508623    
Epoch: [0][   20/  207]    Overall Loss 2.015338    Objective Loss 2.015338                                        LR 0.001000    Time 0.280817    
Epoch: [0][   30/  207]    Overall Loss 1.979691    Objective Loss 1.979691                                        LR 0.001000    Time 0.205107    
Epoch: [0][   40/  207]    Overall Loss 1.955405    Objective Loss 1.955405                                        LR 0.001000    Time 0.167439    
Epoch: [0][   50/  207]    Overall Loss 1.928042    Objective Loss 1.928042                                        LR 0.001000    Time 0.144885    
Epoch: [0][   60/  207]    Overall Loss 1.905567    Objective Loss 1.905567                                        LR 0.001000    Time 0.130310    
Epoch: [0][   70/  207]    Overall Loss 1.881534    Objective Loss 1.881534                                        LR 0.001000    Time 0.120247    
Epoch: [0][   80/  207]    Overall Loss 1.836626    Objective Loss 1.836626                                        LR 0.001000    Time 0.112686    
Epoch: [0][   90/  207]    Overall Loss 1.798299    Objective Loss 1.798299                                        LR 0.001000    Time 0.106168    
Epoch: [0][  100/  207]    Overall Loss 1.764576    Objective Loss 1.764576                                        LR 0.001000    Time 0.100858    
Epoch: [0][  110/  207]    Overall Loss 1.735279    Objective Loss 1.735279                                        LR 0.001000    Time 0.096636    
Epoch: [0][  120/  207]    Overall Loss 1.705176    Objective Loss 1.705176                                        LR 0.001000    Time 0.093627    
Epoch: [0][  130/  207]    Overall Loss 1.677600    Objective Loss 1.677600                                        LR 0.001000    Time 0.091439    
Epoch: [0][  140/  207]    Overall Loss 1.652258    Objective Loss 1.652258                                        LR 0.001000    Time 0.089134    
Epoch: [0][  150/  207]    Overall Loss 1.629160    Objective Loss 1.629160                                        LR 0.001000    Time 0.086768    
Epoch: [0][  160/  207]    Overall Loss 1.605695    Objective Loss 1.605695                                        LR 0.001000    Time 0.084681    
Epoch: [0][  170/  207]    Overall Loss 1.584945    Objective Loss 1.584945                                        LR 0.001000    Time 0.083086    
Epoch: [0][  180/  207]    Overall Loss 1.562800    Objective Loss 1.562800                                        LR 0.001000    Time 0.081481    
Epoch: [0][  190/  207]    Overall Loss 1.543104    Objective Loss 1.543104                                        LR 0.001000    Time 0.080057    
Epoch: [0][  200/  207]    Overall Loss 1.523060    Objective Loss 1.523060                                        LR 0.001000    Time 0.078948    
Epoch: [0][  207/  207]    Overall Loss 1.510019    Objective Loss 1.510019    Top1 67.497169    Top5 95.356738    LR 0.001000    Time 0.078376    
--- validate (epoch=0)-----------
5136 samples (512 per mini-batch)
Epoch: [0][   10/   11]    Loss 1.149724    Top1 45.917969    Top5 91.933594    
Epoch: [0][   11/   11]    Loss 1.159708    Top1 45.872274    Top5 91.939252    
==> Top1: 45.872    Top5: 91.939    Loss: 1.160

==> Confusion:
[[207  22   6   4   3  31   4  23]
 [ 22 145 111   2   0   2   0  18]
 [  8 112 164   1   0   4   0  11]
 [ 49   1  10 571 110  18  58  20]
 [ 48   1   0  74 515  20 193  28]
 [258  29  27  55  40 298 151  36]
 [ 68   1   0  19 225  77 420  27]
 [180  45  47 111  71 180 119  36]]

==> Best [Top1: 45.872   Top5: 91.939   Sparsity:0.00   Params: 117200 on epoch: 0]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [1][   10/  207]    Overall Loss 1.095002    Objective Loss 1.095002                                        LR 0.001000    Time 0.120469    
Epoch: [1][   20/  207]    Overall Loss 1.100280    Objective Loss 1.100280                                        LR 0.001000    Time 0.089967    
Epoch: [1][   30/  207]    Overall Loss 1.081595    Objective Loss 1.081595                                        LR 0.001000    Time 0.079360    
Epoch: [1][   40/  207]    Overall Loss 1.084887    Objective Loss 1.084887                                        LR 0.001000    Time 0.074399    
Epoch: [1][   50/  207]    Overall Loss 1.084705    Objective Loss 1.084705                                        LR 0.001000    Time 0.071110    
Epoch: [1][   60/  207]    Overall Loss 1.074958    Objective Loss 1.074958                                        LR 0.001000    Time 0.068984    
Epoch: [1][   70/  207]    Overall Loss 1.063846    Objective Loss 1.063846                                        LR 0.001000    Time 0.067218    
Epoch: [1][   80/  207]    Overall Loss 1.058675    Objective Loss 1.058675                                        LR 0.001000    Time 0.065814    
Epoch: [1][   90/  207]    Overall Loss 1.055306    Objective Loss 1.055306                                        LR 0.001000    Time 0.065111    
Epoch: [1][  100/  207]    Overall Loss 1.050745    Objective Loss 1.050745                                        LR 0.001000    Time 0.064293    
Epoch: [1][  110/  207]    Overall Loss 1.045614    Objective Loss 1.045614                                        LR 0.001000    Time 0.063308    
Epoch: [1][  120/  207]    Overall Loss 1.042715    Objective Loss 1.042715                                        LR 0.001000    Time 0.062661    
Epoch: [1][  130/  207]    Overall Loss 1.043102    Objective Loss 1.043102                                        LR 0.001000    Time 0.062134    
Epoch: [1][  140/  207]    Overall Loss 1.035848    Objective Loss 1.035848                                        LR 0.001000    Time 0.061540    
Epoch: [1][  150/  207]    Overall Loss 1.032471    Objective Loss 1.032471                                        LR 0.001000    Time 0.060984    
Epoch: [1][  160/  207]    Overall Loss 1.029049    Objective Loss 1.029049                                        LR 0.001000    Time 0.060503    
Epoch: [1][  170/  207]    Overall Loss 1.024437    Objective Loss 1.024437                                        LR 0.001000    Time 0.060063    
Epoch: [1][  180/  207]    Overall Loss 1.018875    Objective Loss 1.018875                                        LR 0.001000    Time 0.059816    
Epoch: [1][  190/  207]    Overall Loss 1.011931    Objective Loss 1.011931                                        LR 0.001000    Time 0.059575    
Epoch: [1][  200/  207]    Overall Loss 1.004825    Objective Loss 1.004825                                        LR 0.001000    Time 0.059435    
Epoch: [1][  207/  207]    Overall Loss 1.001541    Objective Loss 1.001541    Top1 70.554926    Top5 97.508494    LR 0.001000    Time 0.059236    
--- validate (epoch=1)-----------
5136 samples (512 per mini-batch)
Epoch: [1][   10/   11]    Loss 0.928183    Top1 56.035156    Top5 96.054688    
Epoch: [1][   11/   11]    Loss 0.964999    Top1 56.035826    Top5 96.066978    
==> Top1: 56.036    Top5: 96.067    Loss: 0.965

==> Confusion:
[[184  18  27   6   1  36   6  22]
 [  3 137 153   2   0   0   0   5]
 [  4  50 233   1   0   3   0   9]
 [ 11  19   3 677  92  13  13   9]
 [ 17   5   0 136 622  29  50  20]
 [222  23  48  54  34 415  87  11]
 [ 38   9   1   7  78  91 584  29]
 [136  42  71 141  78 227  68  26]]

==> Best [Top1: 56.036   Top5: 96.067   Sparsity:0.00   Params: 117200 on epoch: 1]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [2][   10/  207]    Overall Loss 0.882580    Objective Loss 0.882580                                        LR 0.001000    Time 0.112162    
Epoch: [2][   20/  207]    Overall Loss 0.887844    Objective Loss 0.887844                                        LR 0.001000    Time 0.083328    
Epoch: [2][   30/  207]    Overall Loss 0.888676    Objective Loss 0.888676                                        LR 0.001000    Time 0.074923    
Epoch: [2][   40/  207]    Overall Loss 0.885321    Objective Loss 0.885321                                        LR 0.001000    Time 0.069562    
Epoch: [2][   50/  207]    Overall Loss 0.887627    Objective Loss 0.887627                                        LR 0.001000    Time 0.066304    
Epoch: [2][   60/  207]    Overall Loss 0.887162    Objective Loss 0.887162                                        LR 0.001000    Time 0.064077    
Epoch: [2][   70/  207]    Overall Loss 0.899714    Objective Loss 0.899714                                        LR 0.001000    Time 0.062477    
Epoch: [2][   80/  207]    Overall Loss 0.896822    Objective Loss 0.896822                                        LR 0.001000    Time 0.061347    
Epoch: [2][   90/  207]    Overall Loss 0.893529    Objective Loss 0.893529                                        LR 0.001000    Time 0.060769    
Epoch: [2][  100/  207]    Overall Loss 0.887986    Objective Loss 0.887986                                        LR 0.001000    Time 0.060052    
Epoch: [2][  110/  207]    Overall Loss 0.883603    Objective Loss 0.883603                                        LR 0.001000    Time 0.059414    
Epoch: [2][  120/  207]    Overall Loss 0.882189    Objective Loss 0.882189                                        LR 0.001000    Time 0.058956    
Epoch: [2][  130/  207]    Overall Loss 0.878551    Objective Loss 0.878551                                        LR 0.001000    Time 0.058493    
Epoch: [2][  140/  207]    Overall Loss 0.869933    Objective Loss 0.869933                                        LR 0.001000    Time 0.058111    
Epoch: [2][  150/  207]    Overall Loss 0.867528    Objective Loss 0.867528                                        LR 0.001000    Time 0.057995    
Epoch: [2][  160/  207]    Overall Loss 0.862959    Objective Loss 0.862959                                        LR 0.001000    Time 0.057725    
Epoch: [2][  170/  207]    Overall Loss 0.861135    Objective Loss 0.861135                                        LR 0.001000    Time 0.057522    
Epoch: [2][  180/  207]    Overall Loss 0.857726    Objective Loss 0.857726                                        LR 0.001000    Time 0.057418    
Epoch: [2][  190/  207]    Overall Loss 0.855393    Objective Loss 0.855393                                        LR 0.001000    Time 0.057221    
Epoch: [2][  200/  207]    Overall Loss 0.850747    Objective Loss 0.850747                                        LR 0.001000    Time 0.057075    
Epoch: [2][  207/  207]    Overall Loss 0.849890    Objective Loss 0.849890    Top1 72.366931    Top5 98.640997    LR 0.001000    Time 0.057080    
--- validate (epoch=2)-----------
5136 samples (512 per mini-batch)
Epoch: [2][   10/   11]    Loss 0.799339    Top1 60.605469    Top5 97.441406    
Epoch: [2][   11/   11]    Loss 0.809434    Top1 60.514019    Top5 97.449377    
==> Top1: 60.514    Top5: 97.449    Loss: 0.809

==> Confusion:
[[216   7  10   1   0  41   5  20]
 [ 18 222  49   2   0   4   0   5]
 [  8  96 181   2   0   5   0   8]
 [ 12   3   0 643 126  29  21   3]
 [ 13   1   0 102 676  24  47  16]
 [218  11  20  30  20 520  66   9]
 [ 37   2   0   2  69  91 610  26]
 [155  33  38 108  83 261  71  40]]

==> Best [Top1: 60.514   Top5: 97.449   Sparsity:0.00   Params: 117200 on epoch: 2]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [3][   10/  207]    Overall Loss 0.760894    Objective Loss 0.760894                                        LR 0.001000    Time 0.111715    
Epoch: [3][   20/  207]    Overall Loss 0.767912    Objective Loss 0.767912                                        LR 0.001000    Time 0.084731    
Epoch: [3][   30/  207]    Overall Loss 0.750903    Objective Loss 0.750903                                        LR 0.001000    Time 0.075061    
Epoch: [3][   40/  207]    Overall Loss 0.747191    Objective Loss 0.747191                                        LR 0.001000    Time 0.069566    
Epoch: [3][   50/  207]    Overall Loss 0.749942    Objective Loss 0.749942                                        LR 0.001000    Time 0.066245    
Epoch: [3][   60/  207]    Overall Loss 0.753041    Objective Loss 0.753041                                        LR 0.001000    Time 0.064110    
Epoch: [3][   70/  207]    Overall Loss 0.753713    Objective Loss 0.753713                                        LR 0.001000    Time 0.062660    
Epoch: [3][   80/  207]    Overall Loss 0.749954    Objective Loss 0.749954                                        LR 0.001000    Time 0.061467    
Epoch: [3][   90/  207]    Overall Loss 0.746156    Objective Loss 0.746156                                        LR 0.001000    Time 0.060552    
Epoch: [3][  100/  207]    Overall Loss 0.743590    Objective Loss 0.743590                                        LR 0.001000    Time 0.059860    
Epoch: [3][  110/  207]    Overall Loss 0.741988    Objective Loss 0.741988                                        LR 0.001000    Time 0.059929    
Epoch: [3][  120/  207]    Overall Loss 0.740738    Objective Loss 0.740738                                        LR 0.001000    Time 0.059776    
Epoch: [3][  130/  207]    Overall Loss 0.738235    Objective Loss 0.738235                                        LR 0.001000    Time 0.059593    
Epoch: [3][  140/  207]    Overall Loss 0.736187    Objective Loss 0.736187                                        LR 0.001000    Time 0.059543    
Epoch: [3][  150/  207]    Overall Loss 0.732473    Objective Loss 0.732473                                        LR 0.001000    Time 0.059173    
Epoch: [3][  160/  207]    Overall Loss 0.730171    Objective Loss 0.730171                                        LR 0.001000    Time 0.059293    
Epoch: [3][  170/  207]    Overall Loss 0.728998    Objective Loss 0.728998                                        LR 0.001000    Time 0.058993    
Epoch: [3][  180/  207]    Overall Loss 0.726429    Objective Loss 0.726429                                        LR 0.001000    Time 0.058666    
Epoch: [3][  190/  207]    Overall Loss 0.725701    Objective Loss 0.725701                                        LR 0.001000    Time 0.058477    
Epoch: [3][  200/  207]    Overall Loss 0.723667    Objective Loss 0.723667                                        LR 0.001000    Time 0.058280    
Epoch: [3][  207/  207]    Overall Loss 0.723561    Objective Loss 0.723561    Top1 73.952435    Top5 98.867497    LR 0.001000    Time 0.058102    
--- validate (epoch=3)-----------
5136 samples (512 per mini-batch)
Epoch: [3][   10/   11]    Loss 0.754838    Top1 61.210937    Top5 97.792969    
Epoch: [3][   11/   11]    Loss 0.780923    Top1 61.176012    Top5 97.799844    
==> Top1: 61.176    Top5: 97.800    Loss: 0.781

==> Confusion:
[[232  11  28   0   0  12   3  14]
 [  6 212  77   0   0   1   0   4]
 [  5  53 237   0   0   1   0   4]
 [ 15  41   1 610 112  28  23   7]
 [ 29   6   0  68 683  20  56  17]
 [276  18  48  15  19 458  53   7]
 [ 51   8   1   2  35  45 672  23]
 [212  59  70  68  68 188  86  38]]

==> Best [Top1: 61.176   Top5: 97.800   Sparsity:0.00   Params: 117200 on epoch: 3]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [4][   10/  207]    Overall Loss 0.708475    Objective Loss 0.708475                                        LR 0.001000    Time 0.094020    
Epoch: [4][   20/  207]    Overall Loss 0.696723    Objective Loss 0.696723                                        LR 0.001000    Time 0.074190    
Epoch: [4][   30/  207]    Overall Loss 0.682257    Objective Loss 0.682257                                        LR 0.001000    Time 0.068564    
Epoch: [4][   40/  207]    Overall Loss 0.670678    Objective Loss 0.670678                                        LR 0.001000    Time 0.066092    
Epoch: [4][   50/  207]    Overall Loss 0.672859    Objective Loss 0.672859                                        LR 0.001000    Time 0.064567    
Epoch: [4][   60/  207]    Overall Loss 0.671930    Objective Loss 0.671930                                        LR 0.001000    Time 0.063570    
Epoch: [4][   70/  207]    Overall Loss 0.668391    Objective Loss 0.668391                                        LR 0.001000    Time 0.062628    
Epoch: [4][   80/  207]    Overall Loss 0.663140    Objective Loss 0.663140                                        LR 0.001000    Time 0.062166    
Epoch: [4][   90/  207]    Overall Loss 0.659553    Objective Loss 0.659553                                        LR 0.001000    Time 0.061693    
Epoch: [4][  100/  207]    Overall Loss 0.658142    Objective Loss 0.658142                                        LR 0.001000    Time 0.061248    
Epoch: [4][  110/  207]    Overall Loss 0.652568    Objective Loss 0.652568                                        LR 0.001000    Time 0.060585    
Epoch: [4][  120/  207]    Overall Loss 0.650264    Objective Loss 0.650264                                        LR 0.001000    Time 0.060027    
Epoch: [4][  130/  207]    Overall Loss 0.649348    Objective Loss 0.649348                                        LR 0.001000    Time 0.059548    
Epoch: [4][  140/  207]    Overall Loss 0.650984    Objective Loss 0.650984                                        LR 0.001000    Time 0.059173    
Epoch: [4][  150/  207]    Overall Loss 0.650503    Objective Loss 0.650503                                        LR 0.001000    Time 0.059392    
Epoch: [4][  160/  207]    Overall Loss 0.648637    Objective Loss 0.648637                                        LR 0.001000    Time 0.059556    
Epoch: [4][  170/  207]    Overall Loss 0.646263    Objective Loss 0.646263                                        LR 0.001000    Time 0.059849    
Epoch: [4][  180/  207]    Overall Loss 0.644058    Objective Loss 0.644058                                        LR 0.001000    Time 0.060364    
Epoch: [4][  190/  207]    Overall Loss 0.642213    Objective Loss 0.642213                                        LR 0.001000    Time 0.060563    
Epoch: [4][  200/  207]    Overall Loss 0.640144    Objective Loss 0.640144                                        LR 0.001000    Time 0.060653    
Epoch: [4][  207/  207]    Overall Loss 0.640292    Objective Loss 0.640292    Top1 77.576444    Top5 99.207248    LR 0.001000    Time 0.060905    
--- validate (epoch=4)-----------
5136 samples (512 per mini-batch)
Epoch: [4][   10/   11]    Loss 0.655891    Top1 66.855469    Top5 98.515625    
Epoch: [4][   11/   11]    Loss 0.653542    Top1 66.822430    Top5 98.520249    
==> Top1: 66.822    Top5: 98.520    Loss: 0.654

==> Confusion:
[[256   9  14   0   0  12   0   9]
 [  9 258  31   0   0   1   0   1]
 [  7  81 206   0   0   2   0   4]
 [ 12   6   0 657  92  28  29  13]
 [ 10   1   0  68 638  21 117  24]
 [135  14  31  18  16 614  60   6]
 [ 22   3   0   1   7  37 746  21]
 [153  45  60  64  52 262  96  57]]

==> Best [Top1: 66.822   Top5: 98.520   Sparsity:0.00   Params: 117200 on epoch: 4]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [5][   10/  207]    Overall Loss 0.558835    Objective Loss 0.558835                                        LR 0.001000    Time 0.115370    
Epoch: [5][   20/  207]    Overall Loss 0.571810    Objective Loss 0.571810                                        LR 0.001000    Time 0.084667    
Epoch: [5][   30/  207]    Overall Loss 0.570465    Objective Loss 0.570465                                        LR 0.001000    Time 0.075675    
Epoch: [5][   40/  207]    Overall Loss 0.566648    Objective Loss 0.566648                                        LR 0.001000    Time 0.072606    
Epoch: [5][   50/  207]    Overall Loss 0.574750    Objective Loss 0.574750                                        LR 0.001000    Time 0.068715    
Epoch: [5][   60/  207]    Overall Loss 0.573853    Objective Loss 0.573853                                        LR 0.001000    Time 0.066292    
Epoch: [5][   70/  207]    Overall Loss 0.569741    Objective Loss 0.569741                                        LR 0.001000    Time 0.064420    
Epoch: [5][   80/  207]    Overall Loss 0.566788    Objective Loss 0.566788                                        LR 0.001000    Time 0.063860    
Epoch: [5][   90/  207]    Overall Loss 0.564430    Objective Loss 0.564430                                        LR 0.001000    Time 0.063263    
Epoch: [5][  100/  207]    Overall Loss 0.562458    Objective Loss 0.562458                                        LR 0.001000    Time 0.062510    
Epoch: [5][  110/  207]    Overall Loss 0.564709    Objective Loss 0.564709                                        LR 0.001000    Time 0.061680    
Epoch: [5][  120/  207]    Overall Loss 0.566979    Objective Loss 0.566979                                        LR 0.001000    Time 0.060987    
Epoch: [5][  130/  207]    Overall Loss 0.565712    Objective Loss 0.565712                                        LR 0.001000    Time 0.060453    
Epoch: [5][  140/  207]    Overall Loss 0.562480    Objective Loss 0.562480                                        LR 0.001000    Time 0.060047    
Epoch: [5][  150/  207]    Overall Loss 0.561010    Objective Loss 0.561010                                        LR 0.001000    Time 0.059620    
Epoch: [5][  160/  207]    Overall Loss 0.559413    Objective Loss 0.559413                                        LR 0.001000    Time 0.059537    
Epoch: [5][  170/  207]    Overall Loss 0.560877    Objective Loss 0.560877                                        LR 0.001000    Time 0.059441    
Epoch: [5][  180/  207]    Overall Loss 0.562014    Objective Loss 0.562014                                        LR 0.001000    Time 0.059290    
Epoch: [5][  190/  207]    Overall Loss 0.561473    Objective Loss 0.561473                                        LR 0.001000    Time 0.059443    
Epoch: [5][  200/  207]    Overall Loss 0.560818    Objective Loss 0.560818                                        LR 0.001000    Time 0.059472    
Epoch: [5][  207/  207]    Overall Loss 0.560779    Objective Loss 0.560779    Top1 76.783692    Top5 99.207248    LR 0.001000    Time 0.059541    
--- validate (epoch=5)-----------
5136 samples (512 per mini-batch)
Epoch: [5][   10/   11]    Loss 0.567648    Top1 71.386719    Top5 98.886719    
Epoch: [5][   11/   11]    Loss 0.565175    Top1 71.339564    Top5 98.890187    
==> Top1: 71.340    Top5: 98.890    Loss: 0.565

==> Confusion:
[[252   5   7   1   0  22   0  13]
 [ 11 209  68   3   0   5   0   4]
 [ 10  35 243   1   0   7   0   4]
 [  7   2   0 690 102  17  17   2]
 [  4   0   0  60 757   7  32  19]
 [ 31   6  19  23  36 723  51   5]
 [  7   0   1   5  41  40 727  16]
 [120  21  49 110  86 266  74  63]]

==> Best [Top1: 71.340   Top5: 98.890   Sparsity:0.00   Params: 117200 on epoch: 5]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [6][   10/  207]    Overall Loss 0.499608    Objective Loss 0.499608                                        LR 0.001000    Time 0.112838    
Epoch: [6][   20/  207]    Overall Loss 0.506583    Objective Loss 0.506583                                        LR 0.001000    Time 0.083552    
Epoch: [6][   30/  207]    Overall Loss 0.506598    Objective Loss 0.506598                                        LR 0.001000    Time 0.074060    
Epoch: [6][   40/  207]    Overall Loss 0.514299    Objective Loss 0.514299                                        LR 0.001000    Time 0.068874    
Epoch: [6][   50/  207]    Overall Loss 0.515021    Objective Loss 0.515021                                        LR 0.001000    Time 0.066324    
Epoch: [6][   60/  207]    Overall Loss 0.508042    Objective Loss 0.508042                                        LR 0.001000    Time 0.064899    
Epoch: [6][   70/  207]    Overall Loss 0.503883    Objective Loss 0.503883                                        LR 0.001000    Time 0.064683    
Epoch: [6][   80/  207]    Overall Loss 0.500005    Objective Loss 0.500005                                        LR 0.001000    Time 0.063608    
Epoch: [6][   90/  207]    Overall Loss 0.502114    Objective Loss 0.502114                                        LR 0.001000    Time 0.062885    
Epoch: [6][  100/  207]    Overall Loss 0.503095    Objective Loss 0.503095                                        LR 0.001000    Time 0.062164    
Epoch: [6][  110/  207]    Overall Loss 0.505232    Objective Loss 0.505232                                        LR 0.001000    Time 0.061405    
Epoch: [6][  120/  207]    Overall Loss 0.503871    Objective Loss 0.503871                                        LR 0.001000    Time 0.060743    
Epoch: [6][  130/  207]    Overall Loss 0.502480    Objective Loss 0.502480                                        LR 0.001000    Time 0.060199    
Epoch: [6][  140/  207]    Overall Loss 0.501916    Objective Loss 0.501916                                        LR 0.001000    Time 0.060008    
Epoch: [6][  150/  207]    Overall Loss 0.502403    Objective Loss 0.502403                                        LR 0.001000    Time 0.060045    
Epoch: [6][  160/  207]    Overall Loss 0.504187    Objective Loss 0.504187                                        LR 0.001000    Time 0.059714    
Epoch: [6][  170/  207]    Overall Loss 0.503758    Objective Loss 0.503758                                        LR 0.001000    Time 0.059354    
Epoch: [6][  180/  207]    Overall Loss 0.504877    Objective Loss 0.504877                                        LR 0.001000    Time 0.059041    
Epoch: [6][  190/  207]    Overall Loss 0.504263    Objective Loss 0.504263                                        LR 0.001000    Time 0.058743    
Epoch: [6][  200/  207]    Overall Loss 0.501637    Objective Loss 0.501637                                        LR 0.001000    Time 0.058529    
Epoch: [6][  207/  207]    Overall Loss 0.501260    Objective Loss 0.501260    Top1 80.634202    Top5 99.320498    LR 0.001000    Time 0.058399    
--- validate (epoch=6)-----------
5136 samples (512 per mini-batch)
Epoch: [6][   10/   11]    Loss 0.601558    Top1 69.765625    Top5 99.160156    
Epoch: [6][   11/   11]    Loss 0.626562    Top1 69.762461    Top5 99.143302    
==> Top1: 69.762    Top5: 99.143    Loss: 0.627

==> Confusion:
[[263   3   8   0   0  11   1  14]
 [ 16 181  99   0   0   1   0   3]
 [ 14  21 258   0   0   3   0   4]
 [ 12  11   0 642 116  17  23  16]
 [  4   0   0  37 752  12  49  25]
 [ 62   5  36  13  28 661  76  13]
 [ 10   1   0   1  40  17 745  23]
 [162  40  59  60  82 207  98  81]]

==> Best [Top1: 71.340   Top5: 98.890   Sparsity:0.00   Params: 117200 on epoch: 5]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [7][   10/  207]    Overall Loss 0.485999    Objective Loss 0.485999                                        LR 0.001000    Time 0.113131    
Epoch: [7][   20/  207]    Overall Loss 0.479306    Objective Loss 0.479306                                        LR 0.001000    Time 0.083026    
Epoch: [7][   30/  207]    Overall Loss 0.473725    Objective Loss 0.473725                                        LR 0.001000    Time 0.073262    
Epoch: [7][   40/  207]    Overall Loss 0.469711    Objective Loss 0.469711                                        LR 0.001000    Time 0.068195    
Epoch: [7][   50/  207]    Overall Loss 0.469584    Objective Loss 0.469584                                        LR 0.001000    Time 0.065244    
Epoch: [7][   60/  207]    Overall Loss 0.467814    Objective Loss 0.467814                                        LR 0.001000    Time 0.064147    
Epoch: [7][   70/  207]    Overall Loss 0.464785    Objective Loss 0.464785                                        LR 0.001000    Time 0.063099    
Epoch: [7][   80/  207]    Overall Loss 0.458014    Objective Loss 0.458014                                        LR 0.001000    Time 0.061885    
Epoch: [7][   90/  207]    Overall Loss 0.459108    Objective Loss 0.459108                                        LR 0.001000    Time 0.061148    
Epoch: [7][  100/  207]    Overall Loss 0.458509    Objective Loss 0.458509                                        LR 0.001000    Time 0.060484    
Epoch: [7][  110/  207]    Overall Loss 0.461050    Objective Loss 0.461050                                        LR 0.001000    Time 0.059907    
Epoch: [7][  120/  207]    Overall Loss 0.461410    Objective Loss 0.461410                                        LR 0.001000    Time 0.059503    
Epoch: [7][  130/  207]    Overall Loss 0.460104    Objective Loss 0.460104                                        LR 0.001000    Time 0.059069    
Epoch: [7][  140/  207]    Overall Loss 0.458432    Objective Loss 0.458432                                        LR 0.001000    Time 0.059095    
Epoch: [7][  150/  207]    Overall Loss 0.458301    Objective Loss 0.458301                                        LR 0.001000    Time 0.058762    
Epoch: [7][  160/  207]    Overall Loss 0.457437    Objective Loss 0.457437                                        LR 0.001000    Time 0.058711    
Epoch: [7][  170/  207]    Overall Loss 0.458804    Objective Loss 0.458804                                        LR 0.001000    Time 0.058470    
Epoch: [7][  180/  207]    Overall Loss 0.458849    Objective Loss 0.458849                                        LR 0.001000    Time 0.058178    
Epoch: [7][  190/  207]    Overall Loss 0.458742    Objective Loss 0.458742                                        LR 0.001000    Time 0.057926    
Epoch: [7][  200/  207]    Overall Loss 0.459179    Objective Loss 0.459179                                        LR 0.001000    Time 0.057711    
Epoch: [7][  207/  207]    Overall Loss 0.457913    Objective Loss 0.457913    Top1 84.711212    Top5 99.660249    LR 0.001000    Time 0.057557    
--- validate (epoch=7)-----------
5136 samples (512 per mini-batch)
Epoch: [7][   10/   11]    Loss 0.577210    Top1 71.464844    Top5 99.335938    
Epoch: [7][   11/   11]    Loss 0.609254    Top1 71.436916    Top5 99.338006    
==> Top1: 71.437    Top5: 99.338    Loss: 0.609

==> Confusion:
[[258  12   1   0   1   7   1  20]
 [  7 283   5   0   0   2   1   2]
 [ 15 108 168   0   0   2   0   7]
 [  4   5   0 691  93  12  18  14]
 [  4   0   0  58 736   9  53  19]
 [ 42  19  10  19  28 660 100  16]
 [  5   1   0   4  20  11 777  19]
 [118  70  16  96  72 206 115  96]]

==> Best [Top1: 71.437   Top5: 99.338   Sparsity:0.00   Params: 117200 on epoch: 7]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [8][   10/  207]    Overall Loss 0.423078    Objective Loss 0.423078                                        LR 0.001000    Time 0.088837    
Epoch: [8][   20/  207]    Overall Loss 0.427428    Objective Loss 0.427428                                        LR 0.001000    Time 0.071045    
Epoch: [8][   30/  207]    Overall Loss 0.432186    Objective Loss 0.432186                                        LR 0.001000    Time 0.065134    
Epoch: [8][   40/  207]    Overall Loss 0.422152    Objective Loss 0.422152                                        LR 0.001000    Time 0.062186    
Epoch: [8][   50/  207]    Overall Loss 0.426607    Objective Loss 0.426607                                        LR 0.001000    Time 0.060391    
Epoch: [8][   60/  207]    Overall Loss 0.426151    Objective Loss 0.426151                                        LR 0.001000    Time 0.059271    
Epoch: [8][   70/  207]    Overall Loss 0.423692    Objective Loss 0.423692                                        LR 0.001000    Time 0.058399    
Epoch: [8][   80/  207]    Overall Loss 0.422953    Objective Loss 0.422953                                        LR 0.001000    Time 0.057754    
Epoch: [8][   90/  207]    Overall Loss 0.423350    Objective Loss 0.423350                                        LR 0.001000    Time 0.057263    
Epoch: [8][  100/  207]    Overall Loss 0.426327    Objective Loss 0.426327                                        LR 0.001000    Time 0.056877    
Epoch: [8][  110/  207]    Overall Loss 0.425797    Objective Loss 0.425797                                        LR 0.001000    Time 0.056572    
Epoch: [8][  120/  207]    Overall Loss 0.427662    Objective Loss 0.427662                                        LR 0.001000    Time 0.056286    
Epoch: [8][  130/  207]    Overall Loss 0.428114    Objective Loss 0.428114                                        LR 0.001000    Time 0.056086    
Epoch: [8][  140/  207]    Overall Loss 0.431805    Objective Loss 0.431805                                        LR 0.001000    Time 0.055870    
Epoch: [8][  150/  207]    Overall Loss 0.431570    Objective Loss 0.431570                                        LR 0.001000    Time 0.055719    
Epoch: [8][  160/  207]    Overall Loss 0.430359    Objective Loss 0.430359                                        LR 0.001000    Time 0.055564    
Epoch: [8][  170/  207]    Overall Loss 0.430006    Objective Loss 0.430006                                        LR 0.001000    Time 0.055421    
Epoch: [8][  180/  207]    Overall Loss 0.428745    Objective Loss 0.428745                                        LR 0.001000    Time 0.055295    
Epoch: [8][  190/  207]    Overall Loss 0.429447    Objective Loss 0.429447                                        LR 0.001000    Time 0.055190    
Epoch: [8][  200/  207]    Overall Loss 0.428113    Objective Loss 0.428113                                        LR 0.001000    Time 0.055103    
Epoch: [8][  207/  207]    Overall Loss 0.426115    Objective Loss 0.426115    Top1 86.409966    Top5 99.660249    LR 0.001000    Time 0.055038    
--- validate (epoch=8)-----------
5136 samples (512 per mini-batch)
Epoch: [8][   10/   11]    Loss 0.508816    Top1 73.066406    Top5 99.472656    
Epoch: [8][   11/   11]    Loss 0.510628    Top1 73.072430    Top5 99.474299    
==> Top1: 73.072    Top5: 99.474    Loss: 0.511

==> Confusion:
[[252   6   4   2   1  15   1  19]
 [ 10 264  16   1   0   2   0   7]
 [  2  69 214   0   0   9   0   6]
 [  2   5   0 684  98  19  25   4]
 [  0   0   0  51 738  10  62  18]
 [ 17   8  11  16  25 729  80   8]
 [  3   0   0   4  25   8 781  16]
 [ 89  53  27  92  71 257 109  91]]

==> Best [Top1: 73.072   Top5: 99.474   Sparsity:0.00   Params: 117200 on epoch: 8]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [9][   10/  207]    Overall Loss 0.395679    Objective Loss 0.395679                                        LR 0.001000    Time 0.105918    
Epoch: [9][   20/  207]    Overall Loss 0.403933    Objective Loss 0.403933                                        LR 0.001000    Time 0.079632    
Epoch: [9][   30/  207]    Overall Loss 0.405179    Objective Loss 0.405179                                        LR 0.001000    Time 0.070836    
Epoch: [9][   40/  207]    Overall Loss 0.426101    Objective Loss 0.426101                                        LR 0.001000    Time 0.066445    
Epoch: [9][   50/  207]    Overall Loss 0.431537    Objective Loss 0.431537                                        LR 0.001000    Time 0.063810    
Epoch: [9][   60/  207]    Overall Loss 0.433547    Objective Loss 0.433547                                        LR 0.001000    Time 0.062076    
Epoch: [9][   70/  207]    Overall Loss 0.430325    Objective Loss 0.430325                                        LR 0.001000    Time 0.060833    
Epoch: [9][   80/  207]    Overall Loss 0.427417    Objective Loss 0.427417                                        LR 0.001000    Time 0.059878    
Epoch: [9][   90/  207]    Overall Loss 0.425216    Objective Loss 0.425216                                        LR 0.001000    Time 0.059205    
Epoch: [9][  100/  207]    Overall Loss 0.420595    Objective Loss 0.420595                                        LR 0.001000    Time 0.058760    
Epoch: [9][  110/  207]    Overall Loss 0.420805    Objective Loss 0.420805                                        LR 0.001000    Time 0.058265    
Epoch: [9][  120/  207]    Overall Loss 0.419139    Objective Loss 0.419139                                        LR 0.001000    Time 0.057871    
Epoch: [9][  130/  207]    Overall Loss 0.415610    Objective Loss 0.415610                                        LR 0.001000    Time 0.057520    
Epoch: [9][  140/  207]    Overall Loss 0.412487    Objective Loss 0.412487                                        LR 0.001000    Time 0.057296    
Epoch: [9][  150/  207]    Overall Loss 0.409266    Objective Loss 0.409266                                        LR 0.001000    Time 0.057029    
Epoch: [9][  160/  207]    Overall Loss 0.405424    Objective Loss 0.405424                                        LR 0.001000    Time 0.056811    
Epoch: [9][  170/  207]    Overall Loss 0.405323    Objective Loss 0.405323                                        LR 0.001000    Time 0.056587    
Epoch: [9][  180/  207]    Overall Loss 0.403676    Objective Loss 0.403676                                        LR 0.001000    Time 0.056418    
Epoch: [9][  190/  207]    Overall Loss 0.403411    Objective Loss 0.403411                                        LR 0.001000    Time 0.056245    
Epoch: [9][  200/  207]    Overall Loss 0.402270    Objective Loss 0.402270                                        LR 0.001000    Time 0.056131    
Epoch: [9][  207/  207]    Overall Loss 0.405914    Objective Loss 0.405914    Top1 79.501699    Top5 99.207248    LR 0.001000    Time 0.056031    
--- validate (epoch=9)-----------
5136 samples (512 per mini-batch)
Epoch: [9][   10/   11]    Loss 0.499415    Top1 72.949219    Top5 99.160156    
Epoch: [9][   11/   11]    Loss 0.469050    Top1 72.955607    Top5 99.123832    
==> Top1: 72.956    Top5: 99.124    Loss: 0.469

==> Confusion:
[[244   9  12   1   1  26   0   7]
 [  3 232  61   0   0   3   0   1]
 [  1  27 264   1   0   6   0   1]
 [  1   5   0 724  88   9  10   0]
 [  0   0   0  80 772  12  11   4]
 [  7   8   9  36  34 775  21   4]
 [  5   2   0   6  78  41 693  12]
 [ 62  51  45 144  86 308  50  43]]

==> Best [Top1: 73.072   Top5: 99.474   Sparsity:0.00   Params: 117200 on epoch: 8]
Saving checkpoint to: logs/2024.01.15-120305/checkpoint.pth.tar


Initiating quantization aware training (QAT)...


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [10][   10/  207]    Overall Loss 0.583015    Objective Loss 0.583015                                        LR 0.001000    Time 0.168376    
Epoch: [10][   20/  207]    Overall Loss 0.535738    Objective Loss 0.535738                                        LR 0.001000    Time 0.139264    
Epoch: [10][   30/  207]    Overall Loss 0.509671    Objective Loss 0.509671                                        LR 0.001000    Time 0.119183    
Epoch: [10][   40/  207]    Overall Loss 0.509086    Objective Loss 0.509086                                        LR 0.001000    Time 0.108130    
Epoch: [10][   50/  207]    Overall Loss 0.493770    Objective Loss 0.493770                                        LR 0.001000    Time 0.101990    
Epoch: [10][   60/  207]    Overall Loss 0.486876    Objective Loss 0.486876                                        LR 0.001000    Time 0.098791    
Epoch: [10][   70/  207]    Overall Loss 0.472383    Objective Loss 0.472383                                        LR 0.001000    Time 0.097840    
Epoch: [10][   80/  207]    Overall Loss 0.463120    Objective Loss 0.463120                                        LR 0.001000    Time 0.096267    
Epoch: [10][   90/  207]    Overall Loss 0.454082    Objective Loss 0.454082                                        LR 0.001000    Time 0.094296    
Epoch: [10][  100/  207]    Overall Loss 0.446907    Objective Loss 0.446907                                        LR 0.001000    Time 0.093782    
Epoch: [10][  110/  207]    Overall Loss 0.443797    Objective Loss 0.443797                                        LR 0.001000    Time 0.092229    
Epoch: [10][  120/  207]    Overall Loss 0.438885    Objective Loss 0.438885                                        LR 0.001000    Time 0.091814    
Epoch: [10][  130/  207]    Overall Loss 0.436381    Objective Loss 0.436381                                        LR 0.001000    Time 0.091182    
Epoch: [10][  140/  207]    Overall Loss 0.433594    Objective Loss 0.433594                                        LR 0.001000    Time 0.089999    
Epoch: [10][  150/  207]    Overall Loss 0.429609    Objective Loss 0.429609                                        LR 0.001000    Time 0.089131    
Epoch: [10][  160/  207]    Overall Loss 0.424178    Objective Loss 0.424178                                        LR 0.001000    Time 0.087984    
Epoch: [10][  170/  207]    Overall Loss 0.420744    Objective Loss 0.420744                                        LR 0.001000    Time 0.087233    
Epoch: [10][  180/  207]    Overall Loss 0.418554    Objective Loss 0.418554                                        LR 0.001000    Time 0.086577    
Epoch: [10][  190/  207]    Overall Loss 0.419609    Objective Loss 0.419609                                        LR 0.001000    Time 0.086040    
Epoch: [10][  200/  207]    Overall Loss 0.418263    Objective Loss 0.418263                                        LR 0.001000    Time 0.085628    
Epoch: [10][  207/  207]    Overall Loss 0.416319    Objective Loss 0.416319    Top1 87.202718    Top5 99.773499    LR 0.001000    Time 0.085046    
--- validate (epoch=10)-----------
5136 samples (512 per mini-batch)
Epoch: [10][   10/   11]    Loss 0.522980    Top1 75.117188    Top5 99.511719    
Epoch: [10][   11/   11]    Loss 0.514600    Top1 75.155763    Top5 99.513240    
==> Top1: 75.156    Top5: 99.513    Loss: 0.515

==> Confusion:
[[268   6   4   0   1   7   0  14]
 [  9 241  45   0   0   2   0   3]
 [  5  34 252   1   0   4   0   4]
 [  3   5   1 719  73  13   8  15]
 [  2   2   0  76 741  12  23  23]
 [ 34   5  13  18  26 759  18  21]
 [  6   0   1   7  40  36 715  32]
 [111  32  33  99  72 236  41 165]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [11][   10/  207]    Overall Loss 0.376898    Objective Loss 0.376898                                        LR 0.001000    Time 0.131563    
Epoch: [11][   20/  207]    Overall Loss 0.370503    Objective Loss 0.370503                                        LR 0.001000    Time 0.101501    
Epoch: [11][   30/  207]    Overall Loss 0.371471    Objective Loss 0.371471                                        LR 0.001000    Time 0.094814    
Epoch: [11][   40/  207]    Overall Loss 0.380091    Objective Loss 0.380091                                        LR 0.001000    Time 0.088554    
Epoch: [11][   50/  207]    Overall Loss 0.372491    Objective Loss 0.372491                                        LR 0.001000    Time 0.084992    
Epoch: [11][   60/  207]    Overall Loss 0.369560    Objective Loss 0.369560                                        LR 0.001000    Time 0.082592    
Epoch: [11][   70/  207]    Overall Loss 0.371351    Objective Loss 0.371351                                        LR 0.001000    Time 0.081402    
Epoch: [11][   80/  207]    Overall Loss 0.367474    Objective Loss 0.367474                                        LR 0.001000    Time 0.080329    
Epoch: [11][   90/  207]    Overall Loss 0.364754    Objective Loss 0.364754                                        LR 0.001000    Time 0.079364    
Epoch: [11][  100/  207]    Overall Loss 0.362277    Objective Loss 0.362277                                        LR 0.001000    Time 0.079328    
Epoch: [11][  110/  207]    Overall Loss 0.363479    Objective Loss 0.363479                                        LR 0.001000    Time 0.079316    
Epoch: [11][  120/  207]    Overall Loss 0.362907    Objective Loss 0.362907                                        LR 0.001000    Time 0.078917    
Epoch: [11][  130/  207]    Overall Loss 0.359190    Objective Loss 0.359190                                        LR 0.001000    Time 0.078266    
Epoch: [11][  140/  207]    Overall Loss 0.356593    Objective Loss 0.356593                                        LR 0.001000    Time 0.078523    
Epoch: [11][  150/  207]    Overall Loss 0.357693    Objective Loss 0.357693                                        LR 0.001000    Time 0.078311    
Epoch: [11][  160/  207]    Overall Loss 0.359606    Objective Loss 0.359606                                        LR 0.001000    Time 0.078357    
Epoch: [11][  170/  207]    Overall Loss 0.358266    Objective Loss 0.358266                                        LR 0.001000    Time 0.078105    
Epoch: [11][  180/  207]    Overall Loss 0.357729    Objective Loss 0.357729                                        LR 0.001000    Time 0.077724    
Epoch: [11][  190/  207]    Overall Loss 0.358346    Objective Loss 0.358346                                        LR 0.001000    Time 0.077522    
Epoch: [11][  200/  207]    Overall Loss 0.357731    Objective Loss 0.357731                                        LR 0.001000    Time 0.077421    
Epoch: [11][  207/  207]    Overall Loss 0.356977    Objective Loss 0.356977    Top1 82.106455    Top5 99.320498    LR 0.001000    Time 0.077200    
--- validate (epoch=11)-----------
5136 samples (512 per mini-batch)
Epoch: [11][   10/   11]    Loss 0.488414    Top1 75.058594    Top5 99.511719    
Epoch: [11][   11/   11]    Loss 0.512902    Top1 75.038941    Top5 99.513240    
==> Top1: 75.039    Top5: 99.513    Loss: 0.513

==> Confusion:
[[262   6   3   1   1  10   0  17]
 [  6 249  33   1   0   2   2   7]
 [  4  41 244   1   0   4   0   6]
 [  1   4   0 689 114  10  14   5]
 [  3   0   0  48 759  11  42  16]
 [ 15   3  15  22  25 754  44  16]
 [  4   0   0   4  34  23 751  21]
 [ 72  26  24 120  76 245  80 146]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [12][   10/  207]    Overall Loss 0.318392    Objective Loss 0.318392                                        LR 0.001000    Time 0.122971    
Epoch: [12][   20/  207]    Overall Loss 0.303403    Objective Loss 0.303403                                        LR 0.001000    Time 0.106545    
Epoch: [12][   30/  207]    Overall Loss 0.305075    Objective Loss 0.305075                                        LR 0.001000    Time 0.099690    
Epoch: [12][   40/  207]    Overall Loss 0.308316    Objective Loss 0.308316                                        LR 0.001000    Time 0.095781    
Epoch: [12][   50/  207]    Overall Loss 0.319528    Objective Loss 0.319528                                        LR 0.001000    Time 0.093103    
Epoch: [12][   60/  207]    Overall Loss 0.320811    Objective Loss 0.320811                                        LR 0.001000    Time 0.091796    
Epoch: [12][   70/  207]    Overall Loss 0.326136    Objective Loss 0.326136                                        LR 0.001000    Time 0.089817    
Epoch: [12][   80/  207]    Overall Loss 0.324590    Objective Loss 0.324590                                        LR 0.001000    Time 0.089074    
Epoch: [12][   90/  207]    Overall Loss 0.323180    Objective Loss 0.323180                                        LR 0.001000    Time 0.088211    
Epoch: [12][  100/  207]    Overall Loss 0.324163    Objective Loss 0.324163                                        LR 0.001000    Time 0.086338    
Epoch: [12][  110/  207]    Overall Loss 0.323928    Objective Loss 0.323928                                        LR 0.001000    Time 0.084929    
Epoch: [12][  120/  207]    Overall Loss 0.321103    Objective Loss 0.321103                                        LR 0.001000    Time 0.083654    
Epoch: [12][  130/  207]    Overall Loss 0.320926    Objective Loss 0.320926                                        LR 0.001000    Time 0.083133    
Epoch: [12][  140/  207]    Overall Loss 0.319798    Objective Loss 0.319798                                        LR 0.001000    Time 0.082344    
Epoch: [12][  150/  207]    Overall Loss 0.318627    Objective Loss 0.318627                                        LR 0.001000    Time 0.081954    
Epoch: [12][  160/  207]    Overall Loss 0.319156    Objective Loss 0.319156                                        LR 0.001000    Time 0.082245    
Epoch: [12][  170/  207]    Overall Loss 0.318790    Objective Loss 0.318790                                        LR 0.001000    Time 0.082426    
Epoch: [12][  180/  207]    Overall Loss 0.318571    Objective Loss 0.318571                                        LR 0.001000    Time 0.081857    
Epoch: [12][  190/  207]    Overall Loss 0.318359    Objective Loss 0.318359                                        LR 0.001000    Time 0.081644    
Epoch: [12][  200/  207]    Overall Loss 0.319532    Objective Loss 0.319532                                        LR 0.001000    Time 0.081929    
Epoch: [12][  207/  207]    Overall Loss 0.320915    Objective Loss 0.320915    Top1 88.674972    Top5 99.660249    LR 0.001000    Time 0.081596    
--- validate (epoch=12)-----------
5136 samples (512 per mini-batch)
Epoch: [12][   10/   11]    Loss 0.457821    Top1 74.921875    Top5 99.472656    
Epoch: [12][   11/   11]    Loss 0.442698    Top1 74.961059    Top5 99.474299    
==> Top1: 74.961    Top5: 99.474    Loss: 0.443

==> Confusion:
[[280   6   4   0   0   2   0   8]
 [ 12 252  30   0   0   2   2   2]
 [ 14  34 243   1   0   5   0   3]
 [  3   5   1 709  92  13  11   3]
 [  4   1   0  68 758  14  16  18]
 [ 36   6  10  15  43 756  20   8]
 [  5   0   0   4  61  30 719  18]
 [106  30  23  83  95 264  55 133]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [13][   10/  207]    Overall Loss 0.356180    Objective Loss 0.356180                                        LR 0.001000    Time 0.143025    
Epoch: [13][   20/  207]    Overall Loss 0.332678    Objective Loss 0.332678                                        LR 0.001000    Time 0.109175    
Epoch: [13][   30/  207]    Overall Loss 0.328884    Objective Loss 0.328884                                        LR 0.001000    Time 0.097961    
Epoch: [13][   40/  207]    Overall Loss 0.323702    Objective Loss 0.323702                                        LR 0.001000    Time 0.091876    
Epoch: [13][   50/  207]    Overall Loss 0.325560    Objective Loss 0.325560                                        LR 0.001000    Time 0.089329    
Epoch: [13][   60/  207]    Overall Loss 0.325074    Objective Loss 0.325074                                        LR 0.001000    Time 0.087054    
Epoch: [13][   70/  207]    Overall Loss 0.322832    Objective Loss 0.322832                                        LR 0.001000    Time 0.085073    
Epoch: [13][   80/  207]    Overall Loss 0.317631    Objective Loss 0.317631                                        LR 0.001000    Time 0.083596    
Epoch: [13][   90/  207]    Overall Loss 0.317330    Objective Loss 0.317330                                        LR 0.001000    Time 0.082566    
Epoch: [13][  100/  207]    Overall Loss 0.319407    Objective Loss 0.319407                                        LR 0.001000    Time 0.082342    
Epoch: [13][  110/  207]    Overall Loss 0.318232    Objective Loss 0.318232                                        LR 0.001000    Time 0.081475    
Epoch: [13][  120/  207]    Overall Loss 0.315669    Objective Loss 0.315669                                        LR 0.001000    Time 0.081733    
Epoch: [13][  130/  207]    Overall Loss 0.315036    Objective Loss 0.315036                                        LR 0.001000    Time 0.082013    
Epoch: [13][  140/  207]    Overall Loss 0.314467    Objective Loss 0.314467                                        LR 0.001000    Time 0.082289    
Epoch: [13][  150/  207]    Overall Loss 0.314876    Objective Loss 0.314876                                        LR 0.001000    Time 0.082608    
Epoch: [13][  160/  207]    Overall Loss 0.314212    Objective Loss 0.314212                                        LR 0.001000    Time 0.082259    
Epoch: [13][  170/  207]    Overall Loss 0.313005    Objective Loss 0.313005                                        LR 0.001000    Time 0.082119    
Epoch: [13][  180/  207]    Overall Loss 0.313547    Objective Loss 0.313547                                        LR 0.001000    Time 0.082445    
Epoch: [13][  190/  207]    Overall Loss 0.313715    Objective Loss 0.313715                                        LR 0.001000    Time 0.082338    
Epoch: [13][  200/  207]    Overall Loss 0.312451    Objective Loss 0.312451                                        LR 0.001000    Time 0.082205    
Epoch: [13][  207/  207]    Overall Loss 0.311887    Objective Loss 0.311887    Top1 87.995470    Top5 99.886750    LR 0.001000    Time 0.082112    
--- validate (epoch=13)-----------
5136 samples (512 per mini-batch)
Epoch: [13][   10/   11]    Loss 0.500172    Top1 76.621094    Top5 99.628906    
Epoch: [13][   11/   11]    Loss 0.478965    Top1 76.616044    Top5 99.630062    
==> Top1: 76.616    Top5: 99.630    Loss: 0.479

==> Confusion:
[[277   8   2   0   0   1   0  12]
 [  7 276  11   1   0   0   0   5]
 [ 11  64 216   1   0   1   0   7]
 [  2   6   1 723  73   9   9  14]
 [  5   1   0  59 736  11  40  27]
 [ 29   8  16  20  23 721  41  36]
 [  6   0   0   8  24  24 746  29]
 [ 91  44  14  84  58 187  71 240]]

==> Best [Top1: 76.616   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 13]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [14][   10/  207]    Overall Loss 0.294264    Objective Loss 0.294264                                        LR 0.001000    Time 0.142041    
Epoch: [14][   20/  207]    Overall Loss 0.275154    Objective Loss 0.275154                                        LR 0.001000    Time 0.110439    
Epoch: [14][   30/  207]    Overall Loss 0.275127    Objective Loss 0.275127                                        LR 0.001000    Time 0.105283    
Epoch: [14][   40/  207]    Overall Loss 0.271858    Objective Loss 0.271858                                        LR 0.001000    Time 0.097461    
Epoch: [14][   50/  207]    Overall Loss 0.275755    Objective Loss 0.275755                                        LR 0.001000    Time 0.092479    
Epoch: [14][   60/  207]    Overall Loss 0.274251    Objective Loss 0.274251                                        LR 0.001000    Time 0.089101    
Epoch: [14][   70/  207]    Overall Loss 0.279270    Objective Loss 0.279270                                        LR 0.001000    Time 0.087994    
Epoch: [14][   80/  207]    Overall Loss 0.287145    Objective Loss 0.287145                                        LR 0.001000    Time 0.086019    
Epoch: [14][   90/  207]    Overall Loss 0.289724    Objective Loss 0.289724                                        LR 0.001000    Time 0.084980    
Epoch: [14][  100/  207]    Overall Loss 0.291298    Objective Loss 0.291298                                        LR 0.001000    Time 0.084338    
Epoch: [14][  110/  207]    Overall Loss 0.294486    Objective Loss 0.294486                                        LR 0.001000    Time 0.083342    
Epoch: [14][  120/  207]    Overall Loss 0.296026    Objective Loss 0.296026                                        LR 0.001000    Time 0.082734    
Epoch: [14][  130/  207]    Overall Loss 0.296524    Objective Loss 0.296524                                        LR 0.001000    Time 0.082287    
Epoch: [14][  140/  207]    Overall Loss 0.298387    Objective Loss 0.298387                                        LR 0.001000    Time 0.081877    
Epoch: [14][  150/  207]    Overall Loss 0.296361    Objective Loss 0.296361                                        LR 0.001000    Time 0.081654    
Epoch: [14][  160/  207]    Overall Loss 0.296916    Objective Loss 0.296916                                        LR 0.001000    Time 0.081541    
Epoch: [14][  170/  207]    Overall Loss 0.299547    Objective Loss 0.299547                                        LR 0.001000    Time 0.081735    
Epoch: [14][  180/  207]    Overall Loss 0.301758    Objective Loss 0.301758                                        LR 0.001000    Time 0.082094    
Epoch: [14][  190/  207]    Overall Loss 0.304098    Objective Loss 0.304098                                        LR 0.001000    Time 0.082843    
Epoch: [14][  200/  207]    Overall Loss 0.302959    Objective Loss 0.302959                                        LR 0.001000    Time 0.082586    
Epoch: [14][  207/  207]    Overall Loss 0.303916    Objective Loss 0.303916    Top1 82.785957    Top5 99.433749    LR 0.001000    Time 0.082277    
--- validate (epoch=14)-----------
5136 samples (512 per mini-batch)
Epoch: [14][   10/   11]    Loss 0.565935    Top1 74.433594    Top5 99.589844    
Epoch: [14][   11/   11]    Loss 0.584503    Top1 74.338006    Top5 99.571651    
==> Top1: 74.338    Top5: 99.572    Loss: 0.585

==> Confusion:
[[285   2   1   0   1   0   1  10]
 [ 17 218  57   0   0   4   1   3]
 [ 12  10 265   1   1   4   0   7]
 [  3   4   0 642 154  14  11   9]
 [  4   0   0  24 804   6  24  17]
 [ 73   2  19   7  48 678  49  18]
 [  9   0   0   2  52  10 738  26]
 [167  19  31  41 132 148  63 188]]

==> Best [Top1: 76.616   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 13]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [15][   10/  207]    Overall Loss 0.314211    Objective Loss 0.314211                                        LR 0.001000    Time 0.141034    
Epoch: [15][   20/  207]    Overall Loss 0.313555    Objective Loss 0.313555                                        LR 0.001000    Time 0.107810    
Epoch: [15][   30/  207]    Overall Loss 0.305406    Objective Loss 0.305406                                        LR 0.001000    Time 0.098532    
Epoch: [15][   40/  207]    Overall Loss 0.294127    Objective Loss 0.294127                                        LR 0.001000    Time 0.093028    
Epoch: [15][   50/  207]    Overall Loss 0.294872    Objective Loss 0.294872                                        LR 0.001000    Time 0.091864    
Epoch: [15][   60/  207]    Overall Loss 0.293732    Objective Loss 0.293732                                        LR 0.001000    Time 0.089057    
Epoch: [15][   70/  207]    Overall Loss 0.290348    Objective Loss 0.290348                                        LR 0.001000    Time 0.087643    
Epoch: [15][   80/  207]    Overall Loss 0.287303    Objective Loss 0.287303                                        LR 0.001000    Time 0.087269    
Epoch: [15][   90/  207]    Overall Loss 0.285417    Objective Loss 0.285417                                        LR 0.001000    Time 0.089373    
Epoch: [15][  100/  207]    Overall Loss 0.284825    Objective Loss 0.284825                                        LR 0.001000    Time 0.088469    
Epoch: [15][  110/  207]    Overall Loss 0.284320    Objective Loss 0.284320                                        LR 0.001000    Time 0.087186    
Epoch: [15][  120/  207]    Overall Loss 0.285158    Objective Loss 0.285158                                        LR 0.001000    Time 0.086207    
Epoch: [15][  130/  207]    Overall Loss 0.286301    Objective Loss 0.286301                                        LR 0.001000    Time 0.085263    
Epoch: [15][  140/  207]    Overall Loss 0.286627    Objective Loss 0.286627                                        LR 0.001000    Time 0.084629    
Epoch: [15][  150/  207]    Overall Loss 0.286152    Objective Loss 0.286152                                        LR 0.001000    Time 0.084164    
Epoch: [15][  160/  207]    Overall Loss 0.286865    Objective Loss 0.286865                                        LR 0.001000    Time 0.083786    
Epoch: [15][  170/  207]    Overall Loss 0.286711    Objective Loss 0.286711                                        LR 0.001000    Time 0.083645    
Epoch: [15][  180/  207]    Overall Loss 0.286212    Objective Loss 0.286212                                        LR 0.001000    Time 0.083479    
Epoch: [15][  190/  207]    Overall Loss 0.285401    Objective Loss 0.285401                                        LR 0.001000    Time 0.083134    
Epoch: [15][  200/  207]    Overall Loss 0.283864    Objective Loss 0.283864                                        LR 0.001000    Time 0.082759    
Epoch: [15][  207/  207]    Overall Loss 0.283905    Objective Loss 0.283905    Top1 88.901472    Top5 100.000000    LR 0.001000    Time 0.082340    
--- validate (epoch=15)-----------
5136 samples (512 per mini-batch)
Epoch: [15][   10/   11]    Loss 0.426429    Top1 77.226562    Top5 99.648438    
Epoch: [15][   11/   11]    Loss 0.410154    Top1 77.239097    Top5 99.649533    
==> Top1: 77.239    Top5: 99.650    Loss: 0.410

==> Confusion:
[[270   6   3   1   1   5   2  12]
 [  8 256  27   0   0   2   0   7]
 [  4  31 255   1   0   4   1   4]
 [  0   5   1 750  54  12  12   3]
 [  3   1   0  68 746   9  40  12]
 [ 15   4  14  17  24 764  46  10]
 [  4   1   0   8  28  10 771  15]
 [ 73  31  17 110  82 230  91 155]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [16][   10/  207]    Overall Loss 0.250688    Objective Loss 0.250688                                        LR 0.001000    Time 0.146595    
Epoch: [16][   20/  207]    Overall Loss 0.264612    Objective Loss 0.264612                                        LR 0.001000    Time 0.109789    
Epoch: [16][   30/  207]    Overall Loss 0.259927    Objective Loss 0.259927                                        LR 0.001000    Time 0.097052    
Epoch: [16][   40/  207]    Overall Loss 0.260306    Objective Loss 0.260306                                        LR 0.001000    Time 0.090962    
Epoch: [16][   50/  207]    Overall Loss 0.255677    Objective Loss 0.255677                                        LR 0.001000    Time 0.087031    
Epoch: [16][   60/  207]    Overall Loss 0.255847    Objective Loss 0.255847                                        LR 0.001000    Time 0.084963    
Epoch: [16][   70/  207]    Overall Loss 0.258546    Objective Loss 0.258546                                        LR 0.001000    Time 0.083728    
Epoch: [16][   80/  207]    Overall Loss 0.264180    Objective Loss 0.264180                                        LR 0.001000    Time 0.083669    
Epoch: [16][   90/  207]    Overall Loss 0.268650    Objective Loss 0.268650                                        LR 0.001000    Time 0.082338    
Epoch: [16][  100/  207]    Overall Loss 0.270293    Objective Loss 0.270293                                        LR 0.001000    Time 0.081235    
Epoch: [16][  110/  207]    Overall Loss 0.270007    Objective Loss 0.270007                                        LR 0.001000    Time 0.080441    
Epoch: [16][  120/  207]    Overall Loss 0.272812    Objective Loss 0.272812                                        LR 0.001000    Time 0.079743    
Epoch: [16][  130/  207]    Overall Loss 0.271060    Objective Loss 0.271060                                        LR 0.001000    Time 0.079351    
Epoch: [16][  140/  207]    Overall Loss 0.271467    Objective Loss 0.271467                                        LR 0.001000    Time 0.079241    
Epoch: [16][  150/  207]    Overall Loss 0.269177    Objective Loss 0.269177                                        LR 0.001000    Time 0.079202    
Epoch: [16][  160/  207]    Overall Loss 0.267935    Objective Loss 0.267935                                        LR 0.001000    Time 0.079514    
Epoch: [16][  170/  207]    Overall Loss 0.269269    Objective Loss 0.269269                                        LR 0.001000    Time 0.079376    
Epoch: [16][  180/  207]    Overall Loss 0.269224    Objective Loss 0.269224                                        LR 0.001000    Time 0.079247    
Epoch: [16][  190/  207]    Overall Loss 0.268856    Objective Loss 0.268856                                        LR 0.001000    Time 0.079097    
Epoch: [16][  200/  207]    Overall Loss 0.269226    Objective Loss 0.269226                                        LR 0.001000    Time 0.078825    
Epoch: [16][  207/  207]    Overall Loss 0.269548    Objective Loss 0.269548    Top1 83.918460    Top5 99.773499    LR 0.001000    Time 0.078550    
--- validate (epoch=16)-----------
5136 samples (512 per mini-batch)
Epoch: [16][   10/   11]    Loss 0.511894    Top1 75.527344    Top5 99.472656    
Epoch: [16][   11/   11]    Loss 0.511468    Top1 75.525701    Top5 99.474299    
==> Top1: 75.526    Top5: 99.474    Loss: 0.511

==> Confusion:
[[239   2  11   1   4  30   5   8]
 [  1 227  60   1   0   8   0   3]
 [  0  14 270   1   1  12   0   2]
 [  0   4   1 685 127   8  12   0]
 [  0   0   0  31 810   9  26   3]
 [  5   4  11  12  32 767  60   3]
 [  2   0   0   6  34  12 778   5]
 [ 43  19  26  89 115 294 100 103]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [17][   10/  207]    Overall Loss 0.267886    Objective Loss 0.267886                                        LR 0.001000    Time 0.140394    
Epoch: [17][   20/  207]    Overall Loss 0.276784    Objective Loss 0.276784                                        LR 0.001000    Time 0.106830    
Epoch: [17][   30/  207]    Overall Loss 0.263090    Objective Loss 0.263090                                        LR 0.001000    Time 0.096091    
Epoch: [17][   40/  207]    Overall Loss 0.259204    Objective Loss 0.259204                                        LR 0.001000    Time 0.091581    
Epoch: [17][   50/  207]    Overall Loss 0.261765    Objective Loss 0.261765                                        LR 0.001000    Time 0.088857    
Epoch: [17][   60/  207]    Overall Loss 0.259998    Objective Loss 0.259998                                        LR 0.001000    Time 0.086565    
Epoch: [17][   70/  207]    Overall Loss 0.257289    Objective Loss 0.257289                                        LR 0.001000    Time 0.085243    
Epoch: [17][   80/  207]    Overall Loss 0.257394    Objective Loss 0.257394                                        LR 0.001000    Time 0.084633    
Epoch: [17][   90/  207]    Overall Loss 0.262155    Objective Loss 0.262155                                        LR 0.001000    Time 0.083875    
Epoch: [17][  100/  207]    Overall Loss 0.264065    Objective Loss 0.264065                                        LR 0.001000    Time 0.084274    
Epoch: [17][  110/  207]    Overall Loss 0.261878    Objective Loss 0.261878                                        LR 0.001000    Time 0.083236    
Epoch: [17][  120/  207]    Overall Loss 0.262174    Objective Loss 0.262174                                        LR 0.001000    Time 0.082416    
Epoch: [17][  130/  207]    Overall Loss 0.263404    Objective Loss 0.263404                                        LR 0.001000    Time 0.081856    
Epoch: [17][  140/  207]    Overall Loss 0.264743    Objective Loss 0.264743                                        LR 0.001000    Time 0.081289    
Epoch: [17][  150/  207]    Overall Loss 0.266683    Objective Loss 0.266683                                        LR 0.001000    Time 0.081258    
Epoch: [17][  160/  207]    Overall Loss 0.266572    Objective Loss 0.266572                                        LR 0.001000    Time 0.081060    
Epoch: [17][  170/  207]    Overall Loss 0.267018    Objective Loss 0.267018                                        LR 0.001000    Time 0.080515    
Epoch: [17][  180/  207]    Overall Loss 0.268064    Objective Loss 0.268064                                        LR 0.001000    Time 0.080057    
Epoch: [17][  190/  207]    Overall Loss 0.268408    Objective Loss 0.268408                                        LR 0.001000    Time 0.079808    
Epoch: [17][  200/  207]    Overall Loss 0.268298    Objective Loss 0.268298                                        LR 0.001000    Time 0.079604    
Epoch: [17][  207/  207]    Overall Loss 0.267725    Objective Loss 0.267725    Top1 89.127973    Top5 99.773499    LR 0.001000    Time 0.079278    
--- validate (epoch=17)-----------
5136 samples (512 per mini-batch)
Epoch: [17][   10/   11]    Loss 0.410365    Top1 76.816406    Top5 99.707031    
Epoch: [17][   11/   11]    Loss 0.444480    Top1 76.830218    Top5 99.707944    
==> Top1: 76.830    Top5: 99.708    Loss: 0.444

==> Confusion:
[[274   7   4   0   1   5   0   9]
 [  5 259  33   0   0   1   0   2]
 [  8  34 253   0   0   2   0   3]
 [  1   4   0 703  99  19   9   2]
 [  3   0   0  43 781  19  23  10]
 [ 30   6  16  11  18 777  29   7]
 [  5   0   0   4  40  22 751  15]
 [ 96  42  24  74  79 271  55 148]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [18][   10/  207]    Overall Loss 0.243582    Objective Loss 0.243582                                        LR 0.001000    Time 0.139235    
Epoch: [18][   20/  207]    Overall Loss 0.231455    Objective Loss 0.231455                                        LR 0.001000    Time 0.108432    
Epoch: [18][   30/  207]    Overall Loss 0.227363    Objective Loss 0.227363                                        LR 0.001000    Time 0.098154    
Epoch: [18][   40/  207]    Overall Loss 0.230836    Objective Loss 0.230836                                        LR 0.001000    Time 0.093489    
Epoch: [18][   50/  207]    Overall Loss 0.228919    Objective Loss 0.228919                                        LR 0.001000    Time 0.090168    
Epoch: [18][   60/  207]    Overall Loss 0.229228    Objective Loss 0.229228                                        LR 0.001000    Time 0.090175    
Epoch: [18][   70/  207]    Overall Loss 0.231489    Objective Loss 0.231489                                        LR 0.001000    Time 0.087754    
Epoch: [18][   80/  207]    Overall Loss 0.237213    Objective Loss 0.237213                                        LR 0.001000    Time 0.086015    
Epoch: [18][   90/  207]    Overall Loss 0.237555    Objective Loss 0.237555                                        LR 0.001000    Time 0.085892    
Epoch: [18][  100/  207]    Overall Loss 0.237266    Objective Loss 0.237266                                        LR 0.001000    Time 0.086521    
Epoch: [18][  110/  207]    Overall Loss 0.237198    Objective Loss 0.237198                                        LR 0.001000    Time 0.085834    
Epoch: [18][  120/  207]    Overall Loss 0.236641    Objective Loss 0.236641                                        LR 0.001000    Time 0.085188    
Epoch: [18][  130/  207]    Overall Loss 0.236457    Objective Loss 0.236457                                        LR 0.001000    Time 0.084922    
Epoch: [18][  140/  207]    Overall Loss 0.238856    Objective Loss 0.238856                                        LR 0.001000    Time 0.084593    
Epoch: [18][  150/  207]    Overall Loss 0.239765    Objective Loss 0.239765                                        LR 0.001000    Time 0.083672    
Epoch: [18][  160/  207]    Overall Loss 0.242236    Objective Loss 0.242236                                        LR 0.001000    Time 0.082765    
Epoch: [18][  170/  207]    Overall Loss 0.245223    Objective Loss 0.245223                                        LR 0.001000    Time 0.082119    
Epoch: [18][  180/  207]    Overall Loss 0.245896    Objective Loss 0.245896                                        LR 0.001000    Time 0.081488    
Epoch: [18][  190/  207]    Overall Loss 0.246739    Objective Loss 0.246739                                        LR 0.001000    Time 0.080901    
Epoch: [18][  200/  207]    Overall Loss 0.249254    Objective Loss 0.249254                                        LR 0.001000    Time 0.080483    
Epoch: [18][  207/  207]    Overall Loss 0.249314    Objective Loss 0.249314    Top1 84.258211    Top5 99.773499    LR 0.001000    Time 0.080056    
--- validate (epoch=18)-----------
5136 samples (512 per mini-batch)
Epoch: [18][   10/   11]    Loss 0.443400    Top1 76.933594    Top5 99.433594    
Epoch: [18][   11/   11]    Loss 0.461887    Top1 76.927570    Top5 99.435358    
==> Top1: 76.928    Top5: 99.435    Loss: 0.462

==> Confusion:
[[260  10  12   0   0   5   1  12]
 [  2 270  26   0   0   1   0   1]
 [  0  34 265   0   0   0   0   1]
 [  0   7   1 701  85  14  20   9]
 [  5   0   0  42 755  11  51  15]
 [ 15   9  35  11  19 752  37  16]
 [  4   0   0   3  25  21 773  11]
 [ 61  54  55  79  71 219  75 175]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [19][   10/  207]    Overall Loss 0.231804    Objective Loss 0.231804                                        LR 0.001000    Time 0.129910    
Epoch: [19][   20/  207]    Overall Loss 0.238639    Objective Loss 0.238639                                        LR 0.001000    Time 0.100392    
Epoch: [19][   30/  207]    Overall Loss 0.234766    Objective Loss 0.234766                                        LR 0.001000    Time 0.090748    
Epoch: [19][   40/  207]    Overall Loss 0.231077    Objective Loss 0.231077                                        LR 0.001000    Time 0.086258    
Epoch: [19][   50/  207]    Overall Loss 0.231474    Objective Loss 0.231474                                        LR 0.001000    Time 0.083171    
Epoch: [19][   60/  207]    Overall Loss 0.230370    Objective Loss 0.230370                                        LR 0.001000    Time 0.081522    
Epoch: [19][   70/  207]    Overall Loss 0.232002    Objective Loss 0.232002                                        LR 0.001000    Time 0.080010    
Epoch: [19][   80/  207]    Overall Loss 0.235333    Objective Loss 0.235333                                        LR 0.001000    Time 0.079011    
Epoch: [19][   90/  207]    Overall Loss 0.236265    Objective Loss 0.236265                                        LR 0.001000    Time 0.078369    
Epoch: [19][  100/  207]    Overall Loss 0.240644    Objective Loss 0.240644                                        LR 0.001000    Time 0.077609    
Epoch: [19][  110/  207]    Overall Loss 0.242354    Objective Loss 0.242354                                        LR 0.001000    Time 0.077171    
Epoch: [19][  120/  207]    Overall Loss 0.239068    Objective Loss 0.239068                                        LR 0.001000    Time 0.076830    
Epoch: [19][  130/  207]    Overall Loss 0.237839    Objective Loss 0.237839                                        LR 0.001000    Time 0.076539    
Epoch: [19][  140/  207]    Overall Loss 0.239438    Objective Loss 0.239438                                        LR 0.001000    Time 0.076326    
Epoch: [19][  150/  207]    Overall Loss 0.239193    Objective Loss 0.239193                                        LR 0.001000    Time 0.076020    
Epoch: [19][  160/  207]    Overall Loss 0.239025    Objective Loss 0.239025                                        LR 0.001000    Time 0.075843    
Epoch: [19][  170/  207]    Overall Loss 0.240705    Objective Loss 0.240705                                        LR 0.001000    Time 0.075590    
Epoch: [19][  180/  207]    Overall Loss 0.240435    Objective Loss 0.240435                                        LR 0.001000    Time 0.075440    
Epoch: [19][  190/  207]    Overall Loss 0.240489    Objective Loss 0.240489                                        LR 0.001000    Time 0.075277    
Epoch: [19][  200/  207]    Overall Loss 0.241032    Objective Loss 0.241032                                        LR 0.001000    Time 0.075165    
Epoch: [19][  207/  207]    Overall Loss 0.241791    Objective Loss 0.241791    Top1 89.580974    Top5 99.886750    LR 0.001000    Time 0.074887    
--- validate (epoch=19)-----------
5136 samples (512 per mini-batch)
Epoch: [19][   10/   11]    Loss 0.471617    Top1 78.632812    Top5 99.550781    
Epoch: [19][   11/   11]    Loss 0.456836    Top1 78.621495    Top5 99.552181    
==> Top1: 78.621    Top5: 99.552    Loss: 0.457

==> Confusion:
[[276   5   3   0   1   1   0  14]
 [  7 255  29   1   0   1   0   7]
 [  5  24 261   1   0   3   0   6]
 [  1   5   1 737  51  12  17  13]
 [  4   3   0  63 730  16  37  26]
 [ 23  10  15  15  18 761  27  25]
 [  5   0   0   8  21  23 759  21]
 [ 79  29  26  86  53 197  60 259]]

==> Best [Top1: 78.621   Top5: 99.552   Sparsity:0.00   Params: 117200 on epoch: 19]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [20][   10/  207]    Overall Loss 0.194061    Objective Loss 0.194061                                        LR 0.001000    Time 0.127957    
Epoch: [20][   20/  207]    Overall Loss 0.210630    Objective Loss 0.210630                                        LR 0.001000    Time 0.099012    
Epoch: [20][   30/  207]    Overall Loss 0.214299    Objective Loss 0.214299                                        LR 0.001000    Time 0.089512    
Epoch: [20][   40/  207]    Overall Loss 0.223578    Objective Loss 0.223578                                        LR 0.001000    Time 0.084832    
Epoch: [20][   50/  207]    Overall Loss 0.227937    Objective Loss 0.227937                                        LR 0.001000    Time 0.081985    
Epoch: [20][   60/  207]    Overall Loss 0.228952    Objective Loss 0.228952                                        LR 0.001000    Time 0.080561    
Epoch: [20][   70/  207]    Overall Loss 0.230667    Objective Loss 0.230667                                        LR 0.001000    Time 0.079400    
Epoch: [20][   80/  207]    Overall Loss 0.230956    Objective Loss 0.230956                                        LR 0.001000    Time 0.078290    
Epoch: [20][   90/  207]    Overall Loss 0.229086    Objective Loss 0.229086                                        LR 0.001000    Time 0.077566    
Epoch: [20][  100/  207]    Overall Loss 0.228147    Objective Loss 0.228147                                        LR 0.001000    Time 0.076926    
Epoch: [20][  110/  207]    Overall Loss 0.228118    Objective Loss 0.228118                                        LR 0.001000    Time 0.076437    
Epoch: [20][  120/  207]    Overall Loss 0.232718    Objective Loss 0.232718                                        LR 0.001000    Time 0.076083    
Epoch: [20][  130/  207]    Overall Loss 0.233846    Objective Loss 0.233846                                        LR 0.001000    Time 0.075699    
Epoch: [20][  140/  207]    Overall Loss 0.234446    Objective Loss 0.234446                                        LR 0.001000    Time 0.075318    
Epoch: [20][  150/  207]    Overall Loss 0.234488    Objective Loss 0.234488                                        LR 0.001000    Time 0.075201    
Epoch: [20][  160/  207]    Overall Loss 0.238344    Objective Loss 0.238344                                        LR 0.001000    Time 0.075205    
Epoch: [20][  170/  207]    Overall Loss 0.238853    Objective Loss 0.238853                                        LR 0.001000    Time 0.075004    
Epoch: [20][  180/  207]    Overall Loss 0.238453    Objective Loss 0.238453                                        LR 0.001000    Time 0.074890    
Epoch: [20][  190/  207]    Overall Loss 0.238292    Objective Loss 0.238292                                        LR 0.001000    Time 0.074758    
Epoch: [20][  200/  207]    Overall Loss 0.237836    Objective Loss 0.237836                                        LR 0.001000    Time 0.074501    
Epoch: [20][  207/  207]    Overall Loss 0.237395    Objective Loss 0.237395    Top1 89.807475    Top5 99.773499    LR 0.001000    Time 0.074255    
--- validate (epoch=20)-----------
5136 samples (512 per mini-batch)
Epoch: [20][   10/   11]    Loss 0.494830    Top1 78.710938    Top5 99.511719    
Epoch: [20][   11/   11]    Loss 0.586379    Top1 78.699377    Top5 99.513240    
==> Top1: 78.699    Top5: 99.513    Loss: 0.586

==> Confusion:
[[247  16   4   0   1  11   1  20]
 [  2 281  12   0   0   1   0   4]
 [  2  51 242   0   1   1   0   3]
 [  0   6   0 737  64  11   9  10]
 [  2   2   0  64 772   7  17  15]
 [  9  10  14  14  28 761  30  28]
 [  3   0   0   7  43  17 750  17]
 [ 42  53  22  90  80 208  42 252]]

==> Best [Top1: 78.699   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 20]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [21][   10/  207]    Overall Loss 0.230572    Objective Loss 0.230572                                        LR 0.001000    Time 0.128963    
Epoch: [21][   20/  207]    Overall Loss 0.235101    Objective Loss 0.235101                                        LR 0.001000    Time 0.099927    
Epoch: [21][   30/  207]    Overall Loss 0.229601    Objective Loss 0.229601                                        LR 0.001000    Time 0.090170    
Epoch: [21][   40/  207]    Overall Loss 0.231632    Objective Loss 0.231632                                        LR 0.001000    Time 0.085300    
Epoch: [21][   50/  207]    Overall Loss 0.228684    Objective Loss 0.228684                                        LR 0.001000    Time 0.082727    
Epoch: [21][   60/  207]    Overall Loss 0.228142    Objective Loss 0.228142                                        LR 0.001000    Time 0.080820    
Epoch: [21][   70/  207]    Overall Loss 0.229269    Objective Loss 0.229269                                        LR 0.001000    Time 0.079223    
Epoch: [21][   80/  207]    Overall Loss 0.228898    Objective Loss 0.228898                                        LR 0.001000    Time 0.078201    
Epoch: [21][   90/  207]    Overall Loss 0.230985    Objective Loss 0.230985                                        LR 0.001000    Time 0.077478    
Epoch: [21][  100/  207]    Overall Loss 0.233009    Objective Loss 0.233009                                        LR 0.001000    Time 0.076877    
Epoch: [21][  110/  207]    Overall Loss 0.234950    Objective Loss 0.234950                                        LR 0.001000    Time 0.076580    
Epoch: [21][  120/  207]    Overall Loss 0.236862    Objective Loss 0.236862                                        LR 0.001000    Time 0.076201    
Epoch: [21][  130/  207]    Overall Loss 0.235303    Objective Loss 0.235303                                        LR 0.001000    Time 0.075835    
Epoch: [21][  140/  207]    Overall Loss 0.238090    Objective Loss 0.238090                                        LR 0.001000    Time 0.075512    
Epoch: [21][  150/  207]    Overall Loss 0.239635    Objective Loss 0.239635                                        LR 0.001000    Time 0.075162    
Epoch: [21][  160/  207]    Overall Loss 0.239640    Objective Loss 0.239640                                        LR 0.001000    Time 0.074859    
Epoch: [21][  170/  207]    Overall Loss 0.239143    Objective Loss 0.239143                                        LR 0.001000    Time 0.074643    
Epoch: [21][  180/  207]    Overall Loss 0.238466    Objective Loss 0.238466                                        LR 0.001000    Time 0.074426    
Epoch: [21][  190/  207]    Overall Loss 0.238222    Objective Loss 0.238222                                        LR 0.001000    Time 0.074205    
Epoch: [21][  200/  207]    Overall Loss 0.238371    Objective Loss 0.238371                                        LR 0.001000    Time 0.074039    
Epoch: [21][  207/  207]    Overall Loss 0.238441    Objective Loss 0.238441    Top1 83.691959    Top5 100.000000    LR 0.001000    Time 0.073813    
--- validate (epoch=21)-----------
5136 samples (512 per mini-batch)
Epoch: [21][   10/   11]    Loss 0.517648    Top1 76.933594    Top5 99.550781    
Epoch: [21][   11/   11]    Loss 0.477283    Top1 76.966511    Top5 99.552181    
==> Top1: 76.967    Top5: 99.552    Loss: 0.477

==> Confusion:
[[246   6  10   3   3  11   4  17]
 [  2 238  50   4   0   2   1   3]
 [  1  20 266   2   0   7   0   4]
 [  0   3   1 729  93   3   6   2]
 [  0   0   0  39 814   4  17   5]
 [  3   1  12  39  37 737  54  11]
 [  2   0   0   8  53  10 755   9]
 [ 41  18  27 138 105 217  75 168]]

==> Best [Top1: 78.699   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 20]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [22][   10/  207]    Overall Loss 0.253451    Objective Loss 0.253451                                        LR 0.001000    Time 0.127197    
Epoch: [22][   20/  207]    Overall Loss 0.237993    Objective Loss 0.237993                                        LR 0.001000    Time 0.098861    
Epoch: [22][   30/  207]    Overall Loss 0.232718    Objective Loss 0.232718                                        LR 0.001000    Time 0.089487    
Epoch: [22][   40/  207]    Overall Loss 0.228314    Objective Loss 0.228314                                        LR 0.001000    Time 0.084937    
Epoch: [22][   50/  207]    Overall Loss 0.225994    Objective Loss 0.225994                                        LR 0.001000    Time 0.082095    
Epoch: [22][   60/  207]    Overall Loss 0.226683    Objective Loss 0.226683                                        LR 0.001000    Time 0.080632    
Epoch: [22][   70/  207]    Overall Loss 0.222447    Objective Loss 0.222447                                        LR 0.001000    Time 0.079074    
Epoch: [22][   80/  207]    Overall Loss 0.221618    Objective Loss 0.221618                                        LR 0.001000    Time 0.078167    
Epoch: [22][   90/  207]    Overall Loss 0.219262    Objective Loss 0.219262                                        LR 0.001000    Time 0.077489    
Epoch: [22][  100/  207]    Overall Loss 0.219112    Objective Loss 0.219112                                        LR 0.001000    Time 0.076921    
Epoch: [22][  110/  207]    Overall Loss 0.218069    Objective Loss 0.218069                                        LR 0.001000    Time 0.076364    
Epoch: [22][  120/  207]    Overall Loss 0.220238    Objective Loss 0.220238                                        LR 0.001000    Time 0.075777    
Epoch: [22][  130/  207]    Overall Loss 0.222199    Objective Loss 0.222199                                        LR 0.001000    Time 0.075604    
Epoch: [22][  140/  207]    Overall Loss 0.224329    Objective Loss 0.224329                                        LR 0.001000    Time 0.075259    
Epoch: [22][  150/  207]    Overall Loss 0.224568    Objective Loss 0.224568                                        LR 0.001000    Time 0.075165    
Epoch: [22][  160/  207]    Overall Loss 0.224839    Objective Loss 0.224839                                        LR 0.001000    Time 0.074905    
Epoch: [22][  170/  207]    Overall Loss 0.224877    Objective Loss 0.224877                                        LR 0.001000    Time 0.074587    
Epoch: [22][  180/  207]    Overall Loss 0.224430    Objective Loss 0.224430                                        LR 0.001000    Time 0.074409    
Epoch: [22][  190/  207]    Overall Loss 0.224569    Objective Loss 0.224569                                        LR 0.001000    Time 0.074367    
Epoch: [22][  200/  207]    Overall Loss 0.225544    Objective Loss 0.225544                                        LR 0.001000    Time 0.074176    
Epoch: [22][  207/  207]    Overall Loss 0.224488    Objective Loss 0.224488    Top1 91.166478    Top5 99.886750    LR 0.001000    Time 0.073935    
--- validate (epoch=22)-----------
5136 samples (512 per mini-batch)
Epoch: [22][   10/   11]    Loss 0.425537    Top1 79.492188    Top5 99.667969    
Epoch: [22][   11/   11]    Loss 0.419203    Top1 79.458723    Top5 99.669003    
==> Top1: 79.459    Top5: 99.669    Loss: 0.419

==> Confusion:
[[272   5   6   0   0   5   1  11]
 [  5 253  37   0   0   1   0   4]
 [  2  21 271   0   1   2   0   3]
 [  0   6   2 714  81  14   9  11]
 [  5   0   0  33 791  10  21  19]
 [ 22   7  15   7  25 792  12  14]
 [  5   0   0   5  39  20 746  22]
 [ 73  30  26  51  91 240  36 242]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [23][   10/  207]    Overall Loss 0.198285    Objective Loss 0.198285                                        LR 0.001000    Time 0.126532    
Epoch: [23][   20/  207]    Overall Loss 0.216461    Objective Loss 0.216461                                        LR 0.001000    Time 0.098459    
Epoch: [23][   30/  207]    Overall Loss 0.211884    Objective Loss 0.211884                                        LR 0.001000    Time 0.089132    
Epoch: [23][   40/  207]    Overall Loss 0.205376    Objective Loss 0.205376                                        LR 0.001000    Time 0.085228    
Epoch: [23][   50/  207]    Overall Loss 0.203608    Objective Loss 0.203608                                        LR 0.001000    Time 0.083010    
Epoch: [23][   60/  207]    Overall Loss 0.199263    Objective Loss 0.199263                                        LR 0.001000    Time 0.081286    
Epoch: [23][   70/  207]    Overall Loss 0.200283    Objective Loss 0.200283                                        LR 0.001000    Time 0.080013    
Epoch: [23][   80/  207]    Overall Loss 0.204318    Objective Loss 0.204318                                        LR 0.001000    Time 0.079175    
Epoch: [23][   90/  207]    Overall Loss 0.208070    Objective Loss 0.208070                                        LR 0.001000    Time 0.078560    
Epoch: [23][  100/  207]    Overall Loss 0.209383    Objective Loss 0.209383                                        LR 0.001000    Time 0.077991    
Epoch: [23][  110/  207]    Overall Loss 0.208898    Objective Loss 0.208898                                        LR 0.001000    Time 0.077385    
Epoch: [23][  120/  207]    Overall Loss 0.210184    Objective Loss 0.210184                                        LR 0.001000    Time 0.076839    
Epoch: [23][  130/  207]    Overall Loss 0.212338    Objective Loss 0.212338                                        LR 0.001000    Time 0.076335    
Epoch: [23][  140/  207]    Overall Loss 0.209918    Objective Loss 0.209918                                        LR 0.001000    Time 0.075950    
Epoch: [23][  150/  207]    Overall Loss 0.208876    Objective Loss 0.208876                                        LR 0.001000    Time 0.075678    
Epoch: [23][  160/  207]    Overall Loss 0.207498    Objective Loss 0.207498                                        LR 0.001000    Time 0.075489    
Epoch: [23][  170/  207]    Overall Loss 0.208255    Objective Loss 0.208255                                        LR 0.001000    Time 0.075215    
Epoch: [23][  180/  207]    Overall Loss 0.209358    Objective Loss 0.209358                                        LR 0.001000    Time 0.075102    
Epoch: [23][  190/  207]    Overall Loss 0.209486    Objective Loss 0.209486                                        LR 0.001000    Time 0.074924    
Epoch: [23][  200/  207]    Overall Loss 0.210044    Objective Loss 0.210044                                        LR 0.001000    Time 0.074798    
Epoch: [23][  207/  207]    Overall Loss 0.210716    Objective Loss 0.210716    Top1 92.298981    Top5 100.000000    LR 0.001000    Time 0.074777    
--- validate (epoch=23)-----------
5136 samples (512 per mini-batch)
Epoch: [23][   10/   11]    Loss 0.458988    Top1 77.949219    Top5 99.375000    
Epoch: [23][   11/   11]    Loss 0.429378    Top1 77.978972    Top5 99.376947    
==> Top1: 77.979    Top5: 99.377    Loss: 0.429

==> Confusion:
[[266   4  10   0   0  12   0   8]
 [  3 243  48   2   0   4   0   0]
 [  0  19 275   1   0   5   0   0]
 [  0   4   1 719  89  18   5   1]
 [  2   0   1  32 807  23   7   7]
 [  5   7  11   8  22 828   9   4]
 [  2   0   0   6  56  37 726  10]
 [ 47  27  37  67 110 327  33 141]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [24][   10/  207]    Overall Loss 0.200254    Objective Loss 0.200254                                        LR 0.001000    Time 0.130293    
Epoch: [24][   20/  207]    Overall Loss 0.198802    Objective Loss 0.198802                                        LR 0.001000    Time 0.101446    
Epoch: [24][   30/  207]    Overall Loss 0.191094    Objective Loss 0.191094                                        LR 0.001000    Time 0.092081    
Epoch: [24][   40/  207]    Overall Loss 0.183423    Objective Loss 0.183423                                        LR 0.001000    Time 0.087268    
Epoch: [24][   50/  207]    Overall Loss 0.180038    Objective Loss 0.180038                                        LR 0.001000    Time 0.084549    
Epoch: [24][   60/  207]    Overall Loss 0.179304    Objective Loss 0.179304                                        LR 0.001000    Time 0.082603    
Epoch: [24][   70/  207]    Overall Loss 0.181958    Objective Loss 0.181958                                        LR 0.001000    Time 0.081125    
Epoch: [24][   80/  207]    Overall Loss 0.182671    Objective Loss 0.182671                                        LR 0.001000    Time 0.079752    
Epoch: [24][   90/  207]    Overall Loss 0.182262    Objective Loss 0.182262                                        LR 0.001000    Time 0.078868    
Epoch: [24][  100/  207]    Overall Loss 0.186201    Objective Loss 0.186201                                        LR 0.001000    Time 0.078129    
Epoch: [24][  110/  207]    Overall Loss 0.191145    Objective Loss 0.191145                                        LR 0.001000    Time 0.077700    
Epoch: [24][  120/  207]    Overall Loss 0.194076    Objective Loss 0.194076                                        LR 0.001000    Time 0.077168    
Epoch: [24][  130/  207]    Overall Loss 0.195612    Objective Loss 0.195612                                        LR 0.001000    Time 0.076617    
Epoch: [24][  140/  207]    Overall Loss 0.197709    Objective Loss 0.197709                                        LR 0.001000    Time 0.076205    
Epoch: [24][  150/  207]    Overall Loss 0.198956    Objective Loss 0.198956                                        LR 0.001000    Time 0.075849    
Epoch: [24][  160/  207]    Overall Loss 0.201845    Objective Loss 0.201845                                        LR 0.001000    Time 0.075501    
Epoch: [24][  170/  207]    Overall Loss 0.203189    Objective Loss 0.203189                                        LR 0.001000    Time 0.075295    
Epoch: [24][  180/  207]    Overall Loss 0.203459    Objective Loss 0.203459                                        LR 0.001000    Time 0.075193    
Epoch: [24][  190/  207]    Overall Loss 0.204666    Objective Loss 0.204666                                        LR 0.001000    Time 0.074995    
Epoch: [24][  200/  207]    Overall Loss 0.206823    Objective Loss 0.206823                                        LR 0.001000    Time 0.074736    
Epoch: [24][  207/  207]    Overall Loss 0.207128    Objective Loss 0.207128    Top1 92.072480    Top5 100.000000    LR 0.001000    Time 0.074495    
--- validate (epoch=24)-----------
5136 samples (512 per mini-batch)
Epoch: [24][   10/   11]    Loss 0.458914    Top1 78.906250    Top5 99.843750    
Epoch: [24][   11/   11]    Loss 0.434045    Top1 78.874611    Top5 99.844237    
==> Top1: 78.875    Top5: 99.844    Loss: 0.434

==> Confusion:
[[275   1   5   0   1   2   2  14]
 [  5 236  52   0   0   1   0   6]
 [  6  15 273   0   0   1   0   5]
 [  0   4   1 730  62   9  23   8]
 [  2   0   0  58 763   7  26  23]
 [ 29   5  21  15  24 757  21  22]
 [  4   1   0   4  29  18 763  18]
 [ 77  26  38  71  80 179  64 254]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [25][   10/  207]    Overall Loss 0.203687    Objective Loss 0.203687                                        LR 0.001000    Time 0.129433    
Epoch: [25][   20/  207]    Overall Loss 0.211217    Objective Loss 0.211217                                        LR 0.001000    Time 0.100824    
Epoch: [25][   30/  207]    Overall Loss 0.209763    Objective Loss 0.209763                                        LR 0.001000    Time 0.091462    
Epoch: [25][   40/  207]    Overall Loss 0.213729    Objective Loss 0.213729                                        LR 0.001000    Time 0.086629    
Epoch: [25][   50/  207]    Overall Loss 0.215228    Objective Loss 0.215228                                        LR 0.001000    Time 0.083438    
Epoch: [25][   60/  207]    Overall Loss 0.211034    Objective Loss 0.211034                                        LR 0.001000    Time 0.081383    
Epoch: [25][   70/  207]    Overall Loss 0.212838    Objective Loss 0.212838                                        LR 0.001000    Time 0.079893    
Epoch: [25][   80/  207]    Overall Loss 0.213788    Objective Loss 0.213788                                        LR 0.001000    Time 0.078874    
Epoch: [25][   90/  207]    Overall Loss 0.213433    Objective Loss 0.213433                                        LR 0.001000    Time 0.078142    
Epoch: [25][  100/  207]    Overall Loss 0.213220    Objective Loss 0.213220                                        LR 0.001000    Time 0.077307    
Epoch: [25][  110/  207]    Overall Loss 0.213106    Objective Loss 0.213106                                        LR 0.001000    Time 0.076595    
Epoch: [25][  120/  207]    Overall Loss 0.211872    Objective Loss 0.211872                                        LR 0.001000    Time 0.076275    
Epoch: [25][  130/  207]    Overall Loss 0.210049    Objective Loss 0.210049                                        LR 0.001000    Time 0.075925    
Epoch: [25][  140/  207]    Overall Loss 0.210579    Objective Loss 0.210579                                        LR 0.001000    Time 0.075584    
Epoch: [25][  150/  207]    Overall Loss 0.209692    Objective Loss 0.209692                                        LR 0.001000    Time 0.075592    
Epoch: [25][  160/  207]    Overall Loss 0.209079    Objective Loss 0.209079                                        LR 0.001000    Time 0.075265    
Epoch: [25][  170/  207]    Overall Loss 0.209297    Objective Loss 0.209297                                        LR 0.001000    Time 0.074989    
Epoch: [25][  180/  207]    Overall Loss 0.209232    Objective Loss 0.209232                                        LR 0.001000    Time 0.074784    
Epoch: [25][  190/  207]    Overall Loss 0.209723    Objective Loss 0.209723                                        LR 0.001000    Time 0.074635    
Epoch: [25][  200/  207]    Overall Loss 0.208570    Objective Loss 0.208570                                        LR 0.001000    Time 0.074431    
Epoch: [25][  207/  207]    Overall Loss 0.208682    Objective Loss 0.208682    Top1 88.674972    Top5 99.773499    LR 0.001000    Time 0.074178    
--- validate (epoch=25)-----------
5136 samples (512 per mini-batch)
Epoch: [25][   10/   11]    Loss 0.484194    Top1 78.398438    Top5 99.667969    
Epoch: [25][   11/   11]    Loss 0.503791    Top1 78.368380    Top5 99.669003    
==> Top1: 78.368    Top5: 99.669    Loss: 0.504

==> Confusion:
[[266   3   4   3   2   5   4  13]
 [  4 261  25   3   0   1   0   6]
 [  2  25 256   2   1   3   0  11]
 [  0   2   0 752  74   7   2   0]
 [  2   0   0  66 785   4  10  12]
 [  9   5  12  37  35 753  23  20]
 [  2   0   0   7  72  15 726  15]
 [ 45  23  17 128  99 191  60 226]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [26][   10/  207]    Overall Loss 0.220929    Objective Loss 0.220929                                        LR 0.001000    Time 0.126851    
Epoch: [26][   20/  207]    Overall Loss 0.220266    Objective Loss 0.220266                                        LR 0.001000    Time 0.099364    
Epoch: [26][   30/  207]    Overall Loss 0.213934    Objective Loss 0.213934                                        LR 0.001000    Time 0.090468    
Epoch: [26][   40/  207]    Overall Loss 0.210324    Objective Loss 0.210324                                        LR 0.001000    Time 0.085296    
Epoch: [26][   50/  207]    Overall Loss 0.211799    Objective Loss 0.211799                                        LR 0.001000    Time 0.082666    
Epoch: [26][   60/  207]    Overall Loss 0.212058    Objective Loss 0.212058                                        LR 0.001000    Time 0.080845    
Epoch: [26][   70/  207]    Overall Loss 0.209950    Objective Loss 0.209950                                        LR 0.001000    Time 0.079264    
Epoch: [26][   80/  207]    Overall Loss 0.210956    Objective Loss 0.210956                                        LR 0.001000    Time 0.078162    
Epoch: [26][   90/  207]    Overall Loss 0.211492    Objective Loss 0.211492                                        LR 0.001000    Time 0.077568    
Epoch: [26][  100/  207]    Overall Loss 0.208065    Objective Loss 0.208065                                        LR 0.001000    Time 0.076887    
Epoch: [26][  110/  207]    Overall Loss 0.206233    Objective Loss 0.206233                                        LR 0.001000    Time 0.076314    
Epoch: [26][  120/  207]    Overall Loss 0.206345    Objective Loss 0.206345                                        LR 0.001000    Time 0.075833    
Epoch: [26][  130/  207]    Overall Loss 0.205409    Objective Loss 0.205409                                        LR 0.001000    Time 0.075533    
Epoch: [26][  140/  207]    Overall Loss 0.204303    Objective Loss 0.204303                                        LR 0.001000    Time 0.075313    
Epoch: [26][  150/  207]    Overall Loss 0.202096    Objective Loss 0.202096                                        LR 0.001000    Time 0.075115    
Epoch: [26][  160/  207]    Overall Loss 0.200035    Objective Loss 0.200035                                        LR 0.001000    Time 0.074993    
Epoch: [26][  170/  207]    Overall Loss 0.198249    Objective Loss 0.198249                                        LR 0.001000    Time 0.074827    
Epoch: [26][  180/  207]    Overall Loss 0.197521    Objective Loss 0.197521                                        LR 0.001000    Time 0.074589    
Epoch: [26][  190/  207]    Overall Loss 0.195951    Objective Loss 0.195951                                        LR 0.001000    Time 0.074394    
Epoch: [26][  200/  207]    Overall Loss 0.194758    Objective Loss 0.194758                                        LR 0.001000    Time 0.074254    
Epoch: [26][  207/  207]    Overall Loss 0.195799    Objective Loss 0.195799    Top1 86.409966    Top5 100.000000    LR 0.001000    Time 0.074021    
--- validate (epoch=26)-----------
5136 samples (512 per mini-batch)
Epoch: [26][   10/   11]    Loss 0.446914    Top1 77.656250    Top5 99.375000    
Epoch: [26][   11/   11]    Loss 0.438385    Top1 77.667445    Top5 99.376947    
==> Top1: 77.667    Top5: 99.377    Loss: 0.438

==> Confusion:
[[258   9  12   1   0  13   0   7]
 [  1 248  48   0   0   3   0   0]
 [  1  22 274   0   0   3   0   0]
 [  0  11   1 717  55  29  15   9]
 [  2   3   0  48 761  36  21   8]
 [  4   7  19   5  13 835   7   4]
 [  2   2   1   4  30  38 754   6]
 [ 56  40  41  51  79 327  53 142]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [27][   10/  207]    Overall Loss 0.192822    Objective Loss 0.192822                                        LR 0.001000    Time 0.129591    
Epoch: [27][   20/  207]    Overall Loss 0.181300    Objective Loss 0.181300                                        LR 0.001000    Time 0.099765    
Epoch: [27][   30/  207]    Overall Loss 0.178457    Objective Loss 0.178457                                        LR 0.001000    Time 0.089930    
Epoch: [27][   40/  207]    Overall Loss 0.186196    Objective Loss 0.186196                                        LR 0.001000    Time 0.085939    
Epoch: [27][   50/  207]    Overall Loss 0.195752    Objective Loss 0.195752                                        LR 0.001000    Time 0.083335    
Epoch: [27][   60/  207]    Overall Loss 0.199893    Objective Loss 0.199893                                        LR 0.001000    Time 0.081205    
Epoch: [27][   70/  207]    Overall Loss 0.200191    Objective Loss 0.200191                                        LR 0.001000    Time 0.079657    
Epoch: [27][   80/  207]    Overall Loss 0.199975    Objective Loss 0.199975                                        LR 0.001000    Time 0.078492    
Epoch: [27][   90/  207]    Overall Loss 0.196008    Objective Loss 0.196008                                        LR 0.001000    Time 0.077943    
Epoch: [27][  100/  207]    Overall Loss 0.196834    Objective Loss 0.196834                                        LR 0.001000    Time 0.077171    
Epoch: [27][  110/  207]    Overall Loss 0.196749    Objective Loss 0.196749                                        LR 0.001000    Time 0.076711    
Epoch: [27][  120/  207]    Overall Loss 0.195784    Objective Loss 0.195784                                        LR 0.001000    Time 0.076301    
Epoch: [27][  130/  207]    Overall Loss 0.194562    Objective Loss 0.194562                                        LR 0.001000    Time 0.075934    
Epoch: [27][  140/  207]    Overall Loss 0.193583    Objective Loss 0.193583                                        LR 0.001000    Time 0.075575    
Epoch: [27][  150/  207]    Overall Loss 0.192695    Objective Loss 0.192695                                        LR 0.001000    Time 0.075401    
Epoch: [27][  160/  207]    Overall Loss 0.191511    Objective Loss 0.191511                                        LR 0.001000    Time 0.075158    
Epoch: [27][  170/  207]    Overall Loss 0.191873    Objective Loss 0.191873                                        LR 0.001000    Time 0.075003    
Epoch: [27][  180/  207]    Overall Loss 0.191106    Objective Loss 0.191106                                        LR 0.001000    Time 0.074821    
Epoch: [27][  190/  207]    Overall Loss 0.190998    Objective Loss 0.190998                                        LR 0.001000    Time 0.074651    
Epoch: [27][  200/  207]    Overall Loss 0.190612    Objective Loss 0.190612                                        LR 0.001000    Time 0.074451    
Epoch: [27][  207/  207]    Overall Loss 0.191312    Objective Loss 0.191312    Top1 89.694224    Top5 99.546999    LR 0.001000    Time 0.074218    
--- validate (epoch=27)-----------
5136 samples (512 per mini-batch)
Epoch: [27][   10/   11]    Loss 0.462782    Top1 79.375000    Top5 99.687500    
Epoch: [27][   11/   11]    Loss 0.460120    Top1 79.361371    Top5 99.688474    
==> Top1: 79.361    Top5: 99.688    Loss: 0.460

==> Confusion:
[[288   3   1   0   0   0   2   6]
 [ 13 262  21   0   0   1   0   3]
 [ 10  26 255   0   0   2   0   7]
 [  2   7   0 700  71  15  20  22]
 [  6   0   0  35 767   8  36  27]
 [ 47   6  11   8  16 776   9  21]
 [  6   0   0   2  27  27 747  28]
 [102  33  17  46  56 218  36 281]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [28][   10/  207]    Overall Loss 0.174433    Objective Loss 0.174433                                        LR 0.001000    Time 0.128743    
Epoch: [28][   20/  207]    Overall Loss 0.180138    Objective Loss 0.180138                                        LR 0.001000    Time 0.100506    
Epoch: [28][   30/  207]    Overall Loss 0.178391    Objective Loss 0.178391                                        LR 0.001000    Time 0.091726    
Epoch: [28][   40/  207]    Overall Loss 0.175842    Objective Loss 0.175842                                        LR 0.001000    Time 0.086325    
Epoch: [28][   50/  207]    Overall Loss 0.175954    Objective Loss 0.175954                                        LR 0.001000    Time 0.083742    
Epoch: [28][   60/  207]    Overall Loss 0.175019    Objective Loss 0.175019                                        LR 0.001000    Time 0.081906    
Epoch: [28][   70/  207]    Overall Loss 0.174044    Objective Loss 0.174044                                        LR 0.001000    Time 0.080457    
Epoch: [28][   80/  207]    Overall Loss 0.173875    Objective Loss 0.173875                                        LR 0.001000    Time 0.079647    
Epoch: [28][   90/  207]    Overall Loss 0.171418    Objective Loss 0.171418                                        LR 0.001000    Time 0.078670    
Epoch: [28][  100/  207]    Overall Loss 0.170142    Objective Loss 0.170142                                        LR 0.001000    Time 0.077964    
Epoch: [28][  110/  207]    Overall Loss 0.169591    Objective Loss 0.169591                                        LR 0.001000    Time 0.077383    
Epoch: [28][  120/  207]    Overall Loss 0.169765    Objective Loss 0.169765                                        LR 0.001000    Time 0.076921    
Epoch: [28][  130/  207]    Overall Loss 0.173436    Objective Loss 0.173436                                        LR 0.001000    Time 0.076516    
Epoch: [28][  140/  207]    Overall Loss 0.173646    Objective Loss 0.173646                                        LR 0.001000    Time 0.076125    
Epoch: [28][  150/  207]    Overall Loss 0.173479    Objective Loss 0.173479                                        LR 0.001000    Time 0.075837    
Epoch: [28][  160/  207]    Overall Loss 0.173652    Objective Loss 0.173652                                        LR 0.001000    Time 0.075543    
Epoch: [28][  170/  207]    Overall Loss 0.173337    Objective Loss 0.173337                                        LR 0.001000    Time 0.075307    
Epoch: [28][  180/  207]    Overall Loss 0.172571    Objective Loss 0.172571                                        LR 0.001000    Time 0.075192    
Epoch: [28][  190/  207]    Overall Loss 0.172835    Objective Loss 0.172835                                        LR 0.001000    Time 0.075014    
Epoch: [28][  200/  207]    Overall Loss 0.172887    Objective Loss 0.172887                                        LR 0.001000    Time 0.074790    
Epoch: [28][  207/  207]    Overall Loss 0.173118    Objective Loss 0.173118    Top1 88.901472    Top5 99.773499    LR 0.001000    Time 0.074536    
--- validate (epoch=28)-----------
5136 samples (512 per mini-batch)
Epoch: [28][   10/   11]    Loss 0.430874    Top1 79.414062    Top5 99.726562    
Epoch: [28][   11/   11]    Loss 0.417038    Top1 79.419782    Top5 99.727414    
==> Top1: 79.420    Top5: 99.727    Loss: 0.417

==> Confusion:
[[278   5   2   0   1   1   2  11]
 [  5 261  27   2   1   1   0   3]
 [  3  27 255   1   1   6   0   7]
 [  1   4   0 723  72  13  19   5]
 [  4   0   0  39 768   6  46  16]
 [ 22   6   9  16  26 747  43  25]
 [  2   0   0   5  18  11 788  13]
 [ 86  20  17  69  81 171  86 259]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [29][   10/  207]    Overall Loss 0.161548    Objective Loss 0.161548                                        LR 0.001000    Time 0.125494    
Epoch: [29][   20/  207]    Overall Loss 0.150101    Objective Loss 0.150101                                        LR 0.001000    Time 0.097569    
Epoch: [29][   30/  207]    Overall Loss 0.148652    Objective Loss 0.148652                                        LR 0.001000    Time 0.088640    
Epoch: [29][   40/  207]    Overall Loss 0.154428    Objective Loss 0.154428                                        LR 0.001000    Time 0.084565    
Epoch: [29][   50/  207]    Overall Loss 0.163944    Objective Loss 0.163944                                        LR 0.001000    Time 0.081986    
Epoch: [29][   60/  207]    Overall Loss 0.167038    Objective Loss 0.167038                                        LR 0.001000    Time 0.080749    
Epoch: [29][   70/  207]    Overall Loss 0.168328    Objective Loss 0.168328                                        LR 0.001000    Time 0.079315    
Epoch: [29][   80/  207]    Overall Loss 0.170568    Objective Loss 0.170568                                        LR 0.001000    Time 0.078407    
Epoch: [29][   90/  207]    Overall Loss 0.170388    Objective Loss 0.170388                                        LR 0.001000    Time 0.077553    
Epoch: [29][  100/  207]    Overall Loss 0.171923    Objective Loss 0.171923                                        LR 0.001000    Time 0.076744    
Epoch: [29][  110/  207]    Overall Loss 0.172246    Objective Loss 0.172246                                        LR 0.001000    Time 0.076383    
Epoch: [29][  120/  207]    Overall Loss 0.172878    Objective Loss 0.172878                                        LR 0.001000    Time 0.075814    
Epoch: [29][  130/  207]    Overall Loss 0.172808    Objective Loss 0.172808                                        LR 0.001000    Time 0.075431    
Epoch: [29][  140/  207]    Overall Loss 0.174358    Objective Loss 0.174358                                        LR 0.001000    Time 0.075161    
Epoch: [29][  150/  207]    Overall Loss 0.174893    Objective Loss 0.174893                                        LR 0.001000    Time 0.074999    
Epoch: [29][  160/  207]    Overall Loss 0.176109    Objective Loss 0.176109                                        LR 0.001000    Time 0.074722    
Epoch: [29][  170/  207]    Overall Loss 0.177199    Objective Loss 0.177199                                        LR 0.001000    Time 0.074510    
Epoch: [29][  180/  207]    Overall Loss 0.177895    Objective Loss 0.177895                                        LR 0.001000    Time 0.074346    
Epoch: [29][  190/  207]    Overall Loss 0.178020    Objective Loss 0.178020                                        LR 0.001000    Time 0.074202    
Epoch: [29][  200/  207]    Overall Loss 0.177923    Objective Loss 0.177923                                        LR 0.001000    Time 0.074150    
Epoch: [29][  207/  207]    Overall Loss 0.177578    Objective Loss 0.177578    Top1 91.166478    Top5 100.000000    LR 0.001000    Time 0.073915    
--- validate (epoch=29)-----------
5136 samples (512 per mini-batch)
Epoch: [29][   10/   11]    Loss 0.428471    Top1 78.574219    Top5 99.667969    
Epoch: [29][   11/   11]    Loss 0.400069    Top1 78.621495    Top5 99.669003    
==> Top1: 78.621    Top5: 99.669    Loss: 0.400

==> Confusion:
[[276   4   7   1   0   2   3   7]
 [  5 269  18   3   0   1   0   4]
 [  8  28 256   1   2   2   0   3]
 [  0   4   0 723  90   6  10   4]
 [  2   0   0  33 806   4  26   8]
 [ 16   6  15  20  44 753  28  12]
 [  2   0   0   4  50  11 762   8]
 [ 79  24  22  83 109 207  72 193]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [30][   10/  207]    Overall Loss 0.182159    Objective Loss 0.182159                                        LR 0.001000    Time 0.141813    
Epoch: [30][   20/  207]    Overall Loss 0.181364    Objective Loss 0.181364                                        LR 0.001000    Time 0.107190    
Epoch: [30][   30/  207]    Overall Loss 0.171985    Objective Loss 0.171985                                        LR 0.001000    Time 0.095121    
Epoch: [30][   40/  207]    Overall Loss 0.167002    Objective Loss 0.167002                                        LR 0.001000    Time 0.089620    
Epoch: [30][   50/  207]    Overall Loss 0.171153    Objective Loss 0.171153                                        LR 0.001000    Time 0.089767    
Epoch: [30][   60/  207]    Overall Loss 0.171722    Objective Loss 0.171722                                        LR 0.001000    Time 0.094438    
Epoch: [30][   70/  207]    Overall Loss 0.168682    Objective Loss 0.168682                                        LR 0.001000    Time 0.091275    
Epoch: [30][   80/  207]    Overall Loss 0.167737    Objective Loss 0.167737                                        LR 0.001000    Time 0.088906    
Epoch: [30][   90/  207]    Overall Loss 0.166849    Objective Loss 0.166849                                        LR 0.001000    Time 0.087549    
Epoch: [30][  100/  207]    Overall Loss 0.168104    Objective Loss 0.168104                                        LR 0.001000    Time 0.086602    
Epoch: [30][  110/  207]    Overall Loss 0.170826    Objective Loss 0.170826                                        LR 0.001000    Time 0.085464    
Epoch: [30][  120/  207]    Overall Loss 0.174445    Objective Loss 0.174445                                        LR 0.001000    Time 0.084491    
Epoch: [30][  130/  207]    Overall Loss 0.178045    Objective Loss 0.178045                                        LR 0.001000    Time 0.083561    
Epoch: [30][  140/  207]    Overall Loss 0.179969    Objective Loss 0.179969                                        LR 0.001000    Time 0.084689    
Epoch: [30][  150/  207]    Overall Loss 0.180501    Objective Loss 0.180501                                        LR 0.001000    Time 0.085131    
Epoch: [30][  160/  207]    Overall Loss 0.180988    Objective Loss 0.180988                                        LR 0.001000    Time 0.084876    
Epoch: [30][  170/  207]    Overall Loss 0.182034    Objective Loss 0.182034                                        LR 0.001000    Time 0.084283    
Epoch: [30][  180/  207]    Overall Loss 0.182886    Objective Loss 0.182886                                        LR 0.001000    Time 0.083997    
Epoch: [30][  190/  207]    Overall Loss 0.183929    Objective Loss 0.183929                                        LR 0.001000    Time 0.083564    
Epoch: [30][  200/  207]    Overall Loss 0.183313    Objective Loss 0.183313                                        LR 0.001000    Time 0.083086    
Epoch: [30][  207/  207]    Overall Loss 0.183887    Objective Loss 0.183887    Top1 87.315968    Top5 100.000000    LR 0.001000    Time 0.082601    
--- validate (epoch=30)-----------
5136 samples (512 per mini-batch)
Epoch: [30][   10/   11]    Loss 0.550221    Top1 77.246094    Top5 99.492188    
Epoch: [30][   11/   11]    Loss 0.583019    Top1 77.278037    Top5 99.493769    
==> Top1: 77.278    Top5: 99.494    Loss: 0.583

==> Confusion:
[[250   6  15   0   3  12   2  12]
 [  1 202  92   0   0   3   0   2]
 [  2  10 280   0   1   7   0   0]
 [  0   2   2 706 109  11   5   2]
 [  1   0   0  25 834   5   7   7]
 [  9   4  16  13  49 778  16   9]
 [  2   0   0   5  80  16 722  12]
 [ 40  22  47  73 135 248  27 197]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [31][   10/  207]    Overall Loss 0.200119    Objective Loss 0.200119                                        LR 0.001000    Time 0.131452    
Epoch: [31][   20/  207]    Overall Loss 0.179394    Objective Loss 0.179394                                        LR 0.001000    Time 0.101238    
Epoch: [31][   30/  207]    Overall Loss 0.170031    Objective Loss 0.170031                                        LR 0.001000    Time 0.091673    
Epoch: [31][   40/  207]    Overall Loss 0.162053    Objective Loss 0.162053                                        LR 0.001000    Time 0.086200    
Epoch: [31][   50/  207]    Overall Loss 0.157979    Objective Loss 0.157979                                        LR 0.001000    Time 0.083174    
Epoch: [31][   60/  207]    Overall Loss 0.153692    Objective Loss 0.153692                                        LR 0.001000    Time 0.081438    
Epoch: [31][   70/  207]    Overall Loss 0.153528    Objective Loss 0.153528                                        LR 0.001000    Time 0.080369    
Epoch: [31][   80/  207]    Overall Loss 0.152430    Objective Loss 0.152430                                        LR 0.001000    Time 0.079311    
Epoch: [31][   90/  207]    Overall Loss 0.150328    Objective Loss 0.150328                                        LR 0.001000    Time 0.078355    
Epoch: [31][  100/  207]    Overall Loss 0.152498    Objective Loss 0.152498                                        LR 0.001000    Time 0.077464    
Epoch: [31][  110/  207]    Overall Loss 0.154262    Objective Loss 0.154262                                        LR 0.001000    Time 0.077058    
Epoch: [31][  120/  207]    Overall Loss 0.154267    Objective Loss 0.154267                                        LR 0.001000    Time 0.076557    
Epoch: [31][  130/  207]    Overall Loss 0.154341    Objective Loss 0.154341                                        LR 0.001000    Time 0.076148    
Epoch: [31][  140/  207]    Overall Loss 0.155027    Objective Loss 0.155027                                        LR 0.001000    Time 0.075731    
Epoch: [31][  150/  207]    Overall Loss 0.154661    Objective Loss 0.154661                                        LR 0.001000    Time 0.075521    
Epoch: [31][  160/  207]    Overall Loss 0.155718    Objective Loss 0.155718                                        LR 0.001000    Time 0.075183    
Epoch: [31][  170/  207]    Overall Loss 0.156460    Objective Loss 0.156460                                        LR 0.001000    Time 0.074985    
Epoch: [31][  180/  207]    Overall Loss 0.156094    Objective Loss 0.156094                                        LR 0.001000    Time 0.074834    
Epoch: [31][  190/  207]    Overall Loss 0.156522    Objective Loss 0.156522                                        LR 0.001000    Time 0.074733    
Epoch: [31][  200/  207]    Overall Loss 0.157446    Objective Loss 0.157446                                        LR 0.001000    Time 0.074530    
Epoch: [31][  207/  207]    Overall Loss 0.157597    Objective Loss 0.157597    Top1 90.939977    Top5 100.000000    LR 0.001000    Time 0.074956    
--- validate (epoch=31)-----------
5136 samples (512 per mini-batch)
Epoch: [31][   10/   11]    Loss 0.470806    Top1 79.394531    Top5 99.453125    
Epoch: [31][   11/   11]    Loss 0.443118    Top1 79.419782    Top5 99.454829    
==> Top1: 79.420    Top5: 99.455    Loss: 0.443

==> Confusion:
[[270   4   5   0   1   6   3  11]
 [  2 248  41   2   0   3   0   4]
 [  3  20 265   0   0   8   0   4]
 [  0   5   0 747  61   9  14   1]
 [  3   0   0  62 773   3  31   7]
 [  8   5   8  15  21 798  28  11]
 [  2   0   0   6  41  14 769   5]
 [ 49  15  25  97  83 232  79 209]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [32][   10/  207]    Overall Loss 0.143191    Objective Loss 0.143191                                        LR 0.001000    Time 0.135322    
Epoch: [32][   20/  207]    Overall Loss 0.163627    Objective Loss 0.163627                                        LR 0.001000    Time 0.112848    
Epoch: [32][   30/  207]    Overall Loss 0.166802    Objective Loss 0.166802                                        LR 0.001000    Time 0.101188    
Epoch: [32][   40/  207]    Overall Loss 0.173746    Objective Loss 0.173746                                        LR 0.001000    Time 0.096050    
Epoch: [32][   50/  207]    Overall Loss 0.179037    Objective Loss 0.179037                                        LR 0.001000    Time 0.091405    
Epoch: [32][   60/  207]    Overall Loss 0.177947    Objective Loss 0.177947                                        LR 0.001000    Time 0.088185    
Epoch: [32][   70/  207]    Overall Loss 0.177088    Objective Loss 0.177088                                        LR 0.001000    Time 0.086048    
Epoch: [32][   80/  207]    Overall Loss 0.176874    Objective Loss 0.176874                                        LR 0.001000    Time 0.084109    
Epoch: [32][   90/  207]    Overall Loss 0.175146    Objective Loss 0.175146                                        LR 0.001000    Time 0.082916    
Epoch: [32][  100/  207]    Overall Loss 0.172918    Objective Loss 0.172918                                        LR 0.001000    Time 0.082013    
Epoch: [32][  110/  207]    Overall Loss 0.172595    Objective Loss 0.172595                                        LR 0.001000    Time 0.081470    
Epoch: [32][  120/  207]    Overall Loss 0.171501    Objective Loss 0.171501                                        LR 0.001000    Time 0.080884    
Epoch: [32][  130/  207]    Overall Loss 0.169917    Objective Loss 0.169917                                        LR 0.001000    Time 0.080781    
Epoch: [32][  140/  207]    Overall Loss 0.169325    Objective Loss 0.169325                                        LR 0.001000    Time 0.080744    
Epoch: [32][  150/  207]    Overall Loss 0.168708    Objective Loss 0.168708                                        LR 0.001000    Time 0.080785    
Epoch: [32][  160/  207]    Overall Loss 0.169217    Objective Loss 0.169217                                        LR 0.001000    Time 0.080814    
Epoch: [32][  170/  207]    Overall Loss 0.170146    Objective Loss 0.170146                                        LR 0.001000    Time 0.080850    
Epoch: [32][  180/  207]    Overall Loss 0.171322    Objective Loss 0.171322                                        LR 0.001000    Time 0.080772    
Epoch: [32][  190/  207]    Overall Loss 0.171266    Objective Loss 0.171266                                        LR 0.001000    Time 0.080725    
Epoch: [32][  200/  207]    Overall Loss 0.171220    Objective Loss 0.171220                                        LR 0.001000    Time 0.080437    
Epoch: [32][  207/  207]    Overall Loss 0.170455    Objective Loss 0.170455    Top1 91.279728    Top5 100.000000    LR 0.001000    Time 0.080007    
--- validate (epoch=32)-----------
5136 samples (512 per mini-batch)
Epoch: [32][   10/   11]    Loss 0.474426    Top1 80.117188    Top5 99.648438    
Epoch: [32][   11/   11]    Loss 0.524177    Top1 80.081776    Top5 99.649533    
==> Top1: 80.082    Top5: 99.650    Loss: 0.524

==> Confusion:
[[256   4   7   0   1   7   3  22]
 [  4 246  43   2   0   0   0   5]
 [  1  10 276   0   0   3   0  10]
 [  0   4   2 754  57   7   5   8]
 [  1   0   0  67 771  13  14  13]
 [  6   3  15  26  20 784  17  23]
 [  2   0   0   8  41  18 748  20]
 [ 37  25  26 107  76 207  33 278]]

==> Best [Top1: 80.082   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 32]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [33][   10/  207]    Overall Loss 0.123766    Objective Loss 0.123766                                        LR 0.001000    Time 0.138242    
Epoch: [33][   20/  207]    Overall Loss 0.132385    Objective Loss 0.132385                                        LR 0.001000    Time 0.109250    
Epoch: [33][   30/  207]    Overall Loss 0.133403    Objective Loss 0.133403                                        LR 0.001000    Time 0.097790    
Epoch: [33][   40/  207]    Overall Loss 0.132568    Objective Loss 0.132568                                        LR 0.001000    Time 0.091378    
Epoch: [33][   50/  207]    Overall Loss 0.129582    Objective Loss 0.129582                                        LR 0.001000    Time 0.088161    
Epoch: [33][   60/  207]    Overall Loss 0.127173    Objective Loss 0.127173                                        LR 0.001000    Time 0.086312    
Epoch: [33][   70/  207]    Overall Loss 0.126016    Objective Loss 0.126016                                        LR 0.001000    Time 0.084781    
Epoch: [33][   80/  207]    Overall Loss 0.128322    Objective Loss 0.128322                                        LR 0.001000    Time 0.083896    
Epoch: [33][   90/  207]    Overall Loss 0.129690    Objective Loss 0.129690                                        LR 0.001000    Time 0.083162    
Epoch: [33][  100/  207]    Overall Loss 0.134129    Objective Loss 0.134129                                        LR 0.001000    Time 0.083966    
Epoch: [33][  110/  207]    Overall Loss 0.142629    Objective Loss 0.142629                                        LR 0.001000    Time 0.083713    
Epoch: [33][  120/  207]    Overall Loss 0.147417    Objective Loss 0.147417                                        LR 0.001000    Time 0.084561    
Epoch: [33][  130/  207]    Overall Loss 0.148485    Objective Loss 0.148485                                        LR 0.001000    Time 0.083605    
Epoch: [33][  140/  207]    Overall Loss 0.148988    Objective Loss 0.148988                                        LR 0.001000    Time 0.082912    
Epoch: [33][  150/  207]    Overall Loss 0.148471    Objective Loss 0.148471                                        LR 0.001000    Time 0.082997    
Epoch: [33][  160/  207]    Overall Loss 0.147602    Objective Loss 0.147602                                        LR 0.001000    Time 0.083041    
Epoch: [33][  170/  207]    Overall Loss 0.147607    Objective Loss 0.147607                                        LR 0.001000    Time 0.082498    
Epoch: [33][  180/  207]    Overall Loss 0.150302    Objective Loss 0.150302                                        LR 0.001000    Time 0.082217    
Epoch: [33][  190/  207]    Overall Loss 0.151172    Objective Loss 0.151172                                        LR 0.001000    Time 0.081897    
Epoch: [33][  200/  207]    Overall Loss 0.151563    Objective Loss 0.151563                                        LR 0.001000    Time 0.081681    
Epoch: [33][  207/  207]    Overall Loss 0.151758    Objective Loss 0.151758    Top1 93.771234    Top5 99.773499    LR 0.001000    Time 0.081479    
--- validate (epoch=33)-----------
5136 samples (512 per mini-batch)
Epoch: [33][   10/   11]    Loss 0.442147    Top1 80.917969    Top5 99.726562    
Epoch: [33][   11/   11]    Loss 0.613007    Top1 80.899533    Top5 99.727414    
==> Top1: 80.900    Top5: 99.727    Loss: 0.613

==> Confusion:
[[277   5   2   3   0   3   2   8]
 [  4 281  10   1   0   1   0   3]
 [  3  43 253   0   0   0   0   1]
 [  0   5   2 745  46  11  13  15]
 [  3   0   0  53 757   8  34  24]
 [ 16  15  18  13  14 780  10  28]
 [  3   0   0   4  28  22 758  22]
 [ 59  33  23  72  62 190  46 304]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [34][   10/  207]    Overall Loss 0.147299    Objective Loss 0.147299                                        LR 0.001000    Time 0.133128    
Epoch: [34][   20/  207]    Overall Loss 0.144028    Objective Loss 0.144028                                        LR 0.001000    Time 0.105481    
Epoch: [34][   30/  207]    Overall Loss 0.143485    Objective Loss 0.143485                                        LR 0.001000    Time 0.094055    
Epoch: [34][   40/  207]    Overall Loss 0.141154    Objective Loss 0.141154                                        LR 0.001000    Time 0.090841    
Epoch: [34][   50/  207]    Overall Loss 0.143965    Objective Loss 0.143965                                        LR 0.001000    Time 0.088344    
Epoch: [34][   60/  207]    Overall Loss 0.145244    Objective Loss 0.145244                                        LR 0.001000    Time 0.087611    
Epoch: [34][   70/  207]    Overall Loss 0.144763    Objective Loss 0.144763                                        LR 0.001000    Time 0.086225    
Epoch: [34][   80/  207]    Overall Loss 0.145882    Objective Loss 0.145882                                        LR 0.001000    Time 0.084696    
Epoch: [34][   90/  207]    Overall Loss 0.144501    Objective Loss 0.144501                                        LR 0.001000    Time 0.083700    
Epoch: [34][  100/  207]    Overall Loss 0.144261    Objective Loss 0.144261                                        LR 0.001000    Time 0.082660    
Epoch: [34][  110/  207]    Overall Loss 0.144145    Objective Loss 0.144145                                        LR 0.001000    Time 0.081930    
Epoch: [34][  120/  207]    Overall Loss 0.145627    Objective Loss 0.145627                                        LR 0.001000    Time 0.081198    
Epoch: [34][  130/  207]    Overall Loss 0.146069    Objective Loss 0.146069                                        LR 0.001000    Time 0.080834    
Epoch: [34][  140/  207]    Overall Loss 0.148175    Objective Loss 0.148175                                        LR 0.001000    Time 0.080729    
Epoch: [34][  150/  207]    Overall Loss 0.148207    Objective Loss 0.148207                                        LR 0.001000    Time 0.080648    
Epoch: [34][  160/  207]    Overall Loss 0.148714    Objective Loss 0.148714                                        LR 0.001000    Time 0.080672    
Epoch: [34][  170/  207]    Overall Loss 0.147284    Objective Loss 0.147284                                        LR 0.001000    Time 0.080418    
Epoch: [34][  180/  207]    Overall Loss 0.145739    Objective Loss 0.145739                                        LR 0.001000    Time 0.080185    
Epoch: [34][  190/  207]    Overall Loss 0.144806    Objective Loss 0.144806                                        LR 0.001000    Time 0.079903    
Epoch: [34][  200/  207]    Overall Loss 0.145117    Objective Loss 0.145117                                        LR 0.001000    Time 0.079614    
Epoch: [34][  207/  207]    Overall Loss 0.146300    Objective Loss 0.146300    Top1 92.638732    Top5 99.886750    LR 0.001000    Time 0.079236    
--- validate (epoch=34)-----------
5136 samples (512 per mini-batch)
Epoch: [34][   10/   11]    Loss 0.419807    Top1 80.351562    Top5 99.609375    
Epoch: [34][   11/   11]    Loss 0.390852    Top1 80.334891    Top5 99.610592    
==> Top1: 80.335    Top5: 99.611    Loss: 0.391

==> Confusion:
[[269   8   2   2   0   3   4  12]
 [  2 266  28   1   0   1   0   2]
 [  2  21 274   1   0   1   0   1]
 [  0   7   0 755  46   9  15   5]
 [  2   0   0  67 757   9  31  13]
 [ 14   8  29  21  26 763  19  14]
 [  3   0   0   6  21  17 773  17]
 [ 61  29  39  89  76 176  50 269]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [35][   10/  207]    Overall Loss 0.124178    Objective Loss 0.124178                                        LR 0.001000    Time 0.136493    
Epoch: [35][   20/  207]    Overall Loss 0.121033    Objective Loss 0.121033                                        LR 0.001000    Time 0.106447    
Epoch: [35][   30/  207]    Overall Loss 0.120050    Objective Loss 0.120050                                        LR 0.001000    Time 0.097031    
Epoch: [35][   40/  207]    Overall Loss 0.117912    Objective Loss 0.117912                                        LR 0.001000    Time 0.092588    
Epoch: [35][   50/  207]    Overall Loss 0.118823    Objective Loss 0.118823                                        LR 0.001000    Time 0.089925    
Epoch: [35][   60/  207]    Overall Loss 0.118278    Objective Loss 0.118278                                        LR 0.001000    Time 0.087858    
Epoch: [35][   70/  207]    Overall Loss 0.117614    Objective Loss 0.117614                                        LR 0.001000    Time 0.086381    
Epoch: [35][   80/  207]    Overall Loss 0.119740    Objective Loss 0.119740                                        LR 0.001000    Time 0.084699    
Epoch: [35][   90/  207]    Overall Loss 0.121644    Objective Loss 0.121644                                        LR 0.001000    Time 0.084193    
Epoch: [35][  100/  207]    Overall Loss 0.122596    Objective Loss 0.122596                                        LR 0.001000    Time 0.083436    
Epoch: [35][  110/  207]    Overall Loss 0.123418    Objective Loss 0.123418                                        LR 0.001000    Time 0.083207    
Epoch: [35][  120/  207]    Overall Loss 0.123534    Objective Loss 0.123534                                        LR 0.001000    Time 0.082470    
Epoch: [35][  130/  207]    Overall Loss 0.124513    Objective Loss 0.124513                                        LR 0.001000    Time 0.081817    
Epoch: [35][  140/  207]    Overall Loss 0.126094    Objective Loss 0.126094                                        LR 0.001000    Time 0.081460    
Epoch: [35][  150/  207]    Overall Loss 0.127774    Objective Loss 0.127774                                        LR 0.001000    Time 0.081398    
Epoch: [35][  160/  207]    Overall Loss 0.128401    Objective Loss 0.128401                                        LR 0.001000    Time 0.081120    
Epoch: [35][  170/  207]    Overall Loss 0.130135    Objective Loss 0.130135                                        LR 0.001000    Time 0.080901    
Epoch: [35][  180/  207]    Overall Loss 0.131767    Objective Loss 0.131767                                        LR 0.001000    Time 0.080763    
Epoch: [35][  190/  207]    Overall Loss 0.134539    Objective Loss 0.134539                                        LR 0.001000    Time 0.080614    
Epoch: [35][  200/  207]    Overall Loss 0.136956    Objective Loss 0.136956                                        LR 0.001000    Time 0.080408    
Epoch: [35][  207/  207]    Overall Loss 0.138231    Objective Loss 0.138231    Top1 91.732729    Top5 99.660249    LR 0.001000    Time 0.080157    
--- validate (epoch=35)-----------
5136 samples (512 per mini-batch)
Epoch: [35][   10/   11]    Loss 0.530208    Top1 80.839844    Top5 99.687500    
Epoch: [35][   11/   11]    Loss 0.511572    Top1 80.821651    Top5 99.688474    
==> Top1: 80.822    Top5: 99.688    Loss: 0.512

==> Confusion:
[[263   4   3   0   0   5   1  24]
 [  1 254  25   2   0   3   0  15]
 [  2  21 244   1   1  11   0  20]
 [  1   2   1 766  37  10   9  11]
 [  0   0   0  69 749  11  26  24]
 [  2   8   4  20  26 776  24  34]
 [  2   0   0   7  33  12 755  28]
 [ 25  13  17  85  79 187  39 344]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [36][   10/  207]    Overall Loss 0.164752    Objective Loss 0.164752                                        LR 0.001000    Time 0.137877    
Epoch: [36][   20/  207]    Overall Loss 0.175644    Objective Loss 0.175644                                        LR 0.001000    Time 0.105495    
Epoch: [36][   30/  207]    Overall Loss 0.168330    Objective Loss 0.168330                                        LR 0.001000    Time 0.094194    
Epoch: [36][   40/  207]    Overall Loss 0.164854    Objective Loss 0.164854                                        LR 0.001000    Time 0.089968    
Epoch: [36][   50/  207]    Overall Loss 0.165603    Objective Loss 0.165603                                        LR 0.001000    Time 0.086778    
Epoch: [36][   60/  207]    Overall Loss 0.165687    Objective Loss 0.165687                                        LR 0.001000    Time 0.084258    
Epoch: [36][   70/  207]    Overall Loss 0.165577    Objective Loss 0.165577                                        LR 0.001000    Time 0.082478    
Epoch: [36][   80/  207]    Overall Loss 0.166711    Objective Loss 0.166711                                        LR 0.001000    Time 0.081848    
Epoch: [36][   90/  207]    Overall Loss 0.167397    Objective Loss 0.167397                                        LR 0.001000    Time 0.081306    
Epoch: [36][  100/  207]    Overall Loss 0.169120    Objective Loss 0.169120                                        LR 0.001000    Time 0.080811    
Epoch: [36][  110/  207]    Overall Loss 0.169142    Objective Loss 0.169142                                        LR 0.001000    Time 0.080530    
Epoch: [36][  120/  207]    Overall Loss 0.167288    Objective Loss 0.167288                                        LR 0.001000    Time 0.080412    
Epoch: [36][  130/  207]    Overall Loss 0.169278    Objective Loss 0.169278                                        LR 0.001000    Time 0.080181    
Epoch: [36][  140/  207]    Overall Loss 0.169940    Objective Loss 0.169940                                        LR 0.001000    Time 0.079510    
Epoch: [36][  150/  207]    Overall Loss 0.170226    Objective Loss 0.170226                                        LR 0.001000    Time 0.079492    
Epoch: [36][  160/  207]    Overall Loss 0.172229    Objective Loss 0.172229                                        LR 0.001000    Time 0.079394    
Epoch: [36][  170/  207]    Overall Loss 0.172547    Objective Loss 0.172547                                        LR 0.001000    Time 0.079034    
Epoch: [36][  180/  207]    Overall Loss 0.172418    Objective Loss 0.172418                                        LR 0.001000    Time 0.079549    
Epoch: [36][  190/  207]    Overall Loss 0.171810    Objective Loss 0.171810                                        LR 0.001000    Time 0.079421    
Epoch: [36][  200/  207]    Overall Loss 0.173067    Objective Loss 0.173067                                        LR 0.001000    Time 0.079278    
Epoch: [36][  207/  207]    Overall Loss 0.174041    Objective Loss 0.174041    Top1 90.147225    Top5 100.000000    LR 0.001000    Time 0.078883    
--- validate (epoch=36)-----------
5136 samples (512 per mini-batch)
Epoch: [36][   10/   11]    Loss 0.500471    Top1 80.019531    Top5 99.589844    
Epoch: [36][   11/   11]    Loss 0.455474    Top1 80.081776    Top5 99.591121    
==> Top1: 80.082    Top5: 99.591    Loss: 0.455

==> Confusion:
[[259   5   9   1   0   8   4  14]
 [  3 252  38   2   0   1   0   4]
 [  3  24 269   0   0   3   0   1]
 [  0   6   2 723  79  12   5  10]
 [  2   0   3  36 789   8  22  19]
 [  6   5  16  12  29 782  28  16]
 [  2   1   0   5  37  14 765  13]
 [ 33  36  27  63  91 196  69 274]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [37][   10/  207]    Overall Loss 0.165439    Objective Loss 0.165439                                        LR 0.001000    Time 0.127671    
Epoch: [37][   20/  207]    Overall Loss 0.164766    Objective Loss 0.164766                                        LR 0.001000    Time 0.098843    
Epoch: [37][   30/  207]    Overall Loss 0.160570    Objective Loss 0.160570                                        LR 0.001000    Time 0.094339    
Epoch: [37][   40/  207]    Overall Loss 0.161935    Objective Loss 0.161935                                        LR 0.001000    Time 0.089328    
Epoch: [37][   50/  207]    Overall Loss 0.165627    Objective Loss 0.165627                                        LR 0.001000    Time 0.088756    
Epoch: [37][   60/  207]    Overall Loss 0.165435    Objective Loss 0.165435                                        LR 0.001000    Time 0.086197    
Epoch: [37][   70/  207]    Overall Loss 0.164529    Objective Loss 0.164529                                        LR 0.001000    Time 0.085587    
Epoch: [37][   80/  207]    Overall Loss 0.162208    Objective Loss 0.162208                                        LR 0.001000    Time 0.085196    
Epoch: [37][   90/  207]    Overall Loss 0.163624    Objective Loss 0.163624                                        LR 0.001000    Time 0.083921    
Epoch: [37][  100/  207]    Overall Loss 0.161705    Objective Loss 0.161705                                        LR 0.001000    Time 0.082712    
Epoch: [37][  110/  207]    Overall Loss 0.161956    Objective Loss 0.161956                                        LR 0.001000    Time 0.081763    
Epoch: [37][  120/  207]    Overall Loss 0.159914    Objective Loss 0.159914                                        LR 0.001000    Time 0.080975    
Epoch: [37][  130/  207]    Overall Loss 0.158938    Objective Loss 0.158938                                        LR 0.001000    Time 0.080449    
Epoch: [37][  140/  207]    Overall Loss 0.157221    Objective Loss 0.157221                                        LR 0.001000    Time 0.080069    
Epoch: [37][  150/  207]    Overall Loss 0.156921    Objective Loss 0.156921                                        LR 0.001000    Time 0.079608    
Epoch: [37][  160/  207]    Overall Loss 0.155300    Objective Loss 0.155300                                        LR 0.001000    Time 0.079397    
Epoch: [37][  170/  207]    Overall Loss 0.153652    Objective Loss 0.153652                                        LR 0.001000    Time 0.079467    
Epoch: [37][  180/  207]    Overall Loss 0.151728    Objective Loss 0.151728                                        LR 0.001000    Time 0.079755    
Epoch: [37][  190/  207]    Overall Loss 0.151663    Objective Loss 0.151663                                        LR 0.001000    Time 0.081144    
Epoch: [37][  200/  207]    Overall Loss 0.151989    Objective Loss 0.151989                                        LR 0.001000    Time 0.081490    
Epoch: [37][  207/  207]    Overall Loss 0.152489    Objective Loss 0.152489    Top1 91.845980    Top5 100.000000    LR 0.001000    Time 0.081045    
--- validate (epoch=37)-----------
5136 samples (512 per mini-batch)
Epoch: [37][   10/   11]    Loss 0.535132    Top1 79.589844    Top5 99.628906    
Epoch: [37][   11/   11]    Loss 0.511829    Top1 79.614486    Top5 99.630062    
==> Top1: 79.614    Top5: 99.630    Loss: 0.512

==> Confusion:
[[248   8  13   0   0  11   4  16]
 [  1 237  56   1   0   2   0   3]
 [  0  19 275   0   0   6   0   0]
 [  0   4   0 732  60  19  11  11]
 [  0   0   0  40 792   9  27  11]
 [  1   3  12  16  22 808  24   8]
 [  2   0   0   5  39  11 765  15]
 [ 37  39  35  63  98 235  50 232]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [38][   10/  207]    Overall Loss 0.131367    Objective Loss 0.131367                                        LR 0.001000    Time 0.127725    
Epoch: [38][   20/  207]    Overall Loss 0.131558    Objective Loss 0.131558                                        LR 0.001000    Time 0.100870    
Epoch: [38][   30/  207]    Overall Loss 0.129406    Objective Loss 0.129406                                        LR 0.001000    Time 0.092662    
Epoch: [38][   40/  207]    Overall Loss 0.122098    Objective Loss 0.122098                                        LR 0.001000    Time 0.090030    
Epoch: [38][   50/  207]    Overall Loss 0.121184    Objective Loss 0.121184                                        LR 0.001000    Time 0.089584    
Epoch: [38][   60/  207]    Overall Loss 0.121920    Objective Loss 0.121920                                        LR 0.001000    Time 0.088645    
Epoch: [38][   70/  207]    Overall Loss 0.125075    Objective Loss 0.125075                                        LR 0.001000    Time 0.089624    
Epoch: [38][   80/  207]    Overall Loss 0.125649    Objective Loss 0.125649                                        LR 0.001000    Time 0.089077    
Epoch: [38][   90/  207]    Overall Loss 0.125397    Objective Loss 0.125397                                        LR 0.001000    Time 0.088281    
Epoch: [38][  100/  207]    Overall Loss 0.125965    Objective Loss 0.125965                                        LR 0.001000    Time 0.088253    
Epoch: [38][  110/  207]    Overall Loss 0.126350    Objective Loss 0.126350                                        LR 0.001000    Time 0.086980    
Epoch: [38][  120/  207]    Overall Loss 0.126864    Objective Loss 0.126864                                        LR 0.001000    Time 0.085736    
Epoch: [38][  130/  207]    Overall Loss 0.127053    Objective Loss 0.127053                                        LR 0.001000    Time 0.084809    
Epoch: [38][  140/  207]    Overall Loss 0.128494    Objective Loss 0.128494                                        LR 0.001000    Time 0.084181    
Epoch: [38][  150/  207]    Overall Loss 0.131081    Objective Loss 0.131081                                        LR 0.001000    Time 0.083820    
Epoch: [38][  160/  207]    Overall Loss 0.131706    Objective Loss 0.131706                                        LR 0.001000    Time 0.083460    
Epoch: [38][  170/  207]    Overall Loss 0.133332    Objective Loss 0.133332                                        LR 0.001000    Time 0.083174    
Epoch: [38][  180/  207]    Overall Loss 0.135719    Objective Loss 0.135719                                        LR 0.001000    Time 0.082951    
Epoch: [38][  190/  207]    Overall Loss 0.137750    Objective Loss 0.137750                                        LR 0.001000    Time 0.082619    
Epoch: [38][  200/  207]    Overall Loss 0.138935    Objective Loss 0.138935                                        LR 0.001000    Time 0.082235    
Epoch: [38][  207/  207]    Overall Loss 0.140515    Objective Loss 0.140515    Top1 92.072480    Top5 99.886750    LR 0.001000    Time 0.081791    
--- validate (epoch=38)-----------
5136 samples (512 per mini-batch)
Epoch: [38][   10/   11]    Loss 0.439195    Top1 80.585938    Top5 99.667969    
Epoch: [38][   11/   11]    Loss 0.427010    Top1 80.529595    Top5 99.669003    
==> Top1: 80.530    Top5: 99.669    Loss: 0.427

==> Confusion:
[[269   5   2   1   0   7   4  12]
 [  2 274  16   2   0   2   1   3]
 [  2  38 244   1   0   7   0   8]
 [  0   5   1 749  51  12  12   7]
 [  2   2   0  74 752   5  27  17]
 [ 15   3   7  16  15 802  15  21]
 [  4   0   0   8  30  18 760  17]
 [ 62  23  14  95  60 203  46 286]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [39][   10/  207]    Overall Loss 0.128901    Objective Loss 0.128901                                        LR 0.001000    Time 0.132981    
Epoch: [39][   20/  207]    Overall Loss 0.129422    Objective Loss 0.129422                                        LR 0.001000    Time 0.102744    
Epoch: [39][   30/  207]    Overall Loss 0.131220    Objective Loss 0.131220                                        LR 0.001000    Time 0.093013    
Epoch: [39][   40/  207]    Overall Loss 0.135045    Objective Loss 0.135045                                        LR 0.001000    Time 0.088023    
Epoch: [39][   50/  207]    Overall Loss 0.134108    Objective Loss 0.134108                                        LR 0.001000    Time 0.085311    
Epoch: [39][   60/  207]    Overall Loss 0.132101    Objective Loss 0.132101                                        LR 0.001000    Time 0.083314    
Epoch: [39][   70/  207]    Overall Loss 0.131980    Objective Loss 0.131980                                        LR 0.001000    Time 0.082259    
Epoch: [39][   80/  207]    Overall Loss 0.130695    Objective Loss 0.130695                                        LR 0.001000    Time 0.081287    
Epoch: [39][   90/  207]    Overall Loss 0.130158    Objective Loss 0.130158                                        LR 0.001000    Time 0.080442    
Epoch: [39][  100/  207]    Overall Loss 0.129444    Objective Loss 0.129444                                        LR 0.001000    Time 0.079837    
Epoch: [39][  110/  207]    Overall Loss 0.130023    Objective Loss 0.130023                                        LR 0.001000    Time 0.079281    
Epoch: [39][  120/  207]    Overall Loss 0.131368    Objective Loss 0.131368                                        LR 0.001000    Time 0.078777    
Epoch: [39][  130/  207]    Overall Loss 0.132622    Objective Loss 0.132622                                        LR 0.001000    Time 0.078839    
Epoch: [39][  140/  207]    Overall Loss 0.133303    Objective Loss 0.133303                                        LR 0.001000    Time 0.078810    
Epoch: [39][  150/  207]    Overall Loss 0.134131    Objective Loss 0.134131                                        LR 0.001000    Time 0.078491    
Epoch: [39][  160/  207]    Overall Loss 0.133303    Objective Loss 0.133303                                        LR 0.001000    Time 0.078199    
Epoch: [39][  170/  207]    Overall Loss 0.132894    Objective Loss 0.132894                                        LR 0.001000    Time 0.077974    
Epoch: [39][  180/  207]    Overall Loss 0.132135    Objective Loss 0.132135                                        LR 0.001000    Time 0.077932    
Epoch: [39][  190/  207]    Overall Loss 0.131129    Objective Loss 0.131129                                        LR 0.001000    Time 0.077915    
Epoch: [39][  200/  207]    Overall Loss 0.130837    Objective Loss 0.130837                                        LR 0.001000    Time 0.077879    
Epoch: [39][  207/  207]    Overall Loss 0.131322    Objective Loss 0.131322    Top1 91.053228    Top5 100.000000    LR 0.001000    Time 0.077698    
--- validate (epoch=39)-----------
5136 samples (512 per mini-batch)
Epoch: [39][   10/   11]    Loss 0.434241    Top1 80.839844    Top5 99.687500    
Epoch: [39][   11/   11]    Loss 0.426534    Top1 80.821651    Top5 99.688474    
==> Top1: 80.822    Top5: 99.688    Loss: 0.427

==> Confusion:
[[276   6   1   0   0   2   4  11]
 [  3 265  25   1   0   1   0   5]
 [  7  23 258   1   0   7   0   4]
 [  0   5   0 755  50  15   5   7]
 [  3   0   0  59 778  11  16  12]
 [  9   5   8  13  13 828   8  10]
 [  3   1   0   6  42  30 741  14]
 [ 64  25  16  75  90 237  32 250]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [40][   10/  207]    Overall Loss 0.121080    Objective Loss 0.121080                                        LR 0.001000    Time 0.133436    
Epoch: [40][   20/  207]    Overall Loss 0.115262    Objective Loss 0.115262                                        LR 0.001000    Time 0.106240    
Epoch: [40][   30/  207]    Overall Loss 0.113645    Objective Loss 0.113645                                        LR 0.001000    Time 0.096672    
Epoch: [40][   40/  207]    Overall Loss 0.111601    Objective Loss 0.111601                                        LR 0.001000    Time 0.091835    
Epoch: [40][   50/  207]    Overall Loss 0.110690    Objective Loss 0.110690                                        LR 0.001000    Time 0.089279    
Epoch: [40][   60/  207]    Overall Loss 0.110023    Objective Loss 0.110023                                        LR 0.001000    Time 0.087656    
Epoch: [40][   70/  207]    Overall Loss 0.109029    Objective Loss 0.109029                                        LR 0.001000    Time 0.085664    
Epoch: [40][   80/  207]    Overall Loss 0.108475    Objective Loss 0.108475                                        LR 0.001000    Time 0.083985    
Epoch: [40][   90/  207]    Overall Loss 0.107499    Objective Loss 0.107499                                        LR 0.001000    Time 0.082644    
Epoch: [40][  100/  207]    Overall Loss 0.106473    Objective Loss 0.106473                                        LR 0.001000    Time 0.081570    
Epoch: [40][  110/  207]    Overall Loss 0.106556    Objective Loss 0.106556                                        LR 0.001000    Time 0.081525    
Epoch: [40][  120/  207]    Overall Loss 0.106691    Objective Loss 0.106691                                        LR 0.001000    Time 0.081873    
Epoch: [40][  130/  207]    Overall Loss 0.106529    Objective Loss 0.106529                                        LR 0.001000    Time 0.081061    
Epoch: [40][  140/  207]    Overall Loss 0.107144    Objective Loss 0.107144                                        LR 0.001000    Time 0.080519    
Epoch: [40][  150/  207]    Overall Loss 0.107531    Objective Loss 0.107531                                        LR 0.001000    Time 0.080077    
Epoch: [40][  160/  207]    Overall Loss 0.107470    Objective Loss 0.107470                                        LR 0.001000    Time 0.079694    
Epoch: [40][  170/  207]    Overall Loss 0.107958    Objective Loss 0.107958                                        LR 0.001000    Time 0.079293    
Epoch: [40][  180/  207]    Overall Loss 0.108140    Objective Loss 0.108140                                        LR 0.001000    Time 0.078942    
Epoch: [40][  190/  207]    Overall Loss 0.108086    Objective Loss 0.108086                                        LR 0.001000    Time 0.078591    
Epoch: [40][  200/  207]    Overall Loss 0.108333    Objective Loss 0.108333                                        LR 0.001000    Time 0.078275    
Epoch: [40][  207/  207]    Overall Loss 0.108416    Objective Loss 0.108416    Top1 93.771234    Top5 100.000000    LR 0.001000    Time 0.077915    
--- validate (epoch=40)-----------
5136 samples (512 per mini-batch)
Epoch: [40][   10/   11]    Loss 0.513330    Top1 81.113281    Top5 99.589844    
Epoch: [40][   11/   11]    Loss 0.498211    Top1 81.094237    Top5 99.591121    
==> Top1: 81.094    Top5: 99.591    Loss: 0.498

==> Confusion:
[[260   4   6   0   1   6   3  20]
 [  2 260  27   1   0   3   1   6]
 [  3  20 268   0   0   5   0   4]
 [  0   4   0 724  59  18  14  18]
 [  1   0   0  42 782  14  21  19]
 [  6   2  10   8  15 826  13  14]
 [  1   0   0   5  34  25 754  18]
 [ 26  19  25  46  71 263  48 291]]

==> Best [Top1: 81.094   Top5: 99.591   Sparsity:0.00   Params: 117200 on epoch: 40]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [41][   10/  207]    Overall Loss 0.089364    Objective Loss 0.089364                                        LR 0.001000    Time 0.144874    
Epoch: [41][   20/  207]    Overall Loss 0.093013    Objective Loss 0.093013                                        LR 0.001000    Time 0.115501    
Epoch: [41][   30/  207]    Overall Loss 0.095336    Objective Loss 0.095336                                        LR 0.001000    Time 0.107011    
Epoch: [41][   40/  207]    Overall Loss 0.098120    Objective Loss 0.098120                                        LR 0.001000    Time 0.099999    
Epoch: [41][   50/  207]    Overall Loss 0.101779    Objective Loss 0.101779                                        LR 0.001000    Time 0.094708    
Epoch: [41][   60/  207]    Overall Loss 0.101897    Objective Loss 0.101897                                        LR 0.001000    Time 0.090870    
Epoch: [41][   70/  207]    Overall Loss 0.101742    Objective Loss 0.101742                                        LR 0.001000    Time 0.089352    
Epoch: [41][   80/  207]    Overall Loss 0.102649    Objective Loss 0.102649                                        LR 0.001000    Time 0.087322    
Epoch: [41][   90/  207]    Overall Loss 0.103977    Objective Loss 0.103977                                        LR 0.001000    Time 0.085796    
Epoch: [41][  100/  207]    Overall Loss 0.105768    Objective Loss 0.105768                                        LR 0.001000    Time 0.084517    
Epoch: [41][  110/  207]    Overall Loss 0.108835    Objective Loss 0.108835                                        LR 0.001000    Time 0.083381    
Epoch: [41][  120/  207]    Overall Loss 0.109375    Objective Loss 0.109375                                        LR 0.001000    Time 0.082268    
Epoch: [41][  130/  207]    Overall Loss 0.110414    Objective Loss 0.110414                                        LR 0.001000    Time 0.081580    
Epoch: [41][  140/  207]    Overall Loss 0.111278    Objective Loss 0.111278                                        LR 0.001000    Time 0.081041    
Epoch: [41][  150/  207]    Overall Loss 0.111254    Objective Loss 0.111254                                        LR 0.001000    Time 0.080555    
Epoch: [41][  160/  207]    Overall Loss 0.111124    Objective Loss 0.111124                                        LR 0.001000    Time 0.080047    
Epoch: [41][  170/  207]    Overall Loss 0.110887    Objective Loss 0.110887                                        LR 0.001000    Time 0.079554    
Epoch: [41][  180/  207]    Overall Loss 0.110789    Objective Loss 0.110789                                        LR 0.001000    Time 0.079172    
Epoch: [41][  190/  207]    Overall Loss 0.111197    Objective Loss 0.111197                                        LR 0.001000    Time 0.078695    
Epoch: [41][  200/  207]    Overall Loss 0.111587    Objective Loss 0.111587                                        LR 0.001000    Time 0.078333    
Epoch: [41][  207/  207]    Overall Loss 0.111725    Objective Loss 0.111725    Top1 93.884485    Top5 100.000000    LR 0.001000    Time 0.078125    
--- validate (epoch=41)-----------
5136 samples (512 per mini-batch)
Epoch: [41][   10/   11]    Loss 0.519671    Top1 81.875000    Top5 99.667969    
Epoch: [41][   11/   11]    Loss 0.574896    Top1 81.814642    Top5 99.669003    
==> Top1: 81.815    Top5: 99.669    Loss: 0.575

==> Confusion:
[[259   4   3   1   1   5   2  25]
 [  2 253  36   2   0   1   0   6]
 [  1  10 276   1   0   6   0   6]
 [  0   4   0 739  61   8  11  14]
 [  1   0   0  41 795   3  18  21]
 [  5   4  10  18  30 777  13  37]
 [  1   0   0   4  40  13 757  22]
 [ 28  20  22  62  94 169  48 346]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [42][   10/  207]    Overall Loss 0.113886    Objective Loss 0.113886                                        LR 0.001000    Time 0.128466    
Epoch: [42][   20/  207]    Overall Loss 0.103240    Objective Loss 0.103240                                        LR 0.001000    Time 0.100198    
Epoch: [42][   30/  207]    Overall Loss 0.103182    Objective Loss 0.103182                                        LR 0.001000    Time 0.090491    
Epoch: [42][   40/  207]    Overall Loss 0.109009    Objective Loss 0.109009                                        LR 0.001000    Time 0.085904    
Epoch: [42][   50/  207]    Overall Loss 0.115583    Objective Loss 0.115583                                        LR 0.001000    Time 0.083555    
Epoch: [42][   60/  207]    Overall Loss 0.118426    Objective Loss 0.118426                                        LR 0.001000    Time 0.081817    
Epoch: [42][   70/  207]    Overall Loss 0.121715    Objective Loss 0.121715                                        LR 0.001000    Time 0.080445    
Epoch: [42][   80/  207]    Overall Loss 0.123826    Objective Loss 0.123826                                        LR 0.001000    Time 0.079297    
Epoch: [42][   90/  207]    Overall Loss 0.125557    Objective Loss 0.125557                                        LR 0.001000    Time 0.078590    
Epoch: [42][  100/  207]    Overall Loss 0.128369    Objective Loss 0.128369                                        LR 0.001000    Time 0.077848    
Epoch: [42][  110/  207]    Overall Loss 0.129575    Objective Loss 0.129575                                        LR 0.001000    Time 0.077333    
Epoch: [42][  120/  207]    Overall Loss 0.128541    Objective Loss 0.128541                                        LR 0.001000    Time 0.076924    
Epoch: [42][  130/  207]    Overall Loss 0.129990    Objective Loss 0.129990                                        LR 0.001000    Time 0.076467    
Epoch: [42][  140/  207]    Overall Loss 0.133321    Objective Loss 0.133321                                        LR 0.001000    Time 0.076088    
Epoch: [42][  150/  207]    Overall Loss 0.135517    Objective Loss 0.135517                                        LR 0.001000    Time 0.075781    
Epoch: [42][  160/  207]    Overall Loss 0.138349    Objective Loss 0.138349                                        LR 0.001000    Time 0.075579    
Epoch: [42][  170/  207]    Overall Loss 0.137989    Objective Loss 0.137989                                        LR 0.001000    Time 0.075398    
Epoch: [42][  180/  207]    Overall Loss 0.139271    Objective Loss 0.139271                                        LR 0.001000    Time 0.075252    
Epoch: [42][  190/  207]    Overall Loss 0.142697    Objective Loss 0.142697                                        LR 0.001000    Time 0.074989    
Epoch: [42][  200/  207]    Overall Loss 0.144817    Objective Loss 0.144817                                        LR 0.001000    Time 0.074760    
Epoch: [42][  207/  207]    Overall Loss 0.145479    Objective Loss 0.145479    Top1 91.619479    Top5 99.886750    LR 0.001000    Time 0.074510    
--- validate (epoch=42)-----------
5136 samples (512 per mini-batch)
Epoch: [42][   10/   11]    Loss 0.498971    Top1 80.527344    Top5 99.726562    
Epoch: [42][   11/   11]    Loss 0.462806    Top1 80.549065    Top5 99.727414    
==> Top1: 80.549    Top5: 99.727    Loss: 0.463

==> Confusion:
[[268   4   3   0   1   3   4  17]
 [  3 258  28   0   0   1   1   9]
 [  3  21 263   2   0   7   0   4]
 [  1   6   0 729  61  11  13  16]
 [  2   0   0  40 786   7  26  18]
 [  7   6  12  12  24 813   5  15]
 [  3   0   0   3  42  34 732  23]
 [ 48  18  25  65  94 227  24 288]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [43][   10/  207]    Overall Loss 0.122041    Objective Loss 0.122041                                        LR 0.001000    Time 0.127962    
Epoch: [43][   20/  207]    Overall Loss 0.135084    Objective Loss 0.135084                                        LR 0.001000    Time 0.100436    
Epoch: [43][   30/  207]    Overall Loss 0.137134    Objective Loss 0.137134                                        LR 0.001000    Time 0.090715    
Epoch: [43][   40/  207]    Overall Loss 0.133597    Objective Loss 0.133597                                        LR 0.001000    Time 0.085586    
Epoch: [43][   50/  207]    Overall Loss 0.133820    Objective Loss 0.133820                                        LR 0.001000    Time 0.082628    
Epoch: [43][   60/  207]    Overall Loss 0.133199    Objective Loss 0.133199                                        LR 0.001000    Time 0.080604    
Epoch: [43][   70/  207]    Overall Loss 0.135849    Objective Loss 0.135849                                        LR 0.001000    Time 0.079197    
Epoch: [43][   80/  207]    Overall Loss 0.140346    Objective Loss 0.140346                                        LR 0.001000    Time 0.078469    
Epoch: [43][   90/  207]    Overall Loss 0.142824    Objective Loss 0.142824                                        LR 0.001000    Time 0.077946    
Epoch: [43][  100/  207]    Overall Loss 0.143004    Objective Loss 0.143004                                        LR 0.001000    Time 0.077424    
Epoch: [43][  110/  207]    Overall Loss 0.142984    Objective Loss 0.142984                                        LR 0.001000    Time 0.076716    
Epoch: [43][  120/  207]    Overall Loss 0.142492    Objective Loss 0.142492                                        LR 0.001000    Time 0.076295    
Epoch: [43][  130/  207]    Overall Loss 0.142353    Objective Loss 0.142353                                        LR 0.001000    Time 0.075885    
Epoch: [43][  140/  207]    Overall Loss 0.141676    Objective Loss 0.141676                                        LR 0.001000    Time 0.075605    
Epoch: [43][  150/  207]    Overall Loss 0.140468    Objective Loss 0.140468                                        LR 0.001000    Time 0.075266    
Epoch: [43][  160/  207]    Overall Loss 0.139714    Objective Loss 0.139714                                        LR 0.001000    Time 0.074997    
Epoch: [43][  170/  207]    Overall Loss 0.137985    Objective Loss 0.137985                                        LR 0.001000    Time 0.074796    
Epoch: [43][  180/  207]    Overall Loss 0.136799    Objective Loss 0.136799                                        LR 0.001000    Time 0.074744    
Epoch: [43][  190/  207]    Overall Loss 0.137010    Objective Loss 0.137010                                        LR 0.001000    Time 0.074662    
Epoch: [43][  200/  207]    Overall Loss 0.137100    Objective Loss 0.137100                                        LR 0.001000    Time 0.074550    
Epoch: [43][  207/  207]    Overall Loss 0.136895    Objective Loss 0.136895    Top1 92.525481    Top5 100.000000    LR 0.001000    Time 0.074323    
--- validate (epoch=43)-----------
5136 samples (512 per mini-batch)
Epoch: [43][   10/   11]    Loss 0.648635    Top1 79.042969    Top5 99.609375    
Epoch: [43][   11/   11]    Loss 0.608071    Top1 79.088785    Top5 99.610592    
==> Top1: 79.089    Top5: 99.611    Loss: 0.608

==> Confusion:
[[235   5   7   1   0   9  10  33]
 [  1 252  32   1   0   1   2  11]
 [  0  28 256   1   0   4   1  10]
 [  0   4   1 784  26   4   6  12]
 [  1   0   0 142 694   4  20  18]
 [  2   5  14  40  26 737  30  40]
 [  1   0   0  11  42  11 757  15]
 [ 20  18  16 100  70 166  52 347]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [44][   10/  207]    Overall Loss 0.141842    Objective Loss 0.141842                                        LR 0.001000    Time 0.129808    
Epoch: [44][   20/  207]    Overall Loss 0.140286    Objective Loss 0.140286                                        LR 0.001000    Time 0.099883    
Epoch: [44][   30/  207]    Overall Loss 0.148489    Objective Loss 0.148489                                        LR 0.001000    Time 0.090691    
Epoch: [44][   40/  207]    Overall Loss 0.147421    Objective Loss 0.147421                                        LR 0.001000    Time 0.085687    
Epoch: [44][   50/  207]    Overall Loss 0.147540    Objective Loss 0.147540                                        LR 0.001000    Time 0.082682    
Epoch: [44][   60/  207]    Overall Loss 0.141888    Objective Loss 0.141888                                        LR 0.001000    Time 0.080797    
Epoch: [44][   70/  207]    Overall Loss 0.140043    Objective Loss 0.140043                                        LR 0.001000    Time 0.079492    
Epoch: [44][   80/  207]    Overall Loss 0.137717    Objective Loss 0.137717                                        LR 0.001000    Time 0.078418    
Epoch: [44][   90/  207]    Overall Loss 0.136078    Objective Loss 0.136078                                        LR 0.001000    Time 0.077571    
Epoch: [44][  100/  207]    Overall Loss 0.134497    Objective Loss 0.134497                                        LR 0.001000    Time 0.076895    
Epoch: [44][  110/  207]    Overall Loss 0.132946    Objective Loss 0.132946                                        LR 0.001000    Time 0.076270    
Epoch: [44][  120/  207]    Overall Loss 0.131387    Objective Loss 0.131387                                        LR 0.001000    Time 0.075970    
Epoch: [44][  130/  207]    Overall Loss 0.129792    Objective Loss 0.129792                                        LR 0.001000    Time 0.075561    
Epoch: [44][  140/  207]    Overall Loss 0.127742    Objective Loss 0.127742                                        LR 0.001000    Time 0.075216    
Epoch: [44][  150/  207]    Overall Loss 0.126089    Objective Loss 0.126089                                        LR 0.001000    Time 0.075043    
Epoch: [44][  160/  207]    Overall Loss 0.124542    Objective Loss 0.124542                                        LR 0.001000    Time 0.074916    
Epoch: [44][  170/  207]    Overall Loss 0.123293    Objective Loss 0.123293                                        LR 0.001000    Time 0.074699    
Epoch: [44][  180/  207]    Overall Loss 0.122722    Objective Loss 0.122722                                        LR 0.001000    Time 0.074504    
Epoch: [44][  190/  207]    Overall Loss 0.120882    Objective Loss 0.120882                                        LR 0.001000    Time 0.074310    
Epoch: [44][  200/  207]    Overall Loss 0.120501    Objective Loss 0.120501                                        LR 0.001000    Time 0.074092    
Epoch: [44][  207/  207]    Overall Loss 0.120930    Objective Loss 0.120930    Top1 93.318233    Top5 100.000000    LR 0.001000    Time 0.073851    
--- validate (epoch=44)-----------
5136 samples (512 per mini-batch)
Epoch: [44][   10/   11]    Loss 0.561179    Top1 79.335938    Top5 99.472656    
Epoch: [44][   11/   11]    Loss 0.553890    Top1 79.322430    Top5 99.474299    
==> Top1: 79.322    Top5: 99.474    Loss: 0.554

==> Confusion:
[[260   1   2   4   2  11   5  15]
 [  4 242  35   7   0   3   1   8]
 [  2  12 268   1   1  10   0   6]
 [  1   0   0 793  26   8   9   0]
 [  1   0   0 123 722   2  18  13]
 [  6   3   8  45  32 776  15   9]
 [  2   0   0  14  46  13 745  17]
 [ 29  15  15 138  80 208  36 268]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [45][   10/  207]    Overall Loss 0.120762    Objective Loss 0.120762                                        LR 0.001000    Time 0.129433    
Epoch: [45][   20/  207]    Overall Loss 0.120594    Objective Loss 0.120594                                        LR 0.001000    Time 0.099533    
Epoch: [45][   30/  207]    Overall Loss 0.117592    Objective Loss 0.117592                                        LR 0.001000    Time 0.090166    
Epoch: [45][   40/  207]    Overall Loss 0.116695    Objective Loss 0.116695                                        LR 0.001000    Time 0.085686    
Epoch: [45][   50/  207]    Overall Loss 0.115803    Objective Loss 0.115803                                        LR 0.001000    Time 0.082953    
Epoch: [45][   60/  207]    Overall Loss 0.115088    Objective Loss 0.115088                                        LR 0.001000    Time 0.080663    
Epoch: [45][   70/  207]    Overall Loss 0.113436    Objective Loss 0.113436                                        LR 0.001000    Time 0.079311    
Epoch: [45][   80/  207]    Overall Loss 0.112773    Objective Loss 0.112773                                        LR 0.001000    Time 0.078441    
Epoch: [45][   90/  207]    Overall Loss 0.110151    Objective Loss 0.110151                                        LR 0.001000    Time 0.077664    
Epoch: [45][  100/  207]    Overall Loss 0.110721    Objective Loss 0.110721                                        LR 0.001000    Time 0.076988    
Epoch: [45][  110/  207]    Overall Loss 0.109420    Objective Loss 0.109420                                        LR 0.001000    Time 0.076617    
Epoch: [45][  120/  207]    Overall Loss 0.109346    Objective Loss 0.109346                                        LR 0.001000    Time 0.076206    
Epoch: [45][  130/  207]    Overall Loss 0.108956    Objective Loss 0.108956                                        LR 0.001000    Time 0.075873    
Epoch: [45][  140/  207]    Overall Loss 0.109894    Objective Loss 0.109894                                        LR 0.001000    Time 0.075507    
Epoch: [45][  150/  207]    Overall Loss 0.113079    Objective Loss 0.113079                                        LR 0.001000    Time 0.075160    
Epoch: [45][  160/  207]    Overall Loss 0.113762    Objective Loss 0.113762                                        LR 0.001000    Time 0.074937    
Epoch: [45][  170/  207]    Overall Loss 0.114872    Objective Loss 0.114872                                        LR 0.001000    Time 0.074752    
Epoch: [45][  180/  207]    Overall Loss 0.115240    Objective Loss 0.115240                                        LR 0.001000    Time 0.074546    
Epoch: [45][  190/  207]    Overall Loss 0.115085    Objective Loss 0.115085                                        LR 0.001000    Time 0.074363    
Epoch: [45][  200/  207]    Overall Loss 0.115390    Objective Loss 0.115390                                        LR 0.001000    Time 0.074264    
Epoch: [45][  207/  207]    Overall Loss 0.115051    Objective Loss 0.115051    Top1 93.884485    Top5 99.886750    LR 0.001000    Time 0.074043    
--- validate (epoch=45)-----------
5136 samples (512 per mini-batch)
Epoch: [45][   10/   11]    Loss 0.474232    Top1 81.738281    Top5 99.648438    
Epoch: [45][   11/   11]    Loss 0.441954    Top1 81.775701    Top5 99.649533    
==> Top1: 81.776    Top5: 99.650    Loss: 0.442

==> Confusion:
[[269   2   3   1   2   7   4  12]
 [  1 251  39   2   0   3   0   4]
 [  2  13 275   0   0   6   0   4]
 [  1   4   0 727  49  21  21  14]
 [  0   0   0  49 755  17  45  13]
 [  6   3   9   8  13 816  22  17]
 [  2   0   0   2  15  16 793   9]
 [ 27  17  22  52  64 225  68 314]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [46][   10/  207]    Overall Loss 0.101881    Objective Loss 0.101881                                        LR 0.001000    Time 0.129374    
Epoch: [46][   20/  207]    Overall Loss 0.098871    Objective Loss 0.098871                                        LR 0.001000    Time 0.100108    
Epoch: [46][   30/  207]    Overall Loss 0.099686    Objective Loss 0.099686                                        LR 0.001000    Time 0.091037    
Epoch: [46][   40/  207]    Overall Loss 0.104554    Objective Loss 0.104554                                        LR 0.001000    Time 0.086321    
Epoch: [46][   50/  207]    Overall Loss 0.105234    Objective Loss 0.105234                                        LR 0.001000    Time 0.083434    
Epoch: [46][   60/  207]    Overall Loss 0.107730    Objective Loss 0.107730                                        LR 0.001000    Time 0.082027    
Epoch: [46][   70/  207]    Overall Loss 0.106742    Objective Loss 0.106742                                        LR 0.001000    Time 0.080985    
Epoch: [46][   80/  207]    Overall Loss 0.107699    Objective Loss 0.107699                                        LR 0.001000    Time 0.080231    
Epoch: [46][   90/  207]    Overall Loss 0.107509    Objective Loss 0.107509                                        LR 0.001000    Time 0.079473    
Epoch: [46][  100/  207]    Overall Loss 0.105413    Objective Loss 0.105413                                        LR 0.001000    Time 0.079360    
Epoch: [46][  110/  207]    Overall Loss 0.104744    Objective Loss 0.104744                                        LR 0.001000    Time 0.079197    
Epoch: [46][  120/  207]    Overall Loss 0.103530    Objective Loss 0.103530                                        LR 0.001000    Time 0.079095    
Epoch: [46][  130/  207]    Overall Loss 0.103345    Objective Loss 0.103345                                        LR 0.001000    Time 0.079063    
Epoch: [46][  140/  207]    Overall Loss 0.103837    Objective Loss 0.103837                                        LR 0.001000    Time 0.079136    
Epoch: [46][  150/  207]    Overall Loss 0.104141    Objective Loss 0.104141                                        LR 0.001000    Time 0.079168    
Epoch: [46][  160/  207]    Overall Loss 0.104240    Objective Loss 0.104240                                        LR 0.001000    Time 0.079030    
Epoch: [46][  170/  207]    Overall Loss 0.104656    Objective Loss 0.104656                                        LR 0.001000    Time 0.078729    
Epoch: [46][  180/  207]    Overall Loss 0.105259    Objective Loss 0.105259                                        LR 0.001000    Time 0.078588    
Epoch: [46][  190/  207]    Overall Loss 0.107219    Objective Loss 0.107219                                        LR 0.001000    Time 0.078321    
Epoch: [46][  200/  207]    Overall Loss 0.109620    Objective Loss 0.109620                                        LR 0.001000    Time 0.078316    
Epoch: [46][  207/  207]    Overall Loss 0.112744    Objective Loss 0.112744    Top1 94.224236    Top5 99.886750    LR 0.001000    Time 0.078036    
--- validate (epoch=46)-----------
5136 samples (512 per mini-batch)
Epoch: [46][   10/   11]    Loss 0.573248    Top1 79.277344    Top5 99.589844    
Epoch: [46][   11/   11]    Loss 0.614772    Top1 79.244548    Top5 99.571651    
==> Top1: 79.245    Top5: 99.572    Loss: 0.615

==> Confusion:
[[238  22   8   0   0  10   2  20]
 [  1 273  22   0   0   1   0   3]
 [  1  36 257   0   0   5   0   1]
 [  0   8   0 675 106  11  24  13]
 [  0   1   0  21 800  10  33  14]
 [  6   8   8   5  29 779  41  18]
 [  2   0   0   4  39   8 777   7]
 [ 27  46  16  39 103 196  91 271]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [47][   10/  207]    Overall Loss 0.136979    Objective Loss 0.136979                                        LR 0.001000    Time 0.129950    
Epoch: [47][   20/  207]    Overall Loss 0.134618    Objective Loss 0.134618                                        LR 0.001000    Time 0.101021    
Epoch: [47][   30/  207]    Overall Loss 0.133199    Objective Loss 0.133199                                        LR 0.001000    Time 0.091691    
Epoch: [47][   40/  207]    Overall Loss 0.130491    Objective Loss 0.130491                                        LR 0.001000    Time 0.087970    
Epoch: [47][   50/  207]    Overall Loss 0.131965    Objective Loss 0.131965                                        LR 0.001000    Time 0.086686    
Epoch: [47][   60/  207]    Overall Loss 0.127862    Objective Loss 0.127862                                        LR 0.001000    Time 0.084725    
Epoch: [47][   70/  207]    Overall Loss 0.125889    Objective Loss 0.125889                                        LR 0.001000    Time 0.084209    
Epoch: [47][   80/  207]    Overall Loss 0.123437    Objective Loss 0.123437                                        LR 0.001000    Time 0.083379    
Epoch: [47][   90/  207]    Overall Loss 0.120772    Objective Loss 0.120772                                        LR 0.001000    Time 0.084564    
Epoch: [47][  100/  207]    Overall Loss 0.118271    Objective Loss 0.118271                                        LR 0.001000    Time 0.090122    
Epoch: [47][  110/  207]    Overall Loss 0.115637    Objective Loss 0.115637                                        LR 0.001000    Time 0.090133    
Epoch: [47][  120/  207]    Overall Loss 0.114331    Objective Loss 0.114331                                        LR 0.001000    Time 0.088508    
Epoch: [47][  130/  207]    Overall Loss 0.114495    Objective Loss 0.114495                                        LR 0.001000    Time 0.087603    
Epoch: [47][  140/  207]    Overall Loss 0.113426    Objective Loss 0.113426                                        LR 0.001000    Time 0.087434    
Epoch: [47][  150/  207]    Overall Loss 0.113291    Objective Loss 0.113291                                        LR 0.001000    Time 0.087282    
Epoch: [47][  160/  207]    Overall Loss 0.112908    Objective Loss 0.112908                                        LR 0.001000    Time 0.086232    
Epoch: [47][  170/  207]    Overall Loss 0.112081    Objective Loss 0.112081                                        LR 0.001000    Time 0.085488    
Epoch: [47][  180/  207]    Overall Loss 0.111433    Objective Loss 0.111433                                        LR 0.001000    Time 0.085116    
Epoch: [47][  190/  207]    Overall Loss 0.110547    Objective Loss 0.110547                                        LR 0.001000    Time 0.084394    
Epoch: [47][  200/  207]    Overall Loss 0.111697    Objective Loss 0.111697                                        LR 0.001000    Time 0.083836    
Epoch: [47][  207/  207]    Overall Loss 0.112637    Objective Loss 0.112637    Top1 91.959230    Top5 100.000000    LR 0.001000    Time 0.083314    
--- validate (epoch=47)-----------
5136 samples (512 per mini-batch)
Epoch: [47][   10/   11]    Loss 0.498741    Top1 80.468750    Top5 99.707031    
Epoch: [47][   11/   11]    Loss 0.454423    Top1 80.510125    Top5 99.707944    
==> Top1: 80.510    Top5: 99.708    Loss: 0.454

==> Confusion:
[[279   7   2   1   2   1   1   7]
 [  5 263  23   2   0   5   0   2]
 [  2  49 235   1   1   9   0   3]
 [  1   5   0 751  51  10  12   7]
 [  1   0   0  54 775   8  30  11]
 [ 14   8   5  18  24 786  23  16]
 [  3   0   0   6  33  14 768  13]
 [ 57  30  18  79  94 182  51 278]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [48][   10/  207]    Overall Loss 0.110167    Objective Loss 0.110167                                        LR 0.001000    Time 0.139394    
Epoch: [48][   20/  207]    Overall Loss 0.102210    Objective Loss 0.102210                                        LR 0.001000    Time 0.109459    
Epoch: [48][   30/  207]    Overall Loss 0.108484    Objective Loss 0.108484                                        LR 0.001000    Time 0.099112    
Epoch: [48][   40/  207]    Overall Loss 0.119328    Objective Loss 0.119328                                        LR 0.001000    Time 0.092928    
Epoch: [48][   50/  207]    Overall Loss 0.120919    Objective Loss 0.120919                                        LR 0.001000    Time 0.089212    
Epoch: [48][   60/  207]    Overall Loss 0.127090    Objective Loss 0.127090                                        LR 0.001000    Time 0.087559    
Epoch: [48][   70/  207]    Overall Loss 0.126583    Objective Loss 0.126583                                        LR 0.001000    Time 0.086156    
Epoch: [48][   80/  207]    Overall Loss 0.127247    Objective Loss 0.127247                                        LR 0.001000    Time 0.084835    
Epoch: [48][   90/  207]    Overall Loss 0.126697    Objective Loss 0.126697                                        LR 0.001000    Time 0.083863    
Epoch: [48][  100/  207]    Overall Loss 0.133250    Objective Loss 0.133250                                        LR 0.001000    Time 0.083191    
Epoch: [48][  110/  207]    Overall Loss 0.133332    Objective Loss 0.133332                                        LR 0.001000    Time 0.082282    
Epoch: [48][  120/  207]    Overall Loss 0.132882    Objective Loss 0.132882                                        LR 0.001000    Time 0.081379    
Epoch: [48][  130/  207]    Overall Loss 0.131731    Objective Loss 0.131731                                        LR 0.001000    Time 0.080749    
Epoch: [48][  140/  207]    Overall Loss 0.132009    Objective Loss 0.132009                                        LR 0.001000    Time 0.080226    
Epoch: [48][  150/  207]    Overall Loss 0.132200    Objective Loss 0.132200                                        LR 0.001000    Time 0.079607    
Epoch: [48][  160/  207]    Overall Loss 0.133346    Objective Loss 0.133346                                        LR 0.001000    Time 0.079019    
Epoch: [48][  170/  207]    Overall Loss 0.133609    Objective Loss 0.133609                                        LR 0.001000    Time 0.078978    
Epoch: [48][  180/  207]    Overall Loss 0.132771    Objective Loss 0.132771                                        LR 0.001000    Time 0.078599    
Epoch: [48][  190/  207]    Overall Loss 0.132238    Objective Loss 0.132238                                        LR 0.001000    Time 0.078404    
Epoch: [48][  200/  207]    Overall Loss 0.131286    Objective Loss 0.131286                                        LR 0.001000    Time 0.078479    
Epoch: [48][  207/  207]    Overall Loss 0.131235    Objective Loss 0.131235    Top1 91.053228    Top5 99.886750    LR 0.001000    Time 0.078166    
--- validate (epoch=48)-----------
5136 samples (512 per mini-batch)
Epoch: [48][   10/   11]    Loss 0.569003    Top1 81.640625    Top5 99.609375    
Epoch: [48][   11/   11]    Loss 0.548230    Top1 81.639408    Top5 99.610592    
==> Top1: 81.639    Top5: 99.611    Loss: 0.548

==> Confusion:
[[240   2  10   0   3  10   5  30]
 [  1 260  30   1   0   2   0   6]
 [  0  14 272   1   1   6   0   6]
 [  0   3   1 732  57  14   9  21]
 [  0   0   0  39 782  10  25  23]
 [  2   5  14  11  24 786  19  33]
 [  1   0   0   6  32  12 765  21]
 [ 26  15  26  55  86 165  60 356]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [49][   10/  207]    Overall Loss 0.095596    Objective Loss 0.095596                                        LR 0.001000    Time 0.129315    
Epoch: [49][   20/  207]    Overall Loss 0.094827    Objective Loss 0.094827                                        LR 0.001000    Time 0.102149    
Epoch: [49][   30/  207]    Overall Loss 0.094907    Objective Loss 0.094907                                        LR 0.001000    Time 0.093388    
Epoch: [49][   40/  207]    Overall Loss 0.097063    Objective Loss 0.097063                                        LR 0.001000    Time 0.087913    
Epoch: [49][   50/  207]    Overall Loss 0.099468    Objective Loss 0.099468                                        LR 0.001000    Time 0.085430    
Epoch: [49][   60/  207]    Overall Loss 0.100285    Objective Loss 0.100285                                        LR 0.001000    Time 0.084084    
Epoch: [49][   70/  207]    Overall Loss 0.101202    Objective Loss 0.101202                                        LR 0.001000    Time 0.083356    
Epoch: [49][   80/  207]    Overall Loss 0.100836    Objective Loss 0.100836                                        LR 0.001000    Time 0.082273    
Epoch: [49][   90/  207]    Overall Loss 0.100094    Objective Loss 0.100094                                        LR 0.001000    Time 0.081921    
Epoch: [49][  100/  207]    Overall Loss 0.098625    Objective Loss 0.098625                                        LR 0.001000    Time 0.081546    
Epoch: [49][  110/  207]    Overall Loss 0.096749    Objective Loss 0.096749                                        LR 0.001000    Time 0.081308    
Epoch: [49][  120/  207]    Overall Loss 0.095516    Objective Loss 0.095516                                        LR 0.001000    Time 0.080983    
Epoch: [49][  130/  207]    Overall Loss 0.095348    Objective Loss 0.095348                                        LR 0.001000    Time 0.080582    
Epoch: [49][  140/  207]    Overall Loss 0.095141    Objective Loss 0.095141                                        LR 0.001000    Time 0.080289    
Epoch: [49][  150/  207]    Overall Loss 0.094015    Objective Loss 0.094015                                        LR 0.001000    Time 0.079773    
Epoch: [49][  160/  207]    Overall Loss 0.093700    Objective Loss 0.093700                                        LR 0.001000    Time 0.079524    
Epoch: [49][  170/  207]    Overall Loss 0.093740    Objective Loss 0.093740                                        LR 0.001000    Time 0.079184    
Epoch: [49][  180/  207]    Overall Loss 0.093118    Objective Loss 0.093118                                        LR 0.001000    Time 0.078999    
Epoch: [49][  190/  207]    Overall Loss 0.092590    Objective Loss 0.092590                                        LR 0.001000    Time 0.078737    
Epoch: [49][  200/  207]    Overall Loss 0.092533    Objective Loss 0.092533                                        LR 0.001000    Time 0.078504    
Epoch: [49][  207/  207]    Overall Loss 0.092633    Objective Loss 0.092633    Top1 95.809740    Top5 99.886750    LR 0.001000    Time 0.078170    
--- validate (epoch=49)-----------
5136 samples (512 per mini-batch)
Epoch: [49][   10/   11]    Loss 0.459583    Top1 81.972656    Top5 99.824219    
Epoch: [49][   11/   11]    Loss 0.429132    Top1 82.009346    Top5 99.824766    
==> Top1: 82.009    Top5: 99.825    Loss: 0.429

==> Confusion:
[[263   7   5   1   0  10   3  11]
 [  3 273  19   1   0   1   0   3]
 [  2  26 266   0   0   6   0   0]
 [  0   5   0 742  57  14  10   9]
 [  2   0   0  46 775  14  27  15]
 [  5  10   8  11  14 825  14   7]
 [  2   0   0   6  28  16 770  15]
 [ 32  25  15  72  79 225  43 298]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [50][   10/  207]    Overall Loss 0.077302    Objective Loss 0.077302                                        LR 0.001000    Time 0.130342    
Epoch: [50][   20/  207]    Overall Loss 0.076845    Objective Loss 0.076845                                        LR 0.001000    Time 0.101026    
Epoch: [50][   30/  207]    Overall Loss 0.079534    Objective Loss 0.079534                                        LR 0.001000    Time 0.093652    
Epoch: [50][   40/  207]    Overall Loss 0.079891    Objective Loss 0.079891                                        LR 0.001000    Time 0.088246    
Epoch: [50][   50/  207]    Overall Loss 0.081257    Objective Loss 0.081257                                        LR 0.001000    Time 0.091520    
Epoch: [50][   60/  207]    Overall Loss 0.081513    Objective Loss 0.081513                                        LR 0.001000    Time 0.099101    
Epoch: [50][   70/  207]    Overall Loss 0.081225    Objective Loss 0.081225                                        LR 0.001000    Time 0.095827    
Epoch: [50][   80/  207]    Overall Loss 0.080937    Objective Loss 0.080937                                        LR 0.001000    Time 0.093841    
Epoch: [50][   90/  207]    Overall Loss 0.079792    Objective Loss 0.079792                                        LR 0.001000    Time 0.092150    
Epoch: [50][  100/  207]    Overall Loss 0.079740    Objective Loss 0.079740                                        LR 0.001000    Time 0.090567    
Epoch: [50][  110/  207]    Overall Loss 0.079469    Objective Loss 0.079469                                        LR 0.001000    Time 0.089349    
Epoch: [50][  120/  207]    Overall Loss 0.080237    Objective Loss 0.080237                                        LR 0.001000    Time 0.088052    
Epoch: [50][  130/  207]    Overall Loss 0.080507    Objective Loss 0.080507                                        LR 0.001000    Time 0.087072    
Epoch: [50][  140/  207]    Overall Loss 0.080665    Objective Loss 0.080665                                        LR 0.001000    Time 0.086247    
Epoch: [50][  150/  207]    Overall Loss 0.081445    Objective Loss 0.081445                                        LR 0.001000    Time 0.085332    
Epoch: [50][  160/  207]    Overall Loss 0.082195    Objective Loss 0.082195                                        LR 0.001000    Time 0.084473    
Epoch: [50][  170/  207]    Overall Loss 0.081873    Objective Loss 0.081873                                        LR 0.001000    Time 0.083712    
Epoch: [50][  180/  207]    Overall Loss 0.082121    Objective Loss 0.082121                                        LR 0.001000    Time 0.082994    
Epoch: [50][  190/  207]    Overall Loss 0.082451    Objective Loss 0.082451                                        LR 0.001000    Time 0.082449    
Epoch: [50][  200/  207]    Overall Loss 0.083399    Objective Loss 0.083399                                        LR 0.001000    Time 0.082078    
Epoch: [50][  207/  207]    Overall Loss 0.084160    Objective Loss 0.084160    Top1 90.939977    Top5 100.000000    LR 0.001000    Time 0.081738    
--- validate (epoch=50)-----------
5136 samples (512 per mini-batch)
Epoch: [50][   10/   11]    Loss 0.517262    Top1 80.878906    Top5 99.609375    
Epoch: [50][   11/   11]    Loss 0.475744    Top1 80.919003    Top5 99.610592    
==> Top1: 80.919    Top5: 99.611    Loss: 0.476

==> Confusion:
[[265   2   1   1   4  11   3  13]
 [  7 246  32   2   0   7   2   4]
 [  3  15 267   1   0   9   0   5]
 [  1   0   1 697  98  11  17  12]
 [  2   0   0  21 812   5  27  12]
 [  5   3   9  10  25 813  15  14]
 [  2   0   0   3  39  14 765  14]
 [ 36  20  19  38 102 237  46 291]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [51][   10/  207]    Overall Loss 0.084415    Objective Loss 0.084415                                        LR 0.001000    Time 0.143939    
Epoch: [51][   20/  207]    Overall Loss 0.087076    Objective Loss 0.087076                                        LR 0.001000    Time 0.112512    
Epoch: [51][   30/  207]    Overall Loss 0.087028    Objective Loss 0.087028                                        LR 0.001000    Time 0.099383    
Epoch: [51][   40/  207]    Overall Loss 0.088345    Objective Loss 0.088345                                        LR 0.001000    Time 0.092798    
Epoch: [51][   50/  207]    Overall Loss 0.085459    Objective Loss 0.085459                                        LR 0.001000    Time 0.088534    
Epoch: [51][   60/  207]    Overall Loss 0.086248    Objective Loss 0.086248                                        LR 0.001000    Time 0.085885    
Epoch: [51][   70/  207]    Overall Loss 0.086186    Objective Loss 0.086186                                        LR 0.001000    Time 0.083878    
Epoch: [51][   80/  207]    Overall Loss 0.085592    Objective Loss 0.085592                                        LR 0.001000    Time 0.083052    
Epoch: [51][   90/  207]    Overall Loss 0.084367    Objective Loss 0.084367                                        LR 0.001000    Time 0.082309    
Epoch: [51][  100/  207]    Overall Loss 0.085072    Objective Loss 0.085072                                        LR 0.001000    Time 0.081173    
Epoch: [51][  110/  207]    Overall Loss 0.085024    Objective Loss 0.085024                                        LR 0.001000    Time 0.080650    
Epoch: [51][  120/  207]    Overall Loss 0.085147    Objective Loss 0.085147                                        LR 0.001000    Time 0.080076    
Epoch: [51][  130/  207]    Overall Loss 0.085331    Objective Loss 0.085331                                        LR 0.001000    Time 0.079790    
Epoch: [51][  140/  207]    Overall Loss 0.085977    Objective Loss 0.085977                                        LR 0.001000    Time 0.079756    
Epoch: [51][  150/  207]    Overall Loss 0.086491    Objective Loss 0.086491                                        LR 0.001000    Time 0.079431    
Epoch: [51][  160/  207]    Overall Loss 0.088564    Objective Loss 0.088564                                        LR 0.001000    Time 0.079171    
Epoch: [51][  170/  207]    Overall Loss 0.090725    Objective Loss 0.090725                                        LR 0.001000    Time 0.079138    
Epoch: [51][  180/  207]    Overall Loss 0.092408    Objective Loss 0.092408                                        LR 0.001000    Time 0.078863    
Epoch: [51][  190/  207]    Overall Loss 0.095694    Objective Loss 0.095694                                        LR 0.001000    Time 0.078709    
Epoch: [51][  200/  207]    Overall Loss 0.099539    Objective Loss 0.099539                                        LR 0.001000    Time 0.078360    
Epoch: [51][  207/  207]    Overall Loss 0.101400    Objective Loss 0.101400    Top1 89.807475    Top5 100.000000    LR 0.001000    Time 0.077982    
--- validate (epoch=51)-----------
5136 samples (512 per mini-batch)
Epoch: [51][   10/   11]    Loss 0.565085    Top1 81.835938    Top5 99.687500    
Epoch: [51][   11/   11]    Loss 0.552127    Top1 81.775701    Top5 99.688474    
==> Top1: 81.776    Top5: 99.688    Loss: 0.552

==> Confusion:
[[250   8   7   1   1   7   2  24]
 [  1 250  44   0   0   1   0   4]
 [  2  18 272   1   0   3   0   4]
 [  0   4   0 741  51  19  12  10]
 [  0   0   0  54 771  11  25  18]
 [  4   6  19  13  16 777  20  39]
 [  2   0   0   7  30  10 769  19]
 [ 32  25  28  57  85 149  43 370]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [52][   10/  207]    Overall Loss 0.127035    Objective Loss 0.127035                                        LR 0.001000    Time 0.136736    
Epoch: [52][   20/  207]    Overall Loss 0.118921    Objective Loss 0.118921                                        LR 0.001000    Time 0.104197    
Epoch: [52][   30/  207]    Overall Loss 0.116096    Objective Loss 0.116096                                        LR 0.001000    Time 0.093403    
Epoch: [52][   40/  207]    Overall Loss 0.121438    Objective Loss 0.121438                                        LR 0.001000    Time 0.087980    
Epoch: [52][   50/  207]    Overall Loss 0.124192    Objective Loss 0.124192                                        LR 0.001000    Time 0.084596    
Epoch: [52][   60/  207]    Overall Loss 0.127124    Objective Loss 0.127124                                        LR 0.001000    Time 0.082187    
Epoch: [52][   70/  207]    Overall Loss 0.126696    Objective Loss 0.126696                                        LR 0.001000    Time 0.080769    
Epoch: [52][   80/  207]    Overall Loss 0.125374    Objective Loss 0.125374                                        LR 0.001000    Time 0.079503    
Epoch: [52][   90/  207]    Overall Loss 0.125406    Objective Loss 0.125406                                        LR 0.001000    Time 0.078466    
Epoch: [52][  100/  207]    Overall Loss 0.128229    Objective Loss 0.128229                                        LR 0.001000    Time 0.078095    
Epoch: [52][  110/  207]    Overall Loss 0.127897    Objective Loss 0.127897                                        LR 0.001000    Time 0.077525    
Epoch: [52][  120/  207]    Overall Loss 0.129034    Objective Loss 0.129034                                        LR 0.001000    Time 0.077046    
Epoch: [52][  130/  207]    Overall Loss 0.131212    Objective Loss 0.131212                                        LR 0.001000    Time 0.076993    
Epoch: [52][  140/  207]    Overall Loss 0.134248    Objective Loss 0.134248                                        LR 0.001000    Time 0.076931    
Epoch: [52][  150/  207]    Overall Loss 0.134819    Objective Loss 0.134819                                        LR 0.001000    Time 0.077008    
Epoch: [52][  160/  207]    Overall Loss 0.139504    Objective Loss 0.139504                                        LR 0.001000    Time 0.077196    
Epoch: [52][  170/  207]    Overall Loss 0.147072    Objective Loss 0.147072                                        LR 0.001000    Time 0.077336    
Epoch: [52][  180/  207]    Overall Loss 0.151719    Objective Loss 0.151719                                        LR 0.001000    Time 0.077235    
Epoch: [52][  190/  207]    Overall Loss 0.153281    Objective Loss 0.153281                                        LR 0.001000    Time 0.077543    
Epoch: [52][  200/  207]    Overall Loss 0.154911    Objective Loss 0.154911                                        LR 0.001000    Time 0.077567    
Epoch: [52][  207/  207]    Overall Loss 0.155567    Objective Loss 0.155567    Top1 91.392978    Top5 100.000000    LR 0.001000    Time 0.077460    
--- validate (epoch=52)-----------
5136 samples (512 per mini-batch)
Epoch: [52][   10/   11]    Loss 0.568374    Top1 79.472656    Top5 99.628906    
Epoch: [52][   11/   11]    Loss 0.526468    Top1 79.497664    Top5 99.630062    
==> Top1: 79.498    Top5: 99.630    Loss: 0.526

==> Confusion:
[[270   8   4   1   1   3   0  13]
 [  3 267  25   0   0   1   0   4]
 [  3  39 255   0   0   2   0   1]
 [  1  15   0 738  39  22   3  19]
 [  3   2   0  66 750  24  10  24]
 [ 21  18  14   8  19 794   2  18]
 [  3   1   0   6  49  39 706  33]
 [ 57  53  23  50  65 216  22 303]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [53][   10/  207]    Overall Loss 0.148170    Objective Loss 0.148170                                        LR 0.001000    Time 0.134637    
Epoch: [53][   20/  207]    Overall Loss 0.159441    Objective Loss 0.159441                                        LR 0.001000    Time 0.102752    
Epoch: [53][   30/  207]    Overall Loss 0.169127    Objective Loss 0.169127                                        LR 0.001000    Time 0.091861    
Epoch: [53][   40/  207]    Overall Loss 0.172279    Objective Loss 0.172279                                        LR 0.001000    Time 0.087014    
Epoch: [53][   50/  207]    Overall Loss 0.168002    Objective Loss 0.168002                                        LR 0.001000    Time 0.084766    
Epoch: [53][   60/  207]    Overall Loss 0.161613    Objective Loss 0.161613                                        LR 0.001000    Time 0.082664    
Epoch: [53][   70/  207]    Overall Loss 0.153070    Objective Loss 0.153070                                        LR 0.001000    Time 0.081695    
Epoch: [53][   80/  207]    Overall Loss 0.151018    Objective Loss 0.151018                                        LR 0.001000    Time 0.080578    
Epoch: [53][   90/  207]    Overall Loss 0.151356    Objective Loss 0.151356                                        LR 0.001000    Time 0.079775    
Epoch: [53][  100/  207]    Overall Loss 0.150740    Objective Loss 0.150740                                        LR 0.001000    Time 0.079044    
Epoch: [53][  110/  207]    Overall Loss 0.149683    Objective Loss 0.149683                                        LR 0.001000    Time 0.078723    
Epoch: [53][  120/  207]    Overall Loss 0.146922    Objective Loss 0.146922                                        LR 0.001000    Time 0.078421    
Epoch: [53][  130/  207]    Overall Loss 0.144982    Objective Loss 0.144982                                        LR 0.001000    Time 0.078078    
Epoch: [53][  140/  207]    Overall Loss 0.142508    Objective Loss 0.142508                                        LR 0.001000    Time 0.077632    
Epoch: [53][  150/  207]    Overall Loss 0.140059    Objective Loss 0.140059                                        LR 0.001000    Time 0.077153    
Epoch: [53][  160/  207]    Overall Loss 0.137667    Objective Loss 0.137667                                        LR 0.001000    Time 0.076877    
Epoch: [53][  170/  207]    Overall Loss 0.134931    Objective Loss 0.134931                                        LR 0.001000    Time 0.076612    
Epoch: [53][  180/  207]    Overall Loss 0.132726    Objective Loss 0.132726                                        LR 0.001000    Time 0.076425    
Epoch: [53][  190/  207]    Overall Loss 0.131521    Objective Loss 0.131521                                        LR 0.001000    Time 0.076190    
Epoch: [53][  200/  207]    Overall Loss 0.131046    Objective Loss 0.131046                                        LR 0.001000    Time 0.076105    
Epoch: [53][  207/  207]    Overall Loss 0.131289    Objective Loss 0.131289    Top1 92.751982    Top5 100.000000    LR 0.001000    Time 0.075952    
--- validate (epoch=53)-----------
5136 samples (512 per mini-batch)
Epoch: [53][   10/   11]    Loss 0.501594    Top1 81.992188    Top5 99.687500    
Epoch: [53][   11/   11]    Loss 0.571847    Top1 81.950935    Top5 99.688474    
==> Top1: 81.951    Top5: 99.688    Loss: 0.572

==> Confusion:
[[270   9   5   1   0   1   1  13]
 [  3 266  26   1   0   1   0   3]
 [  3  26 264   1   0   2   0   4]
 [  0   7   1 734  61   5  13  16]
 [  1   0   0  45 788   1  21  23]
 [ 19  10  14  19  29 728  15  60]
 [  3   1   0   5  35   8 752  33]
 [ 49  31  21  55  87 108  31 407]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [54][   10/  207]    Overall Loss 0.137479    Objective Loss 0.137479                                        LR 0.001000    Time 0.131536    
Epoch: [54][   20/  207]    Overall Loss 0.142383    Objective Loss 0.142383                                        LR 0.001000    Time 0.102149    
Epoch: [54][   30/  207]    Overall Loss 0.136351    Objective Loss 0.136351                                        LR 0.001000    Time 0.092997    
Epoch: [54][   40/  207]    Overall Loss 0.139068    Objective Loss 0.139068                                        LR 0.001000    Time 0.088031    
Epoch: [54][   50/  207]    Overall Loss 0.142806    Objective Loss 0.142806                                        LR 0.001000    Time 0.084597    
Epoch: [54][   60/  207]    Overall Loss 0.144952    Objective Loss 0.144952                                        LR 0.001000    Time 0.082278    
Epoch: [54][   70/  207]    Overall Loss 0.143433    Objective Loss 0.143433                                        LR 0.001000    Time 0.080847    
Epoch: [54][   80/  207]    Overall Loss 0.141584    Objective Loss 0.141584                                        LR 0.001000    Time 0.079953    
Epoch: [54][   90/  207]    Overall Loss 0.140533    Objective Loss 0.140533                                        LR 0.001000    Time 0.079443    
Epoch: [54][  100/  207]    Overall Loss 0.139437    Objective Loss 0.139437                                        LR 0.001000    Time 0.078904    
Epoch: [54][  110/  207]    Overall Loss 0.137665    Objective Loss 0.137665                                        LR 0.001000    Time 0.078478    
Epoch: [54][  120/  207]    Overall Loss 0.137103    Objective Loss 0.137103                                        LR 0.001000    Time 0.077849    
Epoch: [54][  130/  207]    Overall Loss 0.143611    Objective Loss 0.143611                                        LR 0.001000    Time 0.077715    
Epoch: [54][  140/  207]    Overall Loss 0.147328    Objective Loss 0.147328                                        LR 0.001000    Time 0.077605    
Epoch: [54][  150/  207]    Overall Loss 0.147713    Objective Loss 0.147713                                        LR 0.001000    Time 0.077235    
Epoch: [54][  160/  207]    Overall Loss 0.147498    Objective Loss 0.147498                                        LR 0.001000    Time 0.077056    
Epoch: [54][  170/  207]    Overall Loss 0.145570    Objective Loss 0.145570                                        LR 0.001000    Time 0.076912    
Epoch: [54][  180/  207]    Overall Loss 0.144079    Objective Loss 0.144079                                        LR 0.001000    Time 0.076665    
Epoch: [54][  190/  207]    Overall Loss 0.142914    Objective Loss 0.142914                                        LR 0.001000    Time 0.076608    
Epoch: [54][  200/  207]    Overall Loss 0.141575    Objective Loss 0.141575                                        LR 0.001000    Time 0.076439    
Epoch: [54][  207/  207]    Overall Loss 0.140874    Objective Loss 0.140874    Top1 94.224236    Top5 100.000000    LR 0.001000    Time 0.076127    
--- validate (epoch=54)-----------
5136 samples (512 per mini-batch)
Epoch: [54][   10/   11]    Loss 0.460192    Top1 81.972656    Top5 99.726562    
Epoch: [54][   11/   11]    Loss 0.601179    Top1 81.931464    Top5 99.707944    
==> Top1: 81.931    Top5: 99.708    Loss: 0.601

==> Confusion:
[[272   5   4   1   1   3   1  13]
 [  4 264  26   1   0   1   0   4]
 [  2  27 267   1   0   1   0   2]
 [  1   2   1 758  48   9   9   9]
 [  3   0   0  67 770   5  16  18]
 [ 15   9   8  22  21 769  16  34]
 [  2   0   0   8  44  14 744  25]
 [ 47  30  20  72  79 155  22 364]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [55][   10/  207]    Overall Loss 0.104474    Objective Loss 0.104474                                        LR 0.001000    Time 0.132138    
Epoch: [55][   20/  207]    Overall Loss 0.100132    Objective Loss 0.100132                                        LR 0.001000    Time 0.102044    
Epoch: [55][   30/  207]    Overall Loss 0.096585    Objective Loss 0.096585                                        LR 0.001000    Time 0.092569    
Epoch: [55][   40/  207]    Overall Loss 0.091972    Objective Loss 0.091972                                        LR 0.001000    Time 0.088005    
Epoch: [55][   50/  207]    Overall Loss 0.090268    Objective Loss 0.090268                                        LR 0.001000    Time 0.085738    
Epoch: [55][   60/  207]    Overall Loss 0.086683    Objective Loss 0.086683                                        LR 0.001000    Time 0.083602    
Epoch: [55][   70/  207]    Overall Loss 0.085214    Objective Loss 0.085214                                        LR 0.001000    Time 0.082049    
Epoch: [55][   80/  207]    Overall Loss 0.083871    Objective Loss 0.083871                                        LR 0.001000    Time 0.080720    
Epoch: [55][   90/  207]    Overall Loss 0.082927    Objective Loss 0.082927                                        LR 0.001000    Time 0.079981    
Epoch: [55][  100/  207]    Overall Loss 0.082005    Objective Loss 0.082005                                        LR 0.001000    Time 0.079442    
Epoch: [55][  110/  207]    Overall Loss 0.080913    Objective Loss 0.080913                                        LR 0.001000    Time 0.078807    
Epoch: [55][  120/  207]    Overall Loss 0.080216    Objective Loss 0.080216                                        LR 0.001000    Time 0.078389    
Epoch: [55][  130/  207]    Overall Loss 0.079305    Objective Loss 0.079305                                        LR 0.001000    Time 0.078088    
Epoch: [55][  140/  207]    Overall Loss 0.079143    Objective Loss 0.079143                                        LR 0.001000    Time 0.077938    
Epoch: [55][  150/  207]    Overall Loss 0.078335    Objective Loss 0.078335                                        LR 0.001000    Time 0.077967    
Epoch: [55][  160/  207]    Overall Loss 0.078865    Objective Loss 0.078865                                        LR 0.001000    Time 0.077639    
Epoch: [55][  170/  207]    Overall Loss 0.078893    Objective Loss 0.078893                                        LR 0.001000    Time 0.077273    
Epoch: [55][  180/  207]    Overall Loss 0.079456    Objective Loss 0.079456                                        LR 0.001000    Time 0.076976    
Epoch: [55][  190/  207]    Overall Loss 0.080264    Objective Loss 0.080264                                        LR 0.001000    Time 0.076613    
Epoch: [55][  200/  207]    Overall Loss 0.080520    Objective Loss 0.080520                                        LR 0.001000    Time 0.076209    
Epoch: [55][  207/  207]    Overall Loss 0.080806    Objective Loss 0.080806    Top1 94.337486    Top5 100.000000    LR 0.001000    Time 0.075908    
--- validate (epoch=55)-----------
5136 samples (512 per mini-batch)
Epoch: [55][   10/   11]    Loss 0.489736    Top1 81.582031    Top5 99.785156    
Epoch: [55][   11/   11]    Loss 0.486043    Top1 81.561526    Top5 99.785826    
==> Top1: 81.562    Top5: 99.786    Loss: 0.486

==> Confusion:
[[273   2   1   1   1   4   2  16]
 [  2 267  19   0   0   1   0  11]
 [  9  27 252   1   1   3   0   7]
 [  0   4   0 731  67  14  11  10]
 [  1   0   0  42 785   6  28  17]
 [ 15   4  10  17  27 779  21  21]
 [  2   0   0   5  37  11 766  16]
 [ 44  25  17  47  88 181  51 336]]

==> Best [Top1: 82.009   Top5: 99.825   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [56][   10/  207]    Overall Loss 0.078616    Objective Loss 0.078616                                        LR 0.001000    Time 0.126897    
Epoch: [56][   20/  207]    Overall Loss 0.074562    Objective Loss 0.074562                                        LR 0.001000    Time 0.099091    
Epoch: [56][   30/  207]    Overall Loss 0.077428    Objective Loss 0.077428                                        LR 0.001000    Time 0.089606    
Epoch: [56][   40/  207]    Overall Loss 0.077081    Objective Loss 0.077081                                        LR 0.001000    Time 0.084919    
Epoch: [56][   50/  207]    Overall Loss 0.075677    Objective Loss 0.075677                                        LR 0.001000    Time 0.082352    
Epoch: [56][   60/  207]    Overall Loss 0.074874    Objective Loss 0.074874                                        LR 0.001000    Time 0.080616    
Epoch: [56][   70/  207]    Overall Loss 0.075005    Objective Loss 0.075005                                        LR 0.001000    Time 0.079537    
Epoch: [56][   80/  207]    Overall Loss 0.075859    Objective Loss 0.075859                                        LR 0.001000    Time 0.079762    
Epoch: [56][   90/  207]    Overall Loss 0.075273    Objective Loss 0.075273                                        LR 0.001000    Time 0.078785    
Epoch: [56][  100/  207]    Overall Loss 0.074605    Objective Loss 0.074605                                        LR 0.001000    Time 0.078152    
Epoch: [56][  110/  207]    Overall Loss 0.073568    Objective Loss 0.073568                                        LR 0.001000    Time 0.077631    
Epoch: [56][  120/  207]    Overall Loss 0.072891    Objective Loss 0.072891                                        LR 0.001000    Time 0.077129    
Epoch: [56][  130/  207]    Overall Loss 0.072351    Objective Loss 0.072351                                        LR 0.001000    Time 0.076851    
Epoch: [56][  140/  207]    Overall Loss 0.072018    Objective Loss 0.072018                                        LR 0.001000    Time 0.076418    
Epoch: [56][  150/  207]    Overall Loss 0.071798    Objective Loss 0.071798                                        LR 0.001000    Time 0.076093    
Epoch: [56][  160/  207]    Overall Loss 0.071682    Objective Loss 0.071682                                        LR 0.001000    Time 0.076033    
Epoch: [56][  170/  207]    Overall Loss 0.071476    Objective Loss 0.071476                                        LR 0.001000    Time 0.075947    
Epoch: [56][  180/  207]    Overall Loss 0.071247    Objective Loss 0.071247                                        LR 0.001000    Time 0.075803    
Epoch: [56][  190/  207]    Overall Loss 0.071303    Objective Loss 0.071303                                        LR 0.001000    Time 0.075699    
Epoch: [56][  200/  207]    Overall Loss 0.071057    Objective Loss 0.071057                                        LR 0.001000    Time 0.075565    
Epoch: [56][  207/  207]    Overall Loss 0.070801    Objective Loss 0.070801    Top1 96.262741    Top5 100.000000    LR 0.001000    Time 0.075436    
--- validate (epoch=56)-----------
5136 samples (512 per mini-batch)
Epoch: [56][   10/   11]    Loss 0.517741    Top1 83.359375    Top5 99.628906    
Epoch: [56][   11/   11]    Loss 0.473701    Top1 83.411215    Top5 99.630062    
==> Top1: 83.411    Top5: 99.630    Loss: 0.474

==> Confusion:
[[262   4   4   1   0   2   3  24]
 [  3 265  20   1   0   1   0  10]
 [  2  19 265   1   0   3   0  10]
 [  0   3   0 755  43  11   6  19]
 [  1   0   0  59 764   7  24  24]
 [  5   8   8  20  19 772  15  47]
 [  1   0   0   7  29  11 765  24]
 [ 19  20  16  63  62 135  38 436]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [57][   10/  207]    Overall Loss 0.058202    Objective Loss 0.058202                                        LR 0.001000    Time 0.143924    
Epoch: [57][   20/  207]    Overall Loss 0.057492    Objective Loss 0.057492                                        LR 0.001000    Time 0.115158    
Epoch: [57][   30/  207]    Overall Loss 0.056072    Objective Loss 0.056072                                        LR 0.001000    Time 0.101911    
Epoch: [57][   40/  207]    Overall Loss 0.057966    Objective Loss 0.057966                                        LR 0.001000    Time 0.094838    
Epoch: [57][   50/  207]    Overall Loss 0.058169    Objective Loss 0.058169                                        LR 0.001000    Time 0.090812    
Epoch: [57][   60/  207]    Overall Loss 0.058164    Objective Loss 0.058164                                        LR 0.001000    Time 0.088919    
Epoch: [57][   70/  207]    Overall Loss 0.057745    Objective Loss 0.057745                                        LR 0.001000    Time 0.086578    
Epoch: [57][   80/  207]    Overall Loss 0.058209    Objective Loss 0.058209                                        LR 0.001000    Time 0.085327    
Epoch: [57][   90/  207]    Overall Loss 0.058891    Objective Loss 0.058891                                        LR 0.001000    Time 0.084427    
Epoch: [57][  100/  207]    Overall Loss 0.059499    Objective Loss 0.059499                                        LR 0.001000    Time 0.083672    
Epoch: [57][  110/  207]    Overall Loss 0.059682    Objective Loss 0.059682                                        LR 0.001000    Time 0.083054    
Epoch: [57][  120/  207]    Overall Loss 0.059972    Objective Loss 0.059972                                        LR 0.001000    Time 0.082106    
Epoch: [57][  130/  207]    Overall Loss 0.060220    Objective Loss 0.060220                                        LR 0.001000    Time 0.081630    
Epoch: [57][  140/  207]    Overall Loss 0.061110    Objective Loss 0.061110                                        LR 0.001000    Time 0.081355    
Epoch: [57][  150/  207]    Overall Loss 0.061396    Objective Loss 0.061396                                        LR 0.001000    Time 0.081475    
Epoch: [57][  160/  207]    Overall Loss 0.061573    Objective Loss 0.061573                                        LR 0.001000    Time 0.081213    
Epoch: [57][  170/  207]    Overall Loss 0.061702    Objective Loss 0.061702                                        LR 0.001000    Time 0.080955    
Epoch: [57][  180/  207]    Overall Loss 0.062427    Objective Loss 0.062427                                        LR 0.001000    Time 0.081779    
Epoch: [57][  190/  207]    Overall Loss 0.063190    Objective Loss 0.063190                                        LR 0.001000    Time 0.081624    
Epoch: [57][  200/  207]    Overall Loss 0.063814    Objective Loss 0.063814                                        LR 0.001000    Time 0.081451    
Epoch: [57][  207/  207]    Overall Loss 0.064170    Objective Loss 0.064170    Top1 95.130238    Top5 100.000000    LR 0.001000    Time 0.081204    
--- validate (epoch=57)-----------
5136 samples (512 per mini-batch)
Epoch: [57][   10/   11]    Loss 0.473216    Top1 82.617188    Top5 99.765625    
Epoch: [57][   11/   11]    Loss 0.435745    Top1 82.651869    Top5 99.766355    
==> Top1: 82.652    Top5: 99.766    Loss: 0.436

==> Confusion:
[[274   7   2   3   0   2   3   9]
 [  1 264  26   4   0   2   0   3]
 [  4  24 264   1   0   3   0   4]
 [  0   4   0 775  43   2   9   4]
 [  2   0   0  54 784   2  25  12]
 [  8   8  12  23  31 755  26  31]
 [  2   1   0   8  38   5 772  11]
 [ 35  27  22  89  84 128  47 357]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [58][   10/  207]    Overall Loss 0.062013    Objective Loss 0.062013                                        LR 0.001000    Time 0.159799    
Epoch: [58][   20/  207]    Overall Loss 0.061426    Objective Loss 0.061426                                        LR 0.001000    Time 0.119314    
Epoch: [58][   30/  207]    Overall Loss 0.061491    Objective Loss 0.061491                                        LR 0.001000    Time 0.105609    
Epoch: [58][   40/  207]    Overall Loss 0.062548    Objective Loss 0.062548                                        LR 0.001000    Time 0.098877    
Epoch: [58][   50/  207]    Overall Loss 0.062394    Objective Loss 0.062394                                        LR 0.001000    Time 0.094633    
Epoch: [58][   60/  207]    Overall Loss 0.063662    Objective Loss 0.063662                                        LR 0.001000    Time 0.091763    
Epoch: [58][   70/  207]    Overall Loss 0.062391    Objective Loss 0.062391                                        LR 0.001000    Time 0.089751    
Epoch: [58][   80/  207]    Overall Loss 0.063763    Objective Loss 0.063763                                        LR 0.001000    Time 0.088335    
Epoch: [58][   90/  207]    Overall Loss 0.064694    Objective Loss 0.064694                                        LR 0.001000    Time 0.087149    
Epoch: [58][  100/  207]    Overall Loss 0.065567    Objective Loss 0.065567                                        LR 0.001000    Time 0.086206    
Epoch: [58][  110/  207]    Overall Loss 0.067077    Objective Loss 0.067077                                        LR 0.001000    Time 0.085460    
Epoch: [58][  120/  207]    Overall Loss 0.069227    Objective Loss 0.069227                                        LR 0.001000    Time 0.084901    
Epoch: [58][  130/  207]    Overall Loss 0.072844    Objective Loss 0.072844                                        LR 0.001000    Time 0.084343    
Epoch: [58][  140/  207]    Overall Loss 0.076555    Objective Loss 0.076555                                        LR 0.001000    Time 0.083871    
Epoch: [58][  150/  207]    Overall Loss 0.080492    Objective Loss 0.080492                                        LR 0.001000    Time 0.083089    
Epoch: [58][  160/  207]    Overall Loss 0.084664    Objective Loss 0.084664                                        LR 0.001000    Time 0.082415    
Epoch: [58][  170/  207]    Overall Loss 0.088199    Objective Loss 0.088199                                        LR 0.001000    Time 0.082093    
Epoch: [58][  180/  207]    Overall Loss 0.091888    Objective Loss 0.091888                                        LR 0.001000    Time 0.081559    
Epoch: [58][  190/  207]    Overall Loss 0.094867    Objective Loss 0.094867                                        LR 0.001000    Time 0.081136    
Epoch: [58][  200/  207]    Overall Loss 0.097646    Objective Loss 0.097646                                        LR 0.001000    Time 0.080646    
Epoch: [58][  207/  207]    Overall Loss 0.098610    Objective Loss 0.098610    Top1 95.016988    Top5 100.000000    LR 0.001000    Time 0.080208    
--- validate (epoch=58)-----------
5136 samples (512 per mini-batch)
Epoch: [58][   10/   11]    Loss 0.538011    Top1 81.542969    Top5 99.765625    
Epoch: [58][   11/   11]    Loss 0.568697    Top1 81.561526    Top5 99.746885    
==> Top1: 81.562    Top5: 99.747    Loss: 0.569

==> Confusion:
[[280   2   1   2   0   2   4   9]
 [  8 264  19   0   0   2   0   7]
 [  8  32 252   0   0   2   0   6]
 [  2   4   0 749  42   8  13  19]
 [  4   2   0  73 720   5  56  19]
 [ 25   5  10  10  12 747  47  38]
 [  4   0   0   6  18   8 785  16]
 [ 59  25  17  49  56 128  63 392]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [59][   10/  207]    Overall Loss 0.111303    Objective Loss 0.111303                                        LR 0.001000    Time 0.129399    
Epoch: [59][   20/  207]    Overall Loss 0.110231    Objective Loss 0.110231                                        LR 0.001000    Time 0.099846    
Epoch: [59][   30/  207]    Overall Loss 0.114548    Objective Loss 0.114548                                        LR 0.001000    Time 0.089926    
Epoch: [59][   40/  207]    Overall Loss 0.119380    Objective Loss 0.119380                                        LR 0.001000    Time 0.086058    
Epoch: [59][   50/  207]    Overall Loss 0.122409    Objective Loss 0.122409                                        LR 0.001000    Time 0.083335    
Epoch: [59][   60/  207]    Overall Loss 0.126377    Objective Loss 0.126377                                        LR 0.001000    Time 0.081760    
Epoch: [59][   70/  207]    Overall Loss 0.124310    Objective Loss 0.124310                                        LR 0.001000    Time 0.080367    
Epoch: [59][   80/  207]    Overall Loss 0.126760    Objective Loss 0.126760                                        LR 0.001000    Time 0.079301    
Epoch: [59][   90/  207]    Overall Loss 0.126519    Objective Loss 0.126519                                        LR 0.001000    Time 0.079606    
Epoch: [59][  100/  207]    Overall Loss 0.127091    Objective Loss 0.127091                                        LR 0.001000    Time 0.078810    
Epoch: [59][  110/  207]    Overall Loss 0.127492    Objective Loss 0.127492                                        LR 0.001000    Time 0.078201    
Epoch: [59][  120/  207]    Overall Loss 0.128989    Objective Loss 0.128989                                        LR 0.001000    Time 0.077775    
Epoch: [59][  130/  207]    Overall Loss 0.132672    Objective Loss 0.132672                                        LR 0.001000    Time 0.077341    
Epoch: [59][  140/  207]    Overall Loss 0.134114    Objective Loss 0.134114                                        LR 0.001000    Time 0.077039    
Epoch: [59][  150/  207]    Overall Loss 0.134149    Objective Loss 0.134149                                        LR 0.001000    Time 0.076715    
Epoch: [59][  160/  207]    Overall Loss 0.133412    Objective Loss 0.133412                                        LR 0.001000    Time 0.076457    
Epoch: [59][  170/  207]    Overall Loss 0.132712    Objective Loss 0.132712                                        LR 0.001000    Time 0.076258    
Epoch: [59][  180/  207]    Overall Loss 0.130936    Objective Loss 0.130936                                        LR 0.001000    Time 0.076081    
Epoch: [59][  190/  207]    Overall Loss 0.130334    Objective Loss 0.130334                                        LR 0.001000    Time 0.075836    
Epoch: [59][  200/  207]    Overall Loss 0.129535    Objective Loss 0.129535                                        LR 0.001000    Time 0.075559    
Epoch: [59][  207/  207]    Overall Loss 0.129547    Objective Loss 0.129547    Top1 92.412231    Top5 99.886750    LR 0.001000    Time 0.075288    
--- validate (epoch=59)-----------
5136 samples (512 per mini-batch)
Epoch: [59][   10/   11]    Loss 0.534588    Top1 80.000000    Top5 99.707031    
Epoch: [59][   11/   11]    Loss 0.644407    Top1 80.023364    Top5 99.707944    
==> Top1: 80.023    Top5: 99.708    Loss: 0.644

==> Confusion:
[[290   1   1   0   0   2   3   3]
 [ 19 225  39   1   1   5   1   9]
 [ 12  15 262   0   0   7   0   4]
 [  2   3   0 727  66  12  18   9]
 [  3   0   0  42 774   5  39  16]
 [ 28   2  12  13  20 782  19  18]
 [  4   0   0   4  23   9 784  13]
 [110  14  25  57  90 180  47 266]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [60][   10/  207]    Overall Loss 0.099260    Objective Loss 0.099260                                        LR 0.001000    Time 0.128269    
Epoch: [60][   20/  207]    Overall Loss 0.095928    Objective Loss 0.095928                                        LR 0.001000    Time 0.099435    
Epoch: [60][   30/  207]    Overall Loss 0.091903    Objective Loss 0.091903                                        LR 0.001000    Time 0.090649    
Epoch: [60][   40/  207]    Overall Loss 0.090725    Objective Loss 0.090725                                        LR 0.001000    Time 0.085582    
Epoch: [60][   50/  207]    Overall Loss 0.094749    Objective Loss 0.094749                                        LR 0.001000    Time 0.082964    
Epoch: [60][   60/  207]    Overall Loss 0.095378    Objective Loss 0.095378                                        LR 0.001000    Time 0.081080    
Epoch: [60][   70/  207]    Overall Loss 0.097326    Objective Loss 0.097326                                        LR 0.001000    Time 0.079902    
Epoch: [60][   80/  207]    Overall Loss 0.098663    Objective Loss 0.098663                                        LR 0.001000    Time 0.078989    
Epoch: [60][   90/  207]    Overall Loss 0.098039    Objective Loss 0.098039                                        LR 0.001000    Time 0.078299    
Epoch: [60][  100/  207]    Overall Loss 0.096833    Objective Loss 0.096833                                        LR 0.001000    Time 0.077564    
Epoch: [60][  110/  207]    Overall Loss 0.098561    Objective Loss 0.098561                                        LR 0.001000    Time 0.076878    
Epoch: [60][  120/  207]    Overall Loss 0.103946    Objective Loss 0.103946                                        LR 0.001000    Time 0.076381    
Epoch: [60][  130/  207]    Overall Loss 0.106361    Objective Loss 0.106361                                        LR 0.001000    Time 0.075982    
Epoch: [60][  140/  207]    Overall Loss 0.106818    Objective Loss 0.106818                                        LR 0.001000    Time 0.075578    
Epoch: [60][  150/  207]    Overall Loss 0.107964    Objective Loss 0.107964                                        LR 0.001000    Time 0.075287    
Epoch: [60][  160/  207]    Overall Loss 0.108702    Objective Loss 0.108702                                        LR 0.001000    Time 0.075159    
Epoch: [60][  170/  207]    Overall Loss 0.108300    Objective Loss 0.108300                                        LR 0.001000    Time 0.074987    
Epoch: [60][  180/  207]    Overall Loss 0.108528    Objective Loss 0.108528                                        LR 0.001000    Time 0.074740    
Epoch: [60][  190/  207]    Overall Loss 0.108731    Objective Loss 0.108731                                        LR 0.001000    Time 0.074488    
Epoch: [60][  200/  207]    Overall Loss 0.108881    Objective Loss 0.108881                                        LR 0.001000    Time 0.074402    
Epoch: [60][  207/  207]    Overall Loss 0.108658    Objective Loss 0.108658    Top1 93.091733    Top5 99.886750    LR 0.001000    Time 0.074168    
--- validate (epoch=60)-----------
5136 samples (512 per mini-batch)
Epoch: [60][   10/   11]    Loss 0.504138    Top1 81.406250    Top5 99.765625    
Epoch: [60][   11/   11]    Loss 0.475231    Top1 81.386293    Top5 99.766355    
==> Top1: 81.386    Top5: 99.766    Loss: 0.475

==> Confusion:
[[281   2   3   1   1   0   1  11]
 [  5 258  33   0   0   1   0   3]
 [  5  24 261   0   1   4   0   5]
 [  1   3   1 699  91  15  12  15]
 [  2   0   1  23 789   4  36  24]
 [ 17   4  12  11  32 764  19  35]
 [  5   0   0   1  41  11 757  22]
 [ 71  18  21  42  86 138  42 371]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [61][   10/  207]    Overall Loss 0.078856    Objective Loss 0.078856                                        LR 0.001000    Time 0.129805    
Epoch: [61][   20/  207]    Overall Loss 0.073778    Objective Loss 0.073778                                        LR 0.001000    Time 0.099691    
Epoch: [61][   30/  207]    Overall Loss 0.072581    Objective Loss 0.072581                                        LR 0.001000    Time 0.089761    
Epoch: [61][   40/  207]    Overall Loss 0.072615    Objective Loss 0.072615                                        LR 0.001000    Time 0.085122    
Epoch: [61][   50/  207]    Overall Loss 0.071021    Objective Loss 0.071021                                        LR 0.001000    Time 0.082226    
Epoch: [61][   60/  207]    Overall Loss 0.071710    Objective Loss 0.071710                                        LR 0.001000    Time 0.080846    
Epoch: [61][   70/  207]    Overall Loss 0.071054    Objective Loss 0.071054                                        LR 0.001000    Time 0.079462    
Epoch: [61][   80/  207]    Overall Loss 0.070353    Objective Loss 0.070353                                        LR 0.001000    Time 0.078463    
Epoch: [61][   90/  207]    Overall Loss 0.069605    Objective Loss 0.069605                                        LR 0.001000    Time 0.077732    
Epoch: [61][  100/  207]    Overall Loss 0.070205    Objective Loss 0.070205                                        LR 0.001000    Time 0.077177    
Epoch: [61][  110/  207]    Overall Loss 0.071044    Objective Loss 0.071044                                        LR 0.001000    Time 0.076607    
Epoch: [61][  120/  207]    Overall Loss 0.071581    Objective Loss 0.071581                                        LR 0.001000    Time 0.076268    
Epoch: [61][  130/  207]    Overall Loss 0.071821    Objective Loss 0.071821                                        LR 0.001000    Time 0.075906    
Epoch: [61][  140/  207]    Overall Loss 0.072593    Objective Loss 0.072593                                        LR 0.001000    Time 0.075547    
Epoch: [61][  150/  207]    Overall Loss 0.073524    Objective Loss 0.073524                                        LR 0.001000    Time 0.075208    
Epoch: [61][  160/  207]    Overall Loss 0.075870    Objective Loss 0.075870                                        LR 0.001000    Time 0.074986    
Epoch: [61][  170/  207]    Overall Loss 0.077522    Objective Loss 0.077522                                        LR 0.001000    Time 0.074737    
Epoch: [61][  180/  207]    Overall Loss 0.078778    Objective Loss 0.078778                                        LR 0.001000    Time 0.074441    
Epoch: [61][  190/  207]    Overall Loss 0.079766    Objective Loss 0.079766                                        LR 0.001000    Time 0.074271    
Epoch: [61][  200/  207]    Overall Loss 0.080168    Objective Loss 0.080168                                        LR 0.001000    Time 0.074179    
Epoch: [61][  207/  207]    Overall Loss 0.080441    Objective Loss 0.080441    Top1 94.903737    Top5 100.000000    LR 0.001000    Time 0.073970    
--- validate (epoch=61)-----------
5136 samples (512 per mini-batch)
Epoch: [61][   10/   11]    Loss 0.510662    Top1 82.128906    Top5 99.687500    
Epoch: [61][   11/   11]    Loss 0.498443    Top1 82.087227    Top5 99.688474    
==> Top1: 82.087    Top5: 99.688    Loss: 0.498

==> Confusion:
[[280   3   6   1   0   0   2   8]
 [  3 262  30   0   0   1   0   4]
 [  1  22 273   1   0   1   0   2]
 [  1   4   0 771  39   5   7  10]
 [  1   0   0  84 751   5  19  19]
 [ 26   5  17  23  23 749  13  38]
 [  2   0   0   9  40  13 753  20]
 [ 55  24  26  67  84 135  21 377]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [62][   10/  207]    Overall Loss 0.081919    Objective Loss 0.081919                                        LR 0.001000    Time 0.126908    
Epoch: [62][   20/  207]    Overall Loss 0.088860    Objective Loss 0.088860                                        LR 0.001000    Time 0.098407    
Epoch: [62][   30/  207]    Overall Loss 0.088720    Objective Loss 0.088720                                        LR 0.001000    Time 0.089416    
Epoch: [62][   40/  207]    Overall Loss 0.092534    Objective Loss 0.092534                                        LR 0.001000    Time 0.085217    
Epoch: [62][   50/  207]    Overall Loss 0.095027    Objective Loss 0.095027                                        LR 0.001000    Time 0.082832    
Epoch: [62][   60/  207]    Overall Loss 0.094355    Objective Loss 0.094355                                        LR 0.001000    Time 0.081188    
Epoch: [62][   70/  207]    Overall Loss 0.092806    Objective Loss 0.092806                                        LR 0.001000    Time 0.079553    
Epoch: [62][   80/  207]    Overall Loss 0.091976    Objective Loss 0.091976                                        LR 0.001000    Time 0.078504    
Epoch: [62][   90/  207]    Overall Loss 0.091756    Objective Loss 0.091756                                        LR 0.001000    Time 0.077744    
Epoch: [62][  100/  207]    Overall Loss 0.090443    Objective Loss 0.090443                                        LR 0.001000    Time 0.077183    
Epoch: [62][  110/  207]    Overall Loss 0.089243    Objective Loss 0.089243                                        LR 0.001000    Time 0.076677    
Epoch: [62][  120/  207]    Overall Loss 0.088758    Objective Loss 0.088758                                        LR 0.001000    Time 0.076270    
Epoch: [62][  130/  207]    Overall Loss 0.087524    Objective Loss 0.087524                                        LR 0.001000    Time 0.075843    
Epoch: [62][  140/  207]    Overall Loss 0.087527    Objective Loss 0.087527                                        LR 0.001000    Time 0.075530    
Epoch: [62][  150/  207]    Overall Loss 0.086413    Objective Loss 0.086413                                        LR 0.001000    Time 0.075342    
Epoch: [62][  160/  207]    Overall Loss 0.086017    Objective Loss 0.086017                                        LR 0.001000    Time 0.075108    
Epoch: [62][  170/  207]    Overall Loss 0.085744    Objective Loss 0.085744                                        LR 0.001000    Time 0.074849    
Epoch: [62][  180/  207]    Overall Loss 0.085709    Objective Loss 0.085709                                        LR 0.001000    Time 0.074574    
Epoch: [62][  190/  207]    Overall Loss 0.085174    Objective Loss 0.085174                                        LR 0.001000    Time 0.074391    
Epoch: [62][  200/  207]    Overall Loss 0.084573    Objective Loss 0.084573                                        LR 0.001000    Time 0.074262    
Epoch: [62][  207/  207]    Overall Loss 0.084373    Objective Loss 0.084373    Top1 93.997735    Top5 100.000000    LR 0.001000    Time 0.074012    
--- validate (epoch=62)-----------
5136 samples (512 per mini-batch)
Epoch: [62][   10/   11]    Loss 0.491650    Top1 82.187500    Top5 99.707031    
Epoch: [62][   11/   11]    Loss 0.470920    Top1 82.223520    Top5 99.707944    
==> Top1: 82.224    Top5: 99.708    Loss: 0.471

==> Confusion:
[[277   6   3   2   0   1   2   9]
 [  4 268  21   0   0   1   0   6]
 [  2  24 271   1   0   1   0   1]
 [  0   6   0 742  52  10  15  12]
 [  3   0   0  42 777   5  40  12]
 [ 14   7  14  21  24 761  29  24]
 [  4   0   0   6  26  10 781  10]
 [ 50  25  18  69  76 136  69 346]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [63][   10/  207]    Overall Loss 0.063439    Objective Loss 0.063439                                        LR 0.001000    Time 0.128650    
Epoch: [63][   20/  207]    Overall Loss 0.066977    Objective Loss 0.066977                                        LR 0.001000    Time 0.100684    
Epoch: [63][   30/  207]    Overall Loss 0.067267    Objective Loss 0.067267                                        LR 0.001000    Time 0.091396    
Epoch: [63][   40/  207]    Overall Loss 0.063887    Objective Loss 0.063887                                        LR 0.001000    Time 0.087363    
Epoch: [63][   50/  207]    Overall Loss 0.063502    Objective Loss 0.063502                                        LR 0.001000    Time 0.085176    
Epoch: [63][   60/  207]    Overall Loss 0.062860    Objective Loss 0.062860                                        LR 0.001000    Time 0.084213    
Epoch: [63][   70/  207]    Overall Loss 0.062676    Objective Loss 0.062676                                        LR 0.001000    Time 0.083456    
Epoch: [63][   80/  207]    Overall Loss 0.062375    Objective Loss 0.062375                                        LR 0.001000    Time 0.082798    
Epoch: [63][   90/  207]    Overall Loss 0.062492    Objective Loss 0.062492                                        LR 0.001000    Time 0.082250    
Epoch: [63][  100/  207]    Overall Loss 0.061732    Objective Loss 0.061732                                        LR 0.001000    Time 0.081742    
Epoch: [63][  110/  207]    Overall Loss 0.060998    Objective Loss 0.060998                                        LR 0.001000    Time 0.083190    
Epoch: [63][  120/  207]    Overall Loss 0.060846    Objective Loss 0.060846                                        LR 0.001000    Time 0.082788    
Epoch: [63][  130/  207]    Overall Loss 0.060414    Objective Loss 0.060414                                        LR 0.001000    Time 0.082437    
Epoch: [63][  140/  207]    Overall Loss 0.060063    Objective Loss 0.060063                                        LR 0.001000    Time 0.082103    
Epoch: [63][  150/  207]    Overall Loss 0.060306    Objective Loss 0.060306                                        LR 0.001000    Time 0.082195    
Epoch: [63][  160/  207]    Overall Loss 0.060285    Objective Loss 0.060285                                        LR 0.001000    Time 0.082106    
Epoch: [63][  170/  207]    Overall Loss 0.060484    Objective Loss 0.060484                                        LR 0.001000    Time 0.081869    
Epoch: [63][  180/  207]    Overall Loss 0.060431    Objective Loss 0.060431                                        LR 0.001000    Time 0.081679    
Epoch: [63][  190/  207]    Overall Loss 0.060771    Objective Loss 0.060771                                        LR 0.001000    Time 0.081482    
Epoch: [63][  200/  207]    Overall Loss 0.061105    Objective Loss 0.061105                                        LR 0.001000    Time 0.081317    
Epoch: [63][  207/  207]    Overall Loss 0.061549    Objective Loss 0.061549    Top1 95.016988    Top5 99.886750    LR 0.001000    Time 0.081074    
--- validate (epoch=63)-----------
5136 samples (512 per mini-batch)
Epoch: [63][   10/   11]    Loss 0.455223    Top1 81.816406    Top5 99.609375    
Epoch: [63][   11/   11]    Loss 0.449556    Top1 81.756231    Top5 99.610592    
==> Top1: 81.756    Top5: 99.611    Loss: 0.450

==> Confusion:
[[277   9   4   1   0   2   0   7]
 [  2 274  22   0   0   1   0   1]
 [  3  26 270   0   0   1   0   0]
 [  1   8   0 741  45  18  11  13]
 [  2   0   0  56 742  16  51  12]
 [ 17  10  18  11   7 795  14  22]
 [  5   0   0   3  15  18 785  11]
 [ 58  35  26  43  64 197  51 315]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [64][   10/  207]    Overall Loss 0.059983    Objective Loss 0.059983                                        LR 0.001000    Time 0.161620    
Epoch: [64][   20/  207]    Overall Loss 0.061535    Objective Loss 0.061535                                        LR 0.001000    Time 0.119904    
Epoch: [64][   30/  207]    Overall Loss 0.060545    Objective Loss 0.060545                                        LR 0.001000    Time 0.105913    
Epoch: [64][   40/  207]    Overall Loss 0.062666    Objective Loss 0.062666                                        LR 0.001000    Time 0.098796    
Epoch: [64][   50/  207]    Overall Loss 0.064002    Objective Loss 0.064002                                        LR 0.001000    Time 0.094267    
Epoch: [64][   60/  207]    Overall Loss 0.064374    Objective Loss 0.064374                                        LR 0.001000    Time 0.090619    
Epoch: [64][   70/  207]    Overall Loss 0.066524    Objective Loss 0.066524                                        LR 0.001000    Time 0.087868    
Epoch: [64][   80/  207]    Overall Loss 0.068019    Objective Loss 0.068019                                        LR 0.001000    Time 0.085921    
Epoch: [64][   90/  207]    Overall Loss 0.070384    Objective Loss 0.070384                                        LR 0.001000    Time 0.084175    
Epoch: [64][  100/  207]    Overall Loss 0.074817    Objective Loss 0.074817                                        LR 0.001000    Time 0.082909    
Epoch: [64][  110/  207]    Overall Loss 0.080369    Objective Loss 0.080369                                        LR 0.001000    Time 0.081900    
Epoch: [64][  120/  207]    Overall Loss 0.084419    Objective Loss 0.084419                                        LR 0.001000    Time 0.081041    
Epoch: [64][  130/  207]    Overall Loss 0.090215    Objective Loss 0.090215                                        LR 0.001000    Time 0.080482    
Epoch: [64][  140/  207]    Overall Loss 0.098381    Objective Loss 0.098381                                        LR 0.001000    Time 0.080005    
Epoch: [64][  150/  207]    Overall Loss 0.105247    Objective Loss 0.105247                                        LR 0.001000    Time 0.079359    
Epoch: [64][  160/  207]    Overall Loss 0.110026    Objective Loss 0.110026                                        LR 0.001000    Time 0.078836    
Epoch: [64][  170/  207]    Overall Loss 0.114387    Objective Loss 0.114387                                        LR 0.001000    Time 0.078306    
Epoch: [64][  180/  207]    Overall Loss 0.118665    Objective Loss 0.118665                                        LR 0.001000    Time 0.078032    
Epoch: [64][  190/  207]    Overall Loss 0.123019    Objective Loss 0.123019                                        LR 0.001000    Time 0.077645    
Epoch: [64][  200/  207]    Overall Loss 0.125641    Objective Loss 0.125641                                        LR 0.001000    Time 0.077371    
Epoch: [64][  207/  207]    Overall Loss 0.126851    Objective Loss 0.126851    Top1 92.412231    Top5 100.000000    LR 0.001000    Time 0.077073    
--- validate (epoch=64)-----------
5136 samples (512 per mini-batch)
Epoch: [64][   10/   11]    Loss 0.552944    Top1 79.316406    Top5 99.589844    
Epoch: [64][   11/   11]    Loss 0.606289    Top1 79.302960    Top5 99.591121    
==> Top1: 79.303    Top5: 99.591    Loss: 0.606

==> Confusion:
[[250   7   3   4   6  11   3  16]
 [  2 255  27   3   1   5   1   6]
 [  1  26 253   1   2  14   0   3]
 [  0   3   1 757  63   7   5   1]
 [  0   0   0  53 791   4  22   9]
 [  4   7   4  24  42 775  28  10]
 [  1   0   0   7  47  10 763   9]
 [ 22  15  19 102  98 216  88 229]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [65][   10/  207]    Overall Loss 0.141140    Objective Loss 0.141140                                        LR 0.001000    Time 0.129493    
Epoch: [65][   20/  207]    Overall Loss 0.143817    Objective Loss 0.143817                                        LR 0.001000    Time 0.100409    
Epoch: [65][   30/  207]    Overall Loss 0.143364    Objective Loss 0.143364                                        LR 0.001000    Time 0.090512    
Epoch: [65][   40/  207]    Overall Loss 0.141226    Objective Loss 0.141226                                        LR 0.001000    Time 0.085758    
Epoch: [65][   50/  207]    Overall Loss 0.138074    Objective Loss 0.138074                                        LR 0.001000    Time 0.083141    
Epoch: [65][   60/  207]    Overall Loss 0.133644    Objective Loss 0.133644                                        LR 0.001000    Time 0.081103    
Epoch: [65][   70/  207]    Overall Loss 0.128478    Objective Loss 0.128478                                        LR 0.001000    Time 0.079845    
Epoch: [65][   80/  207]    Overall Loss 0.122885    Objective Loss 0.122885                                        LR 0.001000    Time 0.078605    
Epoch: [65][   90/  207]    Overall Loss 0.119200    Objective Loss 0.119200                                        LR 0.001000    Time 0.077803    
Epoch: [65][  100/  207]    Overall Loss 0.117019    Objective Loss 0.117019                                        LR 0.001000    Time 0.077087    
Epoch: [65][  110/  207]    Overall Loss 0.115046    Objective Loss 0.115046                                        LR 0.001000    Time 0.076596    
Epoch: [65][  120/  207]    Overall Loss 0.112866    Objective Loss 0.112866                                        LR 0.001000    Time 0.076127    
Epoch: [65][  130/  207]    Overall Loss 0.110982    Objective Loss 0.110982                                        LR 0.001000    Time 0.075780    
Epoch: [65][  140/  207]    Overall Loss 0.110862    Objective Loss 0.110862                                        LR 0.001000    Time 0.075494    
Epoch: [65][  150/  207]    Overall Loss 0.109852    Objective Loss 0.109852                                        LR 0.001000    Time 0.075190    
Epoch: [65][  160/  207]    Overall Loss 0.108973    Objective Loss 0.108973                                        LR 0.001000    Time 0.075037    
Epoch: [65][  170/  207]    Overall Loss 0.108455    Objective Loss 0.108455                                        LR 0.001000    Time 0.074794    
Epoch: [65][  180/  207]    Overall Loss 0.107444    Objective Loss 0.107444                                        LR 0.001000    Time 0.074644    
Epoch: [65][  190/  207]    Overall Loss 0.106511    Objective Loss 0.106511                                        LR 0.001000    Time 0.074533    
Epoch: [65][  200/  207]    Overall Loss 0.105978    Objective Loss 0.105978                                        LR 0.001000    Time 0.074350    
Epoch: [65][  207/  207]    Overall Loss 0.105577    Objective Loss 0.105577    Top1 94.110985    Top5 100.000000    LR 0.001000    Time 0.074115    
--- validate (epoch=65)-----------
5136 samples (512 per mini-batch)
Epoch: [65][   10/   11]    Loss 0.517552    Top1 81.191406    Top5 99.785156    
Epoch: [65][   11/   11]    Loss 0.488705    Top1 81.191589    Top5 99.785826    
==> Top1: 81.192    Top5: 99.786    Loss: 0.489

==> Confusion:
[[278   7   2   2   0   0   2   9]
 [  6 255  31   1   0   1   1   5]
 [  6  23 264   0   0   1   0   6]
 [  1   3   0 742  44  12  14  21]
 [  3   0   0  68 750   8  28  22]
 [ 23   4  12  11  23 775  15  31]
 [  3   0   0   6  29  20 756  23]
 [ 76  19  25  52  72 155  40 350]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [66][   10/  207]    Overall Loss 0.075280    Objective Loss 0.075280                                        LR 0.001000    Time 0.120366    
Epoch: [66][   20/  207]    Overall Loss 0.075554    Objective Loss 0.075554                                        LR 0.001000    Time 0.095670    
Epoch: [66][   30/  207]    Overall Loss 0.072983    Objective Loss 0.072983                                        LR 0.001000    Time 0.087096    
Epoch: [66][   40/  207]    Overall Loss 0.072738    Objective Loss 0.072738                                        LR 0.001000    Time 0.083223    
Epoch: [66][   50/  207]    Overall Loss 0.071780    Objective Loss 0.071780                                        LR 0.001000    Time 0.080722    
Epoch: [66][   60/  207]    Overall Loss 0.071919    Objective Loss 0.071919                                        LR 0.001000    Time 0.079266    
Epoch: [66][   70/  207]    Overall Loss 0.069927    Objective Loss 0.069927                                        LR 0.001000    Time 0.078365    
Epoch: [66][   80/  207]    Overall Loss 0.068529    Objective Loss 0.068529                                        LR 0.001000    Time 0.077528    
Epoch: [66][   90/  207]    Overall Loss 0.068359    Objective Loss 0.068359                                        LR 0.001000    Time 0.076818    
Epoch: [66][  100/  207]    Overall Loss 0.067563    Objective Loss 0.067563                                        LR 0.001000    Time 0.076189    
Epoch: [66][  110/  207]    Overall Loss 0.068685    Objective Loss 0.068685                                        LR 0.001000    Time 0.075719    
Epoch: [66][  120/  207]    Overall Loss 0.069483    Objective Loss 0.069483                                        LR 0.001000    Time 0.075298    
Epoch: [66][  130/  207]    Overall Loss 0.070070    Objective Loss 0.070070                                        LR 0.001000    Time 0.075055    
Epoch: [66][  140/  207]    Overall Loss 0.070044    Objective Loss 0.070044                                        LR 0.001000    Time 0.074792    
Epoch: [66][  150/  207]    Overall Loss 0.070171    Objective Loss 0.070171                                        LR 0.001000    Time 0.074606    
Epoch: [66][  160/  207]    Overall Loss 0.069916    Objective Loss 0.069916                                        LR 0.001000    Time 0.074363    
Epoch: [66][  170/  207]    Overall Loss 0.070228    Objective Loss 0.070228                                        LR 0.001000    Time 0.074116    
Epoch: [66][  180/  207]    Overall Loss 0.070249    Objective Loss 0.070249                                        LR 0.001000    Time 0.073889    
Epoch: [66][  190/  207]    Overall Loss 0.070022    Objective Loss 0.070022                                        LR 0.001000    Time 0.073691    
Epoch: [66][  200/  207]    Overall Loss 0.069884    Objective Loss 0.069884                                        LR 0.001000    Time 0.073543    
Epoch: [66][  207/  207]    Overall Loss 0.069939    Objective Loss 0.069939    Top1 93.204983    Top5 100.000000    LR 0.001000    Time 0.073339    
--- validate (epoch=66)-----------
5136 samples (512 per mini-batch)
Epoch: [66][   10/   11]    Loss 0.491999    Top1 82.207031    Top5 99.726562    
Epoch: [66][   11/   11]    Loss 0.516158    Top1 82.242991    Top5 99.727414    
==> Top1: 82.243    Top5: 99.727    Loss: 0.516

==> Confusion:
[[281   3   4   1   0   4   1   6]
 [  4 243  48   0   0   3   0   2]
 [  2  15 274   0   0   8   0   1]
 [  1   3   0 753  42  18  11   9]
 [  3   0   0  58 765   9  26  18]
 [ 12   3  17  10  15 814  12  11]
 [  4   0   0   5  31  16 771  10]
 [ 50  23  27  53  67 207  39 323]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [67][   10/  207]    Overall Loss 0.069032    Objective Loss 0.069032                                        LR 0.001000    Time 0.128911    
Epoch: [67][   20/  207]    Overall Loss 0.066992    Objective Loss 0.066992                                        LR 0.001000    Time 0.099208    
Epoch: [67][   30/  207]    Overall Loss 0.067811    Objective Loss 0.067811                                        LR 0.001000    Time 0.089515    
Epoch: [67][   40/  207]    Overall Loss 0.068411    Objective Loss 0.068411                                        LR 0.001000    Time 0.084901    
Epoch: [67][   50/  207]    Overall Loss 0.069140    Objective Loss 0.069140                                        LR 0.001000    Time 0.082214    
Epoch: [67][   60/  207]    Overall Loss 0.068613    Objective Loss 0.068613                                        LR 0.001000    Time 0.080797    
Epoch: [67][   70/  207]    Overall Loss 0.069066    Objective Loss 0.069066                                        LR 0.001000    Time 0.079538    
Epoch: [67][   80/  207]    Overall Loss 0.069722    Objective Loss 0.069722                                        LR 0.001000    Time 0.078409    
Epoch: [67][   90/  207]    Overall Loss 0.070269    Objective Loss 0.070269                                        LR 0.001000    Time 0.077705    
Epoch: [67][  100/  207]    Overall Loss 0.072477    Objective Loss 0.072477                                        LR 0.001000    Time 0.077086    
Epoch: [67][  110/  207]    Overall Loss 0.073642    Objective Loss 0.073642                                        LR 0.001000    Time 0.076735    
Epoch: [67][  120/  207]    Overall Loss 0.073393    Objective Loss 0.073393                                        LR 0.001000    Time 0.076411    
Epoch: [67][  130/  207]    Overall Loss 0.072607    Objective Loss 0.072607                                        LR 0.001000    Time 0.076025    
Epoch: [67][  140/  207]    Overall Loss 0.072009    Objective Loss 0.072009                                        LR 0.001000    Time 0.075691    
Epoch: [67][  150/  207]    Overall Loss 0.071332    Objective Loss 0.071332                                        LR 0.001000    Time 0.075438    
Epoch: [67][  160/  207]    Overall Loss 0.071388    Objective Loss 0.071388                                        LR 0.001000    Time 0.075183    
Epoch: [67][  170/  207]    Overall Loss 0.071084    Objective Loss 0.071084                                        LR 0.001000    Time 0.075033    
Epoch: [67][  180/  207]    Overall Loss 0.073288    Objective Loss 0.073288                                        LR 0.001000    Time 0.075311    
Epoch: [67][  190/  207]    Overall Loss 0.075820    Objective Loss 0.075820                                        LR 0.001000    Time 0.075177    
Epoch: [67][  200/  207]    Overall Loss 0.080784    Objective Loss 0.080784                                        LR 0.001000    Time 0.075647    
Epoch: [67][  207/  207]    Overall Loss 0.083628    Objective Loss 0.083628    Top1 90.486976    Top5 99.886750    LR 0.001000    Time 0.075740    
--- validate (epoch=67)-----------
5136 samples (512 per mini-batch)
Epoch: [67][   10/   11]    Loss 0.542615    Top1 81.796875    Top5 99.628906    
Epoch: [67][   11/   11]    Loss 0.511376    Top1 81.775701    Top5 99.630062    
==> Top1: 81.776    Top5: 99.630    Loss: 0.511

==> Confusion:
[[278   9   4   0   0   0   0   9]
 [  3 246  48   0   0   1   0   2]
 [  2  26 269   0   0   2   0   1]
 [  1   6   1 737  63  10   6  13]
 [  2   0   0  39 775   5  34  24]
 [ 22   8  22   8  23 749  23  39]
 [  6   0   0   5  35   8 762  21]
 [ 62  33  29  46  86 111  38 384]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [68][   10/  207]    Overall Loss 0.153138    Objective Loss 0.153138                                        LR 0.001000    Time 0.193740    
Epoch: [68][   20/  207]    Overall Loss 0.157497    Objective Loss 0.157497                                        LR 0.001000    Time 0.133990    
Epoch: [68][   30/  207]    Overall Loss 0.158972    Objective Loss 0.158972                                        LR 0.001000    Time 0.113990    
Epoch: [68][   40/  207]    Overall Loss 0.157039    Objective Loss 0.157039                                        LR 0.001000    Time 0.106156    
Epoch: [68][   50/  207]    Overall Loss 0.158426    Objective Loss 0.158426                                        LR 0.001000    Time 0.103111    
Epoch: [68][   60/  207]    Overall Loss 0.161707    Objective Loss 0.161707                                        LR 0.001000    Time 0.098388    
Epoch: [68][   70/  207]    Overall Loss 0.158872    Objective Loss 0.158872                                        LR 0.001000    Time 0.096424    
Epoch: [68][   80/  207]    Overall Loss 0.154006    Objective Loss 0.154006                                        LR 0.001000    Time 0.094137    
Epoch: [68][   90/  207]    Overall Loss 0.151728    Objective Loss 0.151728                                        LR 0.001000    Time 0.091688    
Epoch: [68][  100/  207]    Overall Loss 0.146978    Objective Loss 0.146978                                        LR 0.001000    Time 0.089767    
Epoch: [68][  110/  207]    Overall Loss 0.142792    Objective Loss 0.142792                                        LR 0.001000    Time 0.088315    
Epoch: [68][  120/  207]    Overall Loss 0.138352    Objective Loss 0.138352                                        LR 0.001000    Time 0.086914    
Epoch: [68][  130/  207]    Overall Loss 0.134297    Objective Loss 0.134297                                        LR 0.001000    Time 0.087686    
Epoch: [68][  140/  207]    Overall Loss 0.130135    Objective Loss 0.130135                                        LR 0.001000    Time 0.086867    
Epoch: [68][  150/  207]    Overall Loss 0.126423    Objective Loss 0.126423                                        LR 0.001000    Time 0.085924    
Epoch: [68][  160/  207]    Overall Loss 0.123381    Objective Loss 0.123381                                        LR 0.001000    Time 0.085343    
Epoch: [68][  170/  207]    Overall Loss 0.120428    Objective Loss 0.120428                                        LR 0.001000    Time 0.085557    
Epoch: [68][  180/  207]    Overall Loss 0.118265    Objective Loss 0.118265                                        LR 0.001000    Time 0.084919    
Epoch: [68][  190/  207]    Overall Loss 0.116036    Objective Loss 0.116036                                        LR 0.001000    Time 0.084172    
Epoch: [68][  200/  207]    Overall Loss 0.114165    Objective Loss 0.114165                                        LR 0.001000    Time 0.083663    
Epoch: [68][  207/  207]    Overall Loss 0.113155    Objective Loss 0.113155    Top1 95.809740    Top5 99.660249    LR 0.001000    Time 0.083218    
--- validate (epoch=68)-----------
5136 samples (512 per mini-batch)
Epoch: [68][   10/   11]    Loss 0.515202    Top1 82.929688    Top5 99.746094    
Epoch: [68][   11/   11]    Loss 0.470827    Top1 82.963396    Top5 99.746885    
==> Top1: 82.963    Top5: 99.747    Loss: 0.471

==> Confusion:
[[263   2   4   1   0   8   2  20]
 [  1 252  36   0   0   2   1   8]
 [  1  15 271   1   0   5   0   7]
 [  1   2   0 748  45  12  13  16]
 [  1   0   0  52 766   6  30  24]
 [  5   2  10  13  12 811  14  27]
 [  1   0   0   4  22  17 775  18]
 [ 24  18  15  63  67 170  57 375]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [69][   10/  207]    Overall Loss 0.061362    Objective Loss 0.061362                                        LR 0.001000    Time 0.131065    
Epoch: [69][   20/  207]    Overall Loss 0.060287    Objective Loss 0.060287                                        LR 0.001000    Time 0.106999    
Epoch: [69][   30/  207]    Overall Loss 0.060223    Objective Loss 0.060223                                        LR 0.001000    Time 0.097195    
Epoch: [69][   40/  207]    Overall Loss 0.060684    Objective Loss 0.060684                                        LR 0.001000    Time 0.091219    
Epoch: [69][   50/  207]    Overall Loss 0.059679    Objective Loss 0.059679                                        LR 0.001000    Time 0.087917    
Epoch: [69][   60/  207]    Overall Loss 0.060212    Objective Loss 0.060212                                        LR 0.001000    Time 0.086082    
Epoch: [69][   70/  207]    Overall Loss 0.061537    Objective Loss 0.061537                                        LR 0.001000    Time 0.086399    
Epoch: [69][   80/  207]    Overall Loss 0.061691    Objective Loss 0.061691                                        LR 0.001000    Time 0.086599    
Epoch: [69][   90/  207]    Overall Loss 0.063187    Objective Loss 0.063187                                        LR 0.001000    Time 0.086564    
Epoch: [69][  100/  207]    Overall Loss 0.063488    Objective Loss 0.063488                                        LR 0.001000    Time 0.086497    
Epoch: [69][  110/  207]    Overall Loss 0.063544    Objective Loss 0.063544                                        LR 0.001000    Time 0.086515    
Epoch: [69][  120/  207]    Overall Loss 0.064659    Objective Loss 0.064659                                        LR 0.001000    Time 0.086546    
Epoch: [69][  130/  207]    Overall Loss 0.064524    Objective Loss 0.064524                                        LR 0.001000    Time 0.086605    
Epoch: [69][  140/  207]    Overall Loss 0.064756    Objective Loss 0.064756                                        LR 0.001000    Time 0.086466    
Epoch: [69][  150/  207]    Overall Loss 0.066084    Objective Loss 0.066084                                        LR 0.001000    Time 0.086468    
Epoch: [69][  160/  207]    Overall Loss 0.067165    Objective Loss 0.067165                                        LR 0.001000    Time 0.086546    
Epoch: [69][  170/  207]    Overall Loss 0.067889    Objective Loss 0.067889                                        LR 0.001000    Time 0.086567    
Epoch: [69][  180/  207]    Overall Loss 0.067797    Objective Loss 0.067797                                        LR 0.001000    Time 0.086605    
Epoch: [69][  190/  207]    Overall Loss 0.067506    Objective Loss 0.067506                                        LR 0.001000    Time 0.086627    
Epoch: [69][  200/  207]    Overall Loss 0.067293    Objective Loss 0.067293                                        LR 0.001000    Time 0.086290    
Epoch: [69][  207/  207]    Overall Loss 0.067220    Objective Loss 0.067220    Top1 96.262741    Top5 99.886750    LR 0.001000    Time 0.086071    
--- validate (epoch=69)-----------
5136 samples (512 per mini-batch)
Epoch: [69][   10/   11]    Loss 0.429883    Top1 83.398438    Top5 99.824219    
Epoch: [69][   11/   11]    Loss 0.414891    Top1 83.391745    Top5 99.824766    
==> Top1: 83.392    Top5: 99.825    Loss: 0.415

==> Confusion:
[[277   7   2   2   0   0   2  10]
 [  2 272  21   0   0   2   0   3]
 [  4  20 266   1   0   6   0   3]
 [  0   4   1 764  45   8   7   8]
 [  2   0   0  59 773   5  23  17]
 [  8   6  12  15  20 774  26  33]
 [  1   0   0   5  29  12 773  17]
 [ 38  24  17  72  75 133  46 384]]

==> Best [Top1: 83.411   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [70][   10/  207]    Overall Loss 0.065854    Objective Loss 0.065854                                        LR 0.001000    Time 0.153660    
Epoch: [70][   20/  207]    Overall Loss 0.059739    Objective Loss 0.059739                                        LR 0.001000    Time 0.116717    
Epoch: [70][   30/  207]    Overall Loss 0.058747    Objective Loss 0.058747                                        LR 0.001000    Time 0.101563    
Epoch: [70][   40/  207]    Overall Loss 0.058559    Objective Loss 0.058559                                        LR 0.001000    Time 0.094218    
Epoch: [70][   50/  207]    Overall Loss 0.057482    Objective Loss 0.057482                                        LR 0.001000    Time 0.089614    
Epoch: [70][   60/  207]    Overall Loss 0.057448    Objective Loss 0.057448                                        LR 0.001000    Time 0.086999    
Epoch: [70][   70/  207]    Overall Loss 0.057428    Objective Loss 0.057428                                        LR 0.001000    Time 0.085457    
Epoch: [70][   80/  207]    Overall Loss 0.055639    Objective Loss 0.055639                                        LR 0.001000    Time 0.084405    
Epoch: [70][   90/  207]    Overall Loss 0.054253    Objective Loss 0.054253                                        LR 0.001000    Time 0.083071    
Epoch: [70][  100/  207]    Overall Loss 0.053931    Objective Loss 0.053931                                        LR 0.001000    Time 0.082575    
Epoch: [70][  110/  207]    Overall Loss 0.053859    Objective Loss 0.053859                                        LR 0.001000    Time 0.082573    
Epoch: [70][  120/  207]    Overall Loss 0.053816    Objective Loss 0.053816                                        LR 0.001000    Time 0.082304    
Epoch: [70][  130/  207]    Overall Loss 0.053519    Objective Loss 0.053519                                        LR 0.001000    Time 0.082047    
Epoch: [70][  140/  207]    Overall Loss 0.053606    Objective Loss 0.053606                                        LR 0.001000    Time 0.081874    
Epoch: [70][  150/  207]    Overall Loss 0.053942    Objective Loss 0.053942                                        LR 0.001000    Time 0.081882    
Epoch: [70][  160/  207]    Overall Loss 0.053870    Objective Loss 0.053870                                        LR 0.001000    Time 0.081923    
Epoch: [70][  170/  207]    Overall Loss 0.053692    Objective Loss 0.053692                                        LR 0.001000    Time 0.081793    
Epoch: [70][  180/  207]    Overall Loss 0.053741    Objective Loss 0.053741                                        LR 0.001000    Time 0.081826    
Epoch: [70][  190/  207]    Overall Loss 0.053568    Objective Loss 0.053568                                        LR 0.001000    Time 0.081941    
Epoch: [70][  200/  207]    Overall Loss 0.053635    Objective Loss 0.053635                                        LR 0.001000    Time 0.081935    
Epoch: [70][  207/  207]    Overall Loss 0.053319    Objective Loss 0.053319    Top1 97.281993    Top5 99.773499    LR 0.001000    Time 0.081455    
--- validate (epoch=70)-----------
5136 samples (512 per mini-batch)
Epoch: [70][   10/   11]    Loss 0.497737    Top1 84.296875    Top5 99.746094    
Epoch: [70][   11/   11]    Loss 0.477685    Top1 84.287383    Top5 99.746885    
==> Top1: 84.287    Top5: 99.747    Loss: 0.478

==> Confusion:
[[271   7   2   1   0   0   1  18]
 [  1 268  24   1   0   1   0   5]
 [  2  20 270   1   0   2   0   5]
 [  0   5   1 748  43  13   9  18]
 [  1   0   0  40 783   4  20  31]
 [  7   9  10   8  23 774  11  52]
 [  1   0   0   4  29  15 760  28]
 [ 27  26  16  49  74 116  26 455]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [71][   10/  207]    Overall Loss 0.036474    Objective Loss 0.036474                                        LR 0.001000    Time 0.127905    
Epoch: [71][   20/  207]    Overall Loss 0.038576    Objective Loss 0.038576                                        LR 0.001000    Time 0.099382    
Epoch: [71][   30/  207]    Overall Loss 0.038684    Objective Loss 0.038684                                        LR 0.001000    Time 0.089895    
Epoch: [71][   40/  207]    Overall Loss 0.039889    Objective Loss 0.039889                                        LR 0.001000    Time 0.085148    
Epoch: [71][   50/  207]    Overall Loss 0.041548    Objective Loss 0.041548                                        LR 0.001000    Time 0.082449    
Epoch: [71][   60/  207]    Overall Loss 0.041637    Objective Loss 0.041637                                        LR 0.001000    Time 0.080582    
Epoch: [71][   70/  207]    Overall Loss 0.042125    Objective Loss 0.042125                                        LR 0.001000    Time 0.079175    
Epoch: [71][   80/  207]    Overall Loss 0.041949    Objective Loss 0.041949                                        LR 0.001000    Time 0.078297    
Epoch: [71][   90/  207]    Overall Loss 0.041608    Objective Loss 0.041608                                        LR 0.001000    Time 0.078129    
Epoch: [71][  100/  207]    Overall Loss 0.041597    Objective Loss 0.041597                                        LR 0.001000    Time 0.077685    
Epoch: [71][  110/  207]    Overall Loss 0.041915    Objective Loss 0.041915                                        LR 0.001000    Time 0.077237    
Epoch: [71][  120/  207]    Overall Loss 0.042292    Objective Loss 0.042292                                        LR 0.001000    Time 0.076843    
Epoch: [71][  130/  207]    Overall Loss 0.042547    Objective Loss 0.042547                                        LR 0.001000    Time 0.076527    
Epoch: [71][  140/  207]    Overall Loss 0.042544    Objective Loss 0.042544                                        LR 0.001000    Time 0.076266    
Epoch: [71][  150/  207]    Overall Loss 0.042512    Objective Loss 0.042512                                        LR 0.001000    Time 0.076043    
Epoch: [71][  160/  207]    Overall Loss 0.042939    Objective Loss 0.042939                                        LR 0.001000    Time 0.075761    
Epoch: [71][  170/  207]    Overall Loss 0.043271    Objective Loss 0.043271                                        LR 0.001000    Time 0.075592    
Epoch: [71][  180/  207]    Overall Loss 0.043694    Objective Loss 0.043694                                        LR 0.001000    Time 0.075307    
Epoch: [71][  190/  207]    Overall Loss 0.044846    Objective Loss 0.044846                                        LR 0.001000    Time 0.075144    
Epoch: [71][  200/  207]    Overall Loss 0.045862    Objective Loss 0.045862                                        LR 0.001000    Time 0.074920    
Epoch: [71][  207/  207]    Overall Loss 0.047155    Objective Loss 0.047155    Top1 93.544734    Top5 100.000000    LR 0.001000    Time 0.074666    
--- validate (epoch=71)-----------
5136 samples (512 per mini-batch)
Epoch: [71][   10/   11]    Loss 0.589197    Top1 83.261719    Top5 99.628906    
Epoch: [71][   11/   11]    Loss 0.635647    Top1 83.274922    Top5 99.630062    
==> Top1: 83.275    Top5: 99.630    Loss: 0.636

==> Confusion:
[[260   4   3   2   1   9   4  17]
 [  2 242  37   2   0   2   2  13]
 [  0  14 263   2   0  12   0   9]
 [  0   0   0 753  49  13   7  15]
 [  0   0   0  47 783   6  20  23]
 [  4   3   5  11  14 829  11  17]
 [  2   0   0   7  40  15 758  15]
 [ 16   8  13  69  67 192  35 389]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [72][   10/  207]    Overall Loss 0.104123    Objective Loss 0.104123                                        LR 0.001000    Time 0.127965    
Epoch: [72][   20/  207]    Overall Loss 0.110577    Objective Loss 0.110577                                        LR 0.001000    Time 0.099643    
Epoch: [72][   30/  207]    Overall Loss 0.126326    Objective Loss 0.126326                                        LR 0.001000    Time 0.089894    
Epoch: [72][   40/  207]    Overall Loss 0.142739    Objective Loss 0.142739                                        LR 0.001000    Time 0.085116    
Epoch: [72][   50/  207]    Overall Loss 0.146594    Objective Loss 0.146594                                        LR 0.001000    Time 0.082345    
Epoch: [72][   60/  207]    Overall Loss 0.154224    Objective Loss 0.154224                                        LR 0.001000    Time 0.080864    
Epoch: [72][   70/  207]    Overall Loss 0.168386    Objective Loss 0.168386                                        LR 0.001000    Time 0.079652    
Epoch: [72][   80/  207]    Overall Loss 0.168340    Objective Loss 0.168340                                        LR 0.001000    Time 0.078586    
Epoch: [72][   90/  207]    Overall Loss 0.177104    Objective Loss 0.177104                                        LR 0.001000    Time 0.077809    
Epoch: [72][  100/  207]    Overall Loss 0.184408    Objective Loss 0.184408                                        LR 0.001000    Time 0.077170    
Epoch: [72][  110/  207]    Overall Loss 0.183438    Objective Loss 0.183438                                        LR 0.001000    Time 0.076787    
Epoch: [72][  120/  207]    Overall Loss 0.182391    Objective Loss 0.182391                                        LR 0.001000    Time 0.076457    
Epoch: [72][  130/  207]    Overall Loss 0.182576    Objective Loss 0.182576                                        LR 0.001000    Time 0.076099    
Epoch: [72][  140/  207]    Overall Loss 0.180949    Objective Loss 0.180949                                        LR 0.001000    Time 0.076277    
Epoch: [72][  150/  207]    Overall Loss 0.179114    Objective Loss 0.179114                                        LR 0.001000    Time 0.076125    
Epoch: [72][  160/  207]    Overall Loss 0.177106    Objective Loss 0.177106                                        LR 0.001000    Time 0.075859    
Epoch: [72][  170/  207]    Overall Loss 0.173320    Objective Loss 0.173320                                        LR 0.001000    Time 0.075693    
Epoch: [72][  180/  207]    Overall Loss 0.170656    Objective Loss 0.170656                                        LR 0.001000    Time 0.075747    
Epoch: [72][  190/  207]    Overall Loss 0.167900    Objective Loss 0.167900                                        LR 0.001000    Time 0.075482    
Epoch: [72][  200/  207]    Overall Loss 0.164513    Objective Loss 0.164513                                        LR 0.001000    Time 0.075507    
Epoch: [72][  207/  207]    Overall Loss 0.162469    Objective Loss 0.162469    Top1 93.544734    Top5 100.000000    LR 0.001000    Time 0.075415    
--- validate (epoch=72)-----------
5136 samples (512 per mini-batch)
Epoch: [72][   10/   11]    Loss 0.515427    Top1 82.734375    Top5 99.843750    
Epoch: [72][   11/   11]    Loss 0.477645    Top1 82.729751    Top5 99.844237    
==> Top1: 82.730    Top5: 99.844    Loss: 0.478

==> Confusion:
[[256   5   9   3   2   9   0  16]
 [  2 266  29   0   0   2   0   1]
 [  1  20 274   0   0   2   0   3]
 [  0   5   1 754  49  11   7  10]
 [  0   0   0  41 796   8  12  22]
 [  0   6  12  11  26 788  25  26]
 [  1   0   0   5  47  10 746  28]
 [ 33  30  23  62  88 152  32 369]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [73][   10/  207]    Overall Loss 0.082432    Objective Loss 0.082432                                        LR 0.001000    Time 0.129497    
Epoch: [73][   20/  207]    Overall Loss 0.084868    Objective Loss 0.084868                                        LR 0.001000    Time 0.104594    
Epoch: [73][   30/  207]    Overall Loss 0.085632    Objective Loss 0.085632                                        LR 0.001000    Time 0.093665    
Epoch: [73][   40/  207]    Overall Loss 0.079864    Objective Loss 0.079864                                        LR 0.001000    Time 0.088286    
Epoch: [73][   50/  207]    Overall Loss 0.077060    Objective Loss 0.077060                                        LR 0.001000    Time 0.085112    
Epoch: [73][   60/  207]    Overall Loss 0.076245    Objective Loss 0.076245                                        LR 0.001000    Time 0.083012    
Epoch: [73][   70/  207]    Overall Loss 0.075502    Objective Loss 0.075502                                        LR 0.001000    Time 0.082421    
Epoch: [73][   80/  207]    Overall Loss 0.075220    Objective Loss 0.075220                                        LR 0.001000    Time 0.081319    
Epoch: [73][   90/  207]    Overall Loss 0.074723    Objective Loss 0.074723                                        LR 0.001000    Time 0.080663    
Epoch: [73][  100/  207]    Overall Loss 0.073205    Objective Loss 0.073205                                        LR 0.001000    Time 0.079860    
Epoch: [73][  110/  207]    Overall Loss 0.072421    Objective Loss 0.072421                                        LR 0.001000    Time 0.079204    
Epoch: [73][  120/  207]    Overall Loss 0.071696    Objective Loss 0.071696                                        LR 0.001000    Time 0.078663    
Epoch: [73][  130/  207]    Overall Loss 0.071627    Objective Loss 0.071627                                        LR 0.001000    Time 0.078083    
Epoch: [73][  140/  207]    Overall Loss 0.071866    Objective Loss 0.071866                                        LR 0.001000    Time 0.077874    
Epoch: [73][  150/  207]    Overall Loss 0.072385    Objective Loss 0.072385                                        LR 0.001000    Time 0.077764    
Epoch: [73][  160/  207]    Overall Loss 0.073797    Objective Loss 0.073797                                        LR 0.001000    Time 0.077568    
Epoch: [73][  170/  207]    Overall Loss 0.074453    Objective Loss 0.074453                                        LR 0.001000    Time 0.077237    
Epoch: [73][  180/  207]    Overall Loss 0.074584    Objective Loss 0.074584                                        LR 0.001000    Time 0.077054    
Epoch: [73][  190/  207]    Overall Loss 0.075061    Objective Loss 0.075061                                        LR 0.001000    Time 0.076759    
Epoch: [73][  200/  207]    Overall Loss 0.075691    Objective Loss 0.075691                                        LR 0.001000    Time 0.076416    
Epoch: [73][  207/  207]    Overall Loss 0.076458    Objective Loss 0.076458    Top1 94.903737    Top5 100.000000    LR 0.001000    Time 0.076114    
--- validate (epoch=73)-----------
5136 samples (512 per mini-batch)
Epoch: [73][   10/   11]    Loss 0.589666    Top1 83.222656    Top5 99.667969    
Epoch: [73][   11/   11]    Loss 0.582814    Top1 83.216511    Top5 99.669003    
==> Top1: 83.217    Top5: 99.669    Loss: 0.583

==> Confusion:
[[267   2   1   0   2   1   2  25]
 [  3 255  26   1   0   0   1  14]
 [  3  27 248   1   1   3   0  17]
 [  1   3   0 736  61   5  13  18]
 [  2   0   0  42 781   3  25  26]
 [  6   5   5  14  28 738  23  75]
 [  1   0   0   4  30   4 767  31]
 [ 28  12  13  45  77  95  37 482]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [74][   10/  207]    Overall Loss 0.090199    Objective Loss 0.090199                                        LR 0.001000    Time 0.133136    
Epoch: [74][   20/  207]    Overall Loss 0.084413    Objective Loss 0.084413                                        LR 0.001000    Time 0.102356    
Epoch: [74][   30/  207]    Overall Loss 0.083615    Objective Loss 0.083615                                        LR 0.001000    Time 0.092087    
Epoch: [74][   40/  207]    Overall Loss 0.087438    Objective Loss 0.087438                                        LR 0.001000    Time 0.086862    
Epoch: [74][   50/  207]    Overall Loss 0.088801    Objective Loss 0.088801                                        LR 0.001000    Time 0.084035    
Epoch: [74][   60/  207]    Overall Loss 0.090528    Objective Loss 0.090528                                        LR 0.001000    Time 0.082450    
Epoch: [74][   70/  207]    Overall Loss 0.089489    Objective Loss 0.089489                                        LR 0.001000    Time 0.081116    
Epoch: [74][   80/  207]    Overall Loss 0.089649    Objective Loss 0.089649                                        LR 0.001000    Time 0.079740    
Epoch: [74][   90/  207]    Overall Loss 0.088831    Objective Loss 0.088831                                        LR 0.001000    Time 0.078631    
Epoch: [74][  100/  207]    Overall Loss 0.087920    Objective Loss 0.087920                                        LR 0.001000    Time 0.077805    
Epoch: [74][  110/  207]    Overall Loss 0.086678    Objective Loss 0.086678                                        LR 0.001000    Time 0.077217    
Epoch: [74][  120/  207]    Overall Loss 0.086235    Objective Loss 0.086235                                        LR 0.001000    Time 0.076859    
Epoch: [74][  130/  207]    Overall Loss 0.085318    Objective Loss 0.085318                                        LR 0.001000    Time 0.076419    
Epoch: [74][  140/  207]    Overall Loss 0.083779    Objective Loss 0.083779                                        LR 0.001000    Time 0.076116    
Epoch: [74][  150/  207]    Overall Loss 0.082633    Objective Loss 0.082633                                        LR 0.001000    Time 0.075964    
Epoch: [74][  160/  207]    Overall Loss 0.081826    Objective Loss 0.081826                                        LR 0.001000    Time 0.075824    
Epoch: [74][  170/  207]    Overall Loss 0.081163    Objective Loss 0.081163                                        LR 0.001000    Time 0.075549    
Epoch: [74][  180/  207]    Overall Loss 0.081199    Objective Loss 0.081199                                        LR 0.001000    Time 0.075460    
Epoch: [74][  190/  207]    Overall Loss 0.082628    Objective Loss 0.082628                                        LR 0.001000    Time 0.075308    
Epoch: [74][  200/  207]    Overall Loss 0.084673    Objective Loss 0.084673                                        LR 0.001000    Time 0.075068    
Epoch: [74][  207/  207]    Overall Loss 0.085594    Objective Loss 0.085594    Top1 93.544734    Top5 99.886750    LR 0.001000    Time 0.074809    
--- validate (epoch=74)-----------
5136 samples (512 per mini-batch)
Epoch: [74][   10/   11]    Loss 0.512138    Top1 81.855469    Top5 99.726562    
Epoch: [74][   11/   11]    Loss 0.468967    Top1 81.892523    Top5 99.727414    
==> Top1: 81.893    Top5: 99.727    Loss: 0.469

==> Confusion:
[[268   4   5   0   3   2   3  15]
 [  2 268  24   0   1   1   1   3]
 [  3  31 262   1   0   1   0   2]
 [  1   3   2 700  91  13  14  13]
 [  1   0   0  17 796   3  45  17]
 [  4   5  12  14  30 780  25  24]
 [  1   0   0   3  31  11 780  11]
 [ 27  23  20  49  96 159  63 352]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [75][   10/  207]    Overall Loss 0.104214    Objective Loss 0.104214                                        LR 0.001000    Time 0.129101    
Epoch: [75][   20/  207]    Overall Loss 0.107289    Objective Loss 0.107289                                        LR 0.001000    Time 0.100122    
Epoch: [75][   30/  207]    Overall Loss 0.107046    Objective Loss 0.107046                                        LR 0.001000    Time 0.090864    
Epoch: [75][   40/  207]    Overall Loss 0.099630    Objective Loss 0.099630                                        LR 0.001000    Time 0.086322    
Epoch: [75][   50/  207]    Overall Loss 0.093229    Objective Loss 0.093229                                        LR 0.001000    Time 0.083094    
Epoch: [75][   60/  207]    Overall Loss 0.088875    Objective Loss 0.088875                                        LR 0.001000    Time 0.081127    
Epoch: [75][   70/  207]    Overall Loss 0.086238    Objective Loss 0.086238                                        LR 0.001000    Time 0.079627    
Epoch: [75][   80/  207]    Overall Loss 0.083444    Objective Loss 0.083444                                        LR 0.001000    Time 0.078994    
Epoch: [75][   90/  207]    Overall Loss 0.082500    Objective Loss 0.082500                                        LR 0.001000    Time 0.078475    
Epoch: [75][  100/  207]    Overall Loss 0.082835    Objective Loss 0.082835                                        LR 0.001000    Time 0.077958    
Epoch: [75][  110/  207]    Overall Loss 0.082735    Objective Loss 0.082735                                        LR 0.001000    Time 0.077387    
Epoch: [75][  120/  207]    Overall Loss 0.084143    Objective Loss 0.084143                                        LR 0.001000    Time 0.077028    
Epoch: [75][  130/  207]    Overall Loss 0.084235    Objective Loss 0.084235                                        LR 0.001000    Time 0.076568    
Epoch: [75][  140/  207]    Overall Loss 0.084158    Objective Loss 0.084158                                        LR 0.001000    Time 0.076328    
Epoch: [75][  150/  207]    Overall Loss 0.082891    Objective Loss 0.082891                                        LR 0.001000    Time 0.076357    
Epoch: [75][  160/  207]    Overall Loss 0.081877    Objective Loss 0.081877                                        LR 0.001000    Time 0.076259    
Epoch: [75][  170/  207]    Overall Loss 0.080680    Objective Loss 0.080680                                        LR 0.001000    Time 0.076153    
Epoch: [75][  180/  207]    Overall Loss 0.080420    Objective Loss 0.080420                                        LR 0.001000    Time 0.076071    
Epoch: [75][  190/  207]    Overall Loss 0.080042    Objective Loss 0.080042                                        LR 0.001000    Time 0.075788    
Epoch: [75][  200/  207]    Overall Loss 0.079486    Objective Loss 0.079486                                        LR 0.001000    Time 0.075493    
Epoch: [75][  207/  207]    Overall Loss 0.079201    Objective Loss 0.079201    Top1 95.696489    Top5 99.886750    LR 0.001000    Time 0.075216    
--- validate (epoch=75)-----------
5136 samples (512 per mini-batch)
Epoch: [75][   10/   11]    Loss 0.484835    Top1 82.636719    Top5 99.765625    
Epoch: [75][   11/   11]    Loss 0.470082    Top1 82.651869    Top5 99.766355    
==> Top1: 82.652    Top5: 99.766    Loss: 0.470

==> Confusion:
[[278   4   4   1   0   2   1  10]
 [  1 263  29   1   0   3   0   3]
 [  3  20 271   1   0   2   0   3]
 [  1   6   1 709  81  12  12  15]
 [  2   0   0  28 793   7  31  18]
 [ 10   6  13   7  25 797  20  16]
 [  2   0   0   2  34  10 776  13]
 [ 40  19  18  41  98 172  43 358]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [76][   10/  207]    Overall Loss 0.047638    Objective Loss 0.047638                                        LR 0.001000    Time 0.130751    
Epoch: [76][   20/  207]    Overall Loss 0.051708    Objective Loss 0.051708                                        LR 0.001000    Time 0.104484    
Epoch: [76][   30/  207]    Overall Loss 0.052833    Objective Loss 0.052833                                        LR 0.001000    Time 0.093377    
Epoch: [76][   40/  207]    Overall Loss 0.051748    Objective Loss 0.051748                                        LR 0.001000    Time 0.088076    
Epoch: [76][   50/  207]    Overall Loss 0.052859    Objective Loss 0.052859                                        LR 0.001000    Time 0.084646    
Epoch: [76][   60/  207]    Overall Loss 0.054428    Objective Loss 0.054428                                        LR 0.001000    Time 0.082222    
Epoch: [76][   70/  207]    Overall Loss 0.054520    Objective Loss 0.054520                                        LR 0.001000    Time 0.080821    
Epoch: [76][   80/  207]    Overall Loss 0.054772    Objective Loss 0.054772                                        LR 0.001000    Time 0.079854    
Epoch: [76][   90/  207]    Overall Loss 0.056455    Objective Loss 0.056455                                        LR 0.001000    Time 0.079249    
Epoch: [76][  100/  207]    Overall Loss 0.057472    Objective Loss 0.057472                                        LR 0.001000    Time 0.078791    
Epoch: [76][  110/  207]    Overall Loss 0.058073    Objective Loss 0.058073                                        LR 0.001000    Time 0.078402    
Epoch: [76][  120/  207]    Overall Loss 0.060201    Objective Loss 0.060201                                        LR 0.001000    Time 0.078104    
Epoch: [76][  130/  207]    Overall Loss 0.060487    Objective Loss 0.060487                                        LR 0.001000    Time 0.077707    
Epoch: [76][  140/  207]    Overall Loss 0.061085    Objective Loss 0.061085                                        LR 0.001000    Time 0.077754    
Epoch: [76][  150/  207]    Overall Loss 0.061828    Objective Loss 0.061828                                        LR 0.001000    Time 0.077822    
Epoch: [76][  160/  207]    Overall Loss 0.062289    Objective Loss 0.062289                                        LR 0.001000    Time 0.077635    
Epoch: [76][  170/  207]    Overall Loss 0.063258    Objective Loss 0.063258                                        LR 0.001000    Time 0.077381    
Epoch: [76][  180/  207]    Overall Loss 0.064470    Objective Loss 0.064470                                        LR 0.001000    Time 0.077119    
Epoch: [76][  190/  207]    Overall Loss 0.064328    Objective Loss 0.064328                                        LR 0.001000    Time 0.076905    
Epoch: [76][  200/  207]    Overall Loss 0.063759    Objective Loss 0.063759                                        LR 0.001000    Time 0.076617    
Epoch: [76][  207/  207]    Overall Loss 0.063555    Objective Loss 0.063555    Top1 95.469989    Top5 100.000000    LR 0.001000    Time 0.076311    
--- validate (epoch=76)-----------
5136 samples (512 per mini-batch)
Epoch: [76][   10/   11]    Loss 0.535580    Top1 83.437500    Top5 99.609375    
Epoch: [76][   11/   11]    Loss 0.493804    Top1 83.469626    Top5 99.610592    
==> Top1: 83.470    Top5: 99.611    Loss: 0.494

==> Confusion:
[[270   3   5   1   0   2   4  15]
 [  1 244  44   3   0   4   0   4]
 [  1  12 276   1   0   2   0   8]
 [  0   2   1 750  55   9   7  13]
 [  1   0   0  43 770   5  37  23]
 [  5   8  13  14  15 772  28  39]
 [  1   0   0   5  21   7 790  13]
 [ 27  16  21  58  72 127  53 415]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [77][   10/  207]    Overall Loss 0.043413    Objective Loss 0.043413                                        LR 0.001000    Time 0.134086    
Epoch: [77][   20/  207]    Overall Loss 0.045624    Objective Loss 0.045624                                        LR 0.001000    Time 0.105894    
Epoch: [77][   30/  207]    Overall Loss 0.046639    Objective Loss 0.046639                                        LR 0.001000    Time 0.096203    
Epoch: [77][   40/  207]    Overall Loss 0.045855    Objective Loss 0.045855                                        LR 0.001000    Time 0.089877    
Epoch: [77][   50/  207]    Overall Loss 0.045979    Objective Loss 0.045979                                        LR 0.001000    Time 0.087188    
Epoch: [77][   60/  207]    Overall Loss 0.046084    Objective Loss 0.046084                                        LR 0.001000    Time 0.085396    
Epoch: [77][   70/  207]    Overall Loss 0.045050    Objective Loss 0.045050                                        LR 0.001000    Time 0.083645    
Epoch: [77][   80/  207]    Overall Loss 0.044838    Objective Loss 0.044838                                        LR 0.001000    Time 0.081975    
Epoch: [77][   90/  207]    Overall Loss 0.044785    Objective Loss 0.044785                                        LR 0.001000    Time 0.080957    
Epoch: [77][  100/  207]    Overall Loss 0.045265    Objective Loss 0.045265                                        LR 0.001000    Time 0.080233    
Epoch: [77][  110/  207]    Overall Loss 0.045708    Objective Loss 0.045708                                        LR 0.001000    Time 0.079418    
Epoch: [77][  120/  207]    Overall Loss 0.045808    Objective Loss 0.045808                                        LR 0.001000    Time 0.078855    
Epoch: [77][  130/  207]    Overall Loss 0.046076    Objective Loss 0.046076                                        LR 0.001000    Time 0.078295    
Epoch: [77][  140/  207]    Overall Loss 0.046061    Objective Loss 0.046061                                        LR 0.001000    Time 0.077970    
Epoch: [77][  150/  207]    Overall Loss 0.045999    Objective Loss 0.045999                                        LR 0.001000    Time 0.077623    
Epoch: [77][  160/  207]    Overall Loss 0.045946    Objective Loss 0.045946                                        LR 0.001000    Time 0.077372    
Epoch: [77][  170/  207]    Overall Loss 0.046136    Objective Loss 0.046136                                        LR 0.001000    Time 0.077103    
Epoch: [77][  180/  207]    Overall Loss 0.046344    Objective Loss 0.046344                                        LR 0.001000    Time 0.076785    
Epoch: [77][  190/  207]    Overall Loss 0.046324    Objective Loss 0.046324                                        LR 0.001000    Time 0.076685    
Epoch: [77][  200/  207]    Overall Loss 0.046761    Objective Loss 0.046761                                        LR 0.001000    Time 0.076404    
Epoch: [77][  207/  207]    Overall Loss 0.047345    Objective Loss 0.047345    Top1 96.489241    Top5 99.773499    LR 0.001000    Time 0.076130    
--- validate (epoch=77)-----------
5136 samples (512 per mini-batch)
Epoch: [77][   10/   11]    Loss 0.582356    Top1 83.164062    Top5 99.707031    
Epoch: [77][   11/   11]    Loss 0.654451    Top1 83.177570    Top5 99.707944    
==> Top1: 83.178    Top5: 99.708    Loss: 0.654

==> Confusion:
[[250   2   2   2   3   5   6  30]
 [  2 240  40   3   0   2   2  11]
 [  1  13 269   4   0   7   0   6]
 [  0   0   0 748  53  12  12  12]
 [  0   0   0  54 767   3  34  21]
 [  0   3   8  13  20 781  28  41]
 [  1   0   0   6  24   4 787  15]
 [ 15   9  13  68  65 125  64 430]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [78][   10/  207]    Overall Loss 0.093757    Objective Loss 0.093757                                        LR 0.001000    Time 0.134347    
Epoch: [78][   20/  207]    Overall Loss 0.090329    Objective Loss 0.090329                                        LR 0.001000    Time 0.102578    
Epoch: [78][   30/  207]    Overall Loss 0.087853    Objective Loss 0.087853                                        LR 0.001000    Time 0.092331    
Epoch: [78][   40/  207]    Overall Loss 0.088871    Objective Loss 0.088871                                        LR 0.001000    Time 0.086885    
Epoch: [78][   50/  207]    Overall Loss 0.091322    Objective Loss 0.091322                                        LR 0.001000    Time 0.083780    
Epoch: [78][   60/  207]    Overall Loss 0.093180    Objective Loss 0.093180                                        LR 0.001000    Time 0.082419    
Epoch: [78][   70/  207]    Overall Loss 0.099284    Objective Loss 0.099284                                        LR 0.001000    Time 0.080933    
Epoch: [78][   80/  207]    Overall Loss 0.101610    Objective Loss 0.101610                                        LR 0.001000    Time 0.080132    
Epoch: [78][   90/  207]    Overall Loss 0.100928    Objective Loss 0.100928                                        LR 0.001000    Time 0.079770    
Epoch: [78][  100/  207]    Overall Loss 0.100436    Objective Loss 0.100436                                        LR 0.001000    Time 0.078937    
Epoch: [78][  110/  207]    Overall Loss 0.100670    Objective Loss 0.100670                                        LR 0.001000    Time 0.078166    
Epoch: [78][  120/  207]    Overall Loss 0.102124    Objective Loss 0.102124                                        LR 0.001000    Time 0.078197    
Epoch: [78][  130/  207]    Overall Loss 0.103976    Objective Loss 0.103976                                        LR 0.001000    Time 0.077917    
Epoch: [78][  140/  207]    Overall Loss 0.105639    Objective Loss 0.105639                                        LR 0.001000    Time 0.077831    
Epoch: [78][  150/  207]    Overall Loss 0.108841    Objective Loss 0.108841                                        LR 0.001000    Time 0.077643    
Epoch: [78][  160/  207]    Overall Loss 0.109021    Objective Loss 0.109021                                        LR 0.001000    Time 0.077504    
Epoch: [78][  170/  207]    Overall Loss 0.109706    Objective Loss 0.109706                                        LR 0.001000    Time 0.077760    
Epoch: [78][  180/  207]    Overall Loss 0.109972    Objective Loss 0.109972                                        LR 0.001000    Time 0.077396    
Epoch: [78][  190/  207]    Overall Loss 0.109518    Objective Loss 0.109518                                        LR 0.001000    Time 0.077139    
Epoch: [78][  200/  207]    Overall Loss 0.110440    Objective Loss 0.110440                                        LR 0.001000    Time 0.076838    
Epoch: [78][  207/  207]    Overall Loss 0.112059    Objective Loss 0.112059    Top1 91.732729    Top5 100.000000    LR 0.001000    Time 0.076508    
--- validate (epoch=78)-----------
5136 samples (512 per mini-batch)
Epoch: [78][   10/   11]    Loss 0.675745    Top1 78.496094    Top5 99.453125    
Epoch: [78][   11/   11]    Loss 0.678113    Top1 78.504673    Top5 99.454829    
==> Top1: 78.505    Top5: 99.455    Loss: 0.678

==> Confusion:
[[251  15  15   4   1   3   0  11]
 [  1 247  50   1   0   0   1   0]
 [  0  16 280   1   0   2   0   1]
 [  0   5   0 800  18   6   4   4]
 [  1   4   0 161 670   7  19  17]
 [  6  19  17  43  18 748  28  15]
 [  2   0   0  16  40  11 751  17]
 [ 34  54  40 125  67 144  40 285]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [79][   10/  207]    Overall Loss 0.131604    Objective Loss 0.131604                                        LR 0.001000    Time 0.111322    
Epoch: [79][   20/  207]    Overall Loss 0.143524    Objective Loss 0.143524                                        LR 0.001000    Time 0.091499    
Epoch: [79][   30/  207]    Overall Loss 0.153759    Objective Loss 0.153759                                        LR 0.001000    Time 0.085046    
Epoch: [79][   40/  207]    Overall Loss 0.157233    Objective Loss 0.157233                                        LR 0.001000    Time 0.084583    
Epoch: [79][   50/  207]    Overall Loss 0.152459    Objective Loss 0.152459                                        LR 0.001000    Time 0.083223    
Epoch: [79][   60/  207]    Overall Loss 0.148642    Objective Loss 0.148642                                        LR 0.001000    Time 0.081080    
Epoch: [79][   70/  207]    Overall Loss 0.142242    Objective Loss 0.142242                                        LR 0.001000    Time 0.079617    
Epoch: [79][   80/  207]    Overall Loss 0.139933    Objective Loss 0.139933                                        LR 0.001000    Time 0.078714    
Epoch: [79][   90/  207]    Overall Loss 0.133779    Objective Loss 0.133779                                        LR 0.001000    Time 0.077932    
Epoch: [79][  100/  207]    Overall Loss 0.128723    Objective Loss 0.128723                                        LR 0.001000    Time 0.077369    
Epoch: [79][  110/  207]    Overall Loss 0.126758    Objective Loss 0.126758                                        LR 0.001000    Time 0.076760    
Epoch: [79][  120/  207]    Overall Loss 0.124734    Objective Loss 0.124734                                        LR 0.001000    Time 0.076350    
Epoch: [79][  130/  207]    Overall Loss 0.121235    Objective Loss 0.121235                                        LR 0.001000    Time 0.076084    
Epoch: [79][  140/  207]    Overall Loss 0.117372    Objective Loss 0.117372                                        LR 0.001000    Time 0.075818    
Epoch: [79][  150/  207]    Overall Loss 0.114972    Objective Loss 0.114972                                        LR 0.001000    Time 0.075621    
Epoch: [79][  160/  207]    Overall Loss 0.113832    Objective Loss 0.113832                                        LR 0.001000    Time 0.075445    
Epoch: [79][  170/  207]    Overall Loss 0.112548    Objective Loss 0.112548                                        LR 0.001000    Time 0.075150    
Epoch: [79][  180/  207]    Overall Loss 0.111002    Objective Loss 0.111002                                        LR 0.001000    Time 0.075005    
Epoch: [79][  190/  207]    Overall Loss 0.109870    Objective Loss 0.109870                                        LR 0.001000    Time 0.074860    
Epoch: [79][  200/  207]    Overall Loss 0.109138    Objective Loss 0.109138                                        LR 0.001000    Time 0.074684    
Epoch: [79][  207/  207]    Overall Loss 0.108183    Objective Loss 0.108183    Top1 95.243488    Top5 100.000000    LR 0.001000    Time 0.074438    
--- validate (epoch=79)-----------
5136 samples (512 per mini-batch)
Epoch: [79][   10/   11]    Loss 0.577757    Top1 83.769531    Top5 99.707031    
Epoch: [79][   11/   11]    Loss 0.586436    Top1 83.742212    Top5 99.688474    
==> Top1: 83.742    Top5: 99.688    Loss: 0.586

==> Confusion:
[[274   2   3   2   0   1   2  16]
 [  3 266  20   1   0   1   0   9]
 [  5  30 253   1   0   1   0  10]
 [  0   5   0 732  52  13   9  26]
 [  1   0   0  39 771  13  27  28]
 [ 14   6  11  11  14 778  13  47]
 [  2   0   0   3  29  18 760  25]
 [ 26  15  13  46  67 118  37 467]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [80][   10/  207]    Overall Loss 0.071556    Objective Loss 0.071556                                        LR 0.001000    Time 0.131471    
Epoch: [80][   20/  207]    Overall Loss 0.075000    Objective Loss 0.075000                                        LR 0.001000    Time 0.101644    
Epoch: [80][   30/  207]    Overall Loss 0.071625    Objective Loss 0.071625                                        LR 0.001000    Time 0.091789    
Epoch: [80][   40/  207]    Overall Loss 0.070311    Objective Loss 0.070311                                        LR 0.001000    Time 0.086966    
Epoch: [80][   50/  207]    Overall Loss 0.068256    Objective Loss 0.068256                                        LR 0.001000    Time 0.083945    
Epoch: [80][   60/  207]    Overall Loss 0.066977    Objective Loss 0.066977                                        LR 0.001000    Time 0.081687    
Epoch: [80][   70/  207]    Overall Loss 0.066138    Objective Loss 0.066138                                        LR 0.001000    Time 0.080322    
Epoch: [80][   80/  207]    Overall Loss 0.067127    Objective Loss 0.067127                                        LR 0.001000    Time 0.079391    
Epoch: [80][   90/  207]    Overall Loss 0.066510    Objective Loss 0.066510                                        LR 0.001000    Time 0.078645    
Epoch: [80][  100/  207]    Overall Loss 0.065283    Objective Loss 0.065283                                        LR 0.001000    Time 0.078318    
Epoch: [80][  110/  207]    Overall Loss 0.064939    Objective Loss 0.064939                                        LR 0.001000    Time 0.078369    
Epoch: [80][  120/  207]    Overall Loss 0.064185    Objective Loss 0.064185                                        LR 0.001000    Time 0.078627    
Epoch: [80][  130/  207]    Overall Loss 0.063303    Objective Loss 0.063303                                        LR 0.001000    Time 0.078371    
Epoch: [80][  140/  207]    Overall Loss 0.062973    Objective Loss 0.062973                                        LR 0.001000    Time 0.078181    
Epoch: [80][  150/  207]    Overall Loss 0.062945    Objective Loss 0.062945                                        LR 0.001000    Time 0.077856    
Epoch: [80][  160/  207]    Overall Loss 0.062600    Objective Loss 0.062600                                        LR 0.001000    Time 0.077766    
Epoch: [80][  170/  207]    Overall Loss 0.063098    Objective Loss 0.063098                                        LR 0.001000    Time 0.077436    
Epoch: [80][  180/  207]    Overall Loss 0.063704    Objective Loss 0.063704                                        LR 0.001000    Time 0.077187    
Epoch: [80][  190/  207]    Overall Loss 0.063706    Objective Loss 0.063706                                        LR 0.001000    Time 0.076933    
Epoch: [80][  200/  207]    Overall Loss 0.063549    Objective Loss 0.063549                                        LR 0.001000    Time 0.077038    
Epoch: [80][  207/  207]    Overall Loss 0.063486    Objective Loss 0.063486    Top1 96.828992    Top5 100.000000    LR 0.001000    Time 0.076742    
--- validate (epoch=80)-----------
5136 samples (512 per mini-batch)
Epoch: [80][   10/   11]    Loss 0.563991    Top1 82.460938    Top5 99.726562    
Epoch: [80][   11/   11]    Loss 0.604961    Top1 82.476636    Top5 99.727414    
==> Top1: 82.477    Top5: 99.727    Loss: 0.605

==> Confusion:
[[250   2   4   4   3  10   2  25]
 [  3 257  25   3   0   7   0   5]
 [  1  15 271   2   0   7   0   4]
 [  0   1   0 747  65   9   9   6]
 [  0   0   0  43 799   4  13  20]
 [  1   5   6  16  31 804  11  20]
 [  1   0   0   5  56  13 744  18]
 [ 14  15  10  78  88 188  32 364]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [81][   10/  207]    Overall Loss 0.052959    Objective Loss 0.052959                                        LR 0.001000    Time 0.130372    
Epoch: [81][   20/  207]    Overall Loss 0.051634    Objective Loss 0.051634                                        LR 0.001000    Time 0.101208    
Epoch: [81][   30/  207]    Overall Loss 0.052500    Objective Loss 0.052500                                        LR 0.001000    Time 0.091879    
Epoch: [81][   40/  207]    Overall Loss 0.052520    Objective Loss 0.052520                                        LR 0.001000    Time 0.088289    
Epoch: [81][   50/  207]    Overall Loss 0.052310    Objective Loss 0.052310                                        LR 0.001000    Time 0.085047    
Epoch: [81][   60/  207]    Overall Loss 0.051620    Objective Loss 0.051620                                        LR 0.001000    Time 0.082816    
Epoch: [81][   70/  207]    Overall Loss 0.050289    Objective Loss 0.050289                                        LR 0.001000    Time 0.082041    
Epoch: [81][   80/  207]    Overall Loss 0.049510    Objective Loss 0.049510                                        LR 0.001000    Time 0.080841    
Epoch: [81][   90/  207]    Overall Loss 0.049027    Objective Loss 0.049027                                        LR 0.001000    Time 0.080130    
Epoch: [81][  100/  207]    Overall Loss 0.048585    Objective Loss 0.048585                                        LR 0.001000    Time 0.079708    
Epoch: [81][  110/  207]    Overall Loss 0.048015    Objective Loss 0.048015                                        LR 0.001000    Time 0.079061    
Epoch: [81][  120/  207]    Overall Loss 0.047366    Objective Loss 0.047366                                        LR 0.001000    Time 0.078897    
Epoch: [81][  130/  207]    Overall Loss 0.047183    Objective Loss 0.047183                                        LR 0.001000    Time 0.078429    
Epoch: [81][  140/  207]    Overall Loss 0.046979    Objective Loss 0.046979                                        LR 0.001000    Time 0.077942    
Epoch: [81][  150/  207]    Overall Loss 0.046669    Objective Loss 0.046669                                        LR 0.001000    Time 0.077780    
Epoch: [81][  160/  207]    Overall Loss 0.046458    Objective Loss 0.046458                                        LR 0.001000    Time 0.077790    
Epoch: [81][  170/  207]    Overall Loss 0.046084    Objective Loss 0.046084                                        LR 0.001000    Time 0.077402    
Epoch: [81][  180/  207]    Overall Loss 0.046080    Objective Loss 0.046080                                        LR 0.001000    Time 0.077107    
Epoch: [81][  190/  207]    Overall Loss 0.045834    Objective Loss 0.045834                                        LR 0.001000    Time 0.076786    
Epoch: [81][  200/  207]    Overall Loss 0.045652    Objective Loss 0.045652                                        LR 0.001000    Time 0.076575    
Epoch: [81][  207/  207]    Overall Loss 0.045575    Objective Loss 0.045575    Top1 95.696489    Top5 99.773499    LR 0.001000    Time 0.076263    
--- validate (epoch=81)-----------
5136 samples (512 per mini-batch)
Epoch: [81][   10/   11]    Loss 0.528341    Top1 84.218750    Top5 99.726562    
Epoch: [81][   11/   11]    Loss 0.733093    Top1 84.151090    Top5 99.727414    
==> Top1: 84.151    Top5: 99.727    Loss: 0.733

==> Confusion:
[[260   2   4   2   0   1   4  27]
 [  2 255  28   2   0   2   0  11]
 [  2  13 269   1   0   1   0  14]
 [  0   2   0 759  43  11   9  13]
 [  0   0   0  48 772   5  30  24]
 [  3   4   8  15  20 774  25  45]
 [  1   0   0  11  23   8 775  19]
 [ 23  13  14  54  72 111  44 458]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [82][   10/  207]    Overall Loss 0.040041    Objective Loss 0.040041                                        LR 0.001000    Time 0.127203    
Epoch: [82][   20/  207]    Overall Loss 0.039812    Objective Loss 0.039812                                        LR 0.001000    Time 0.103955    
Epoch: [82][   30/  207]    Overall Loss 0.037783    Objective Loss 0.037783                                        LR 0.001000    Time 0.093086    
Epoch: [82][   40/  207]    Overall Loss 0.037942    Objective Loss 0.037942                                        LR 0.001000    Time 0.087447    
Epoch: [82][   50/  207]    Overall Loss 0.036945    Objective Loss 0.036945                                        LR 0.001000    Time 0.084077    
Epoch: [82][   60/  207]    Overall Loss 0.037836    Objective Loss 0.037836                                        LR 0.001000    Time 0.082411    
Epoch: [82][   70/  207]    Overall Loss 0.038461    Objective Loss 0.038461                                        LR 0.001000    Time 0.080923    
Epoch: [82][   80/  207]    Overall Loss 0.038378    Objective Loss 0.038378                                        LR 0.001000    Time 0.079566    
Epoch: [82][   90/  207]    Overall Loss 0.038906    Objective Loss 0.038906                                        LR 0.001000    Time 0.078696    
Epoch: [82][  100/  207]    Overall Loss 0.039297    Objective Loss 0.039297                                        LR 0.001000    Time 0.078007    
Epoch: [82][  110/  207]    Overall Loss 0.039617    Objective Loss 0.039617                                        LR 0.001000    Time 0.077623    
Epoch: [82][  120/  207]    Overall Loss 0.040371    Objective Loss 0.040371                                        LR 0.001000    Time 0.077168    
Epoch: [82][  130/  207]    Overall Loss 0.040479    Objective Loss 0.040479                                        LR 0.001000    Time 0.077471    
Epoch: [82][  140/  207]    Overall Loss 0.041188    Objective Loss 0.041188                                        LR 0.001000    Time 0.077175    
Epoch: [82][  150/  207]    Overall Loss 0.041221    Objective Loss 0.041221                                        LR 0.001000    Time 0.076724    
Epoch: [82][  160/  207]    Overall Loss 0.041634    Objective Loss 0.041634                                        LR 0.001000    Time 0.076404    
Epoch: [82][  170/  207]    Overall Loss 0.041809    Objective Loss 0.041809                                        LR 0.001000    Time 0.076577    
Epoch: [82][  180/  207]    Overall Loss 0.041750    Objective Loss 0.041750                                        LR 0.001000    Time 0.076327    
Epoch: [82][  190/  207]    Overall Loss 0.042100    Objective Loss 0.042100                                        LR 0.001000    Time 0.076020    
Epoch: [82][  200/  207]    Overall Loss 0.042332    Objective Loss 0.042332                                        LR 0.001000    Time 0.075704    
Epoch: [82][  207/  207]    Overall Loss 0.042582    Objective Loss 0.042582    Top1 95.922990    Top5 100.000000    LR 0.001000    Time 0.075427    
--- validate (epoch=82)-----------
5136 samples (512 per mini-batch)
Epoch: [82][   10/   11]    Loss 0.487641    Top1 83.085938    Top5 99.765625    
Epoch: [82][   11/   11]    Loss 0.519845    Top1 83.041277    Top5 99.766355    
==> Top1: 83.041    Top5: 99.766    Loss: 0.520

==> Confusion:
[[273   5   3   2   0   2   3  12]
 [  3 254  34   2   0   3   0   4]
 [  3  15 269   1   0   6   0   6]
 [  0   2   0 747  50  18  12   8]
 [  1   0   0  42 776  14  28  18]
 [  6   4   7   9  23 820  11  14]
 [  1   0   0   6  29  21 767  13]
 [ 31  16  14  55  75 197  42 359]]

==> Best [Top1: 84.287   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 70]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [83][   10/  207]    Overall Loss 0.045130    Objective Loss 0.045130                                        LR 0.001000    Time 0.135921    
Epoch: [83][   20/  207]    Overall Loss 0.044692    Objective Loss 0.044692                                        LR 0.001000    Time 0.103127    
Epoch: [83][   30/  207]    Overall Loss 0.042953    Objective Loss 0.042953                                        LR 0.001000    Time 0.094475    
Epoch: [83][   40/  207]    Overall Loss 0.041116    Objective Loss 0.041116                                        LR 0.001000    Time 0.090164    
Epoch: [83][   50/  207]    Overall Loss 0.040498    Objective Loss 0.040498                                        LR 0.001000    Time 0.087334    
Epoch: [83][   60/  207]    Overall Loss 0.039923    Objective Loss 0.039923                                        LR 0.001000    Time 0.084643    
Epoch: [83][   70/  207]    Overall Loss 0.040076    Objective Loss 0.040076                                        LR 0.001000    Time 0.082627    
Epoch: [83][   80/  207]    Overall Loss 0.040619    Objective Loss 0.040619                                        LR 0.001000    Time 0.081122    
Epoch: [83][   90/  207]    Overall Loss 0.040714    Objective Loss 0.040714                                        LR 0.001000    Time 0.080058    
Epoch: [83][  100/  207]    Overall Loss 0.040369    Objective Loss 0.040369                                        LR 0.001000    Time 0.079273    
Epoch: [83][  110/  207]    Overall Loss 0.040288    Objective Loss 0.040288                                        LR 0.001000    Time 0.078642    
Epoch: [83][  120/  207]    Overall Loss 0.040047    Objective Loss 0.040047                                        LR 0.001000    Time 0.078166    
Epoch: [83][  130/  207]    Overall Loss 0.040300    Objective Loss 0.040300                                        LR 0.001000    Time 0.078030    
Epoch: [83][  140/  207]    Overall Loss 0.040474    Objective Loss 0.040474                                        LR 0.001000    Time 0.077911    
Epoch: [83][  150/  207]    Overall Loss 0.040321    Objective Loss 0.040321                                        LR 0.001000    Time 0.077597    
Epoch: [83][  160/  207]    Overall Loss 0.040128    Objective Loss 0.040128                                        LR 0.001000    Time 0.077478    
Epoch: [83][  170/  207]    Overall Loss 0.040475    Objective Loss 0.040475                                        LR 0.001000    Time 0.077488    
Epoch: [83][  180/  207]    Overall Loss 0.041072    Objective Loss 0.041072                                        LR 0.001000    Time 0.077316    
Epoch: [83][  190/  207]    Overall Loss 0.041047    Objective Loss 0.041047                                        LR 0.001000    Time 0.077204    
Epoch: [83][  200/  207]    Overall Loss 0.041613    Objective Loss 0.041613                                        LR 0.001000    Time 0.077198    
Epoch: [83][  207/  207]    Overall Loss 0.042067    Objective Loss 0.042067    Top1 96.036240    Top5 100.000000    LR 0.001000    Time 0.077149    
--- validate (epoch=83)-----------
5136 samples (512 per mini-batch)
Epoch: [83][   10/   11]    Loss 0.586674    Top1 84.277344    Top5 99.707031    
Epoch: [83][   11/   11]    Loss 0.552286    Top1 84.306854    Top5 99.707944    
==> Top1: 84.307    Top5: 99.708    Loss: 0.552

==> Confusion:
[[263   3   4   3   0   1   2  24]
 [  1 265  21   1   0   0   0  12]
 [  3  17 268   1   0   0   0  11]
 [  0   3   0 748  48   5  10  23]
 [  2   0   0  48 779   3  23  24]
 [  3   8  11  17  21 741  16  77]
 [  1   0   0   8  34   9 761  24]
 [ 20  18  15  51  73  72  35 505]]

==> Best [Top1: 84.307   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 83]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [84][   10/  207]    Overall Loss 0.041293    Objective Loss 0.041293                                        LR 0.001000    Time 0.135943    
Epoch: [84][   20/  207]    Overall Loss 0.039001    Objective Loss 0.039001                                        LR 0.001000    Time 0.103900    
Epoch: [84][   30/  207]    Overall Loss 0.036655    Objective Loss 0.036655                                        LR 0.001000    Time 0.094135    
Epoch: [84][   40/  207]    Overall Loss 0.037829    Objective Loss 0.037829                                        LR 0.001000    Time 0.088552    
Epoch: [84][   50/  207]    Overall Loss 0.038428    Objective Loss 0.038428                                        LR 0.001000    Time 0.085148    
Epoch: [84][   60/  207]    Overall Loss 0.039029    Objective Loss 0.039029                                        LR 0.001000    Time 0.082682    
Epoch: [84][   70/  207]    Overall Loss 0.039276    Objective Loss 0.039276                                        LR 0.001000    Time 0.081699    
Epoch: [84][   80/  207]    Overall Loss 0.039570    Objective Loss 0.039570                                        LR 0.001000    Time 0.080280    
Epoch: [84][   90/  207]    Overall Loss 0.039741    Objective Loss 0.039741                                        LR 0.001000    Time 0.079888    
Epoch: [84][  100/  207]    Overall Loss 0.039370    Objective Loss 0.039370                                        LR 0.001000    Time 0.080398    
Epoch: [84][  110/  207]    Overall Loss 0.039857    Objective Loss 0.039857                                        LR 0.001000    Time 0.079544    
Epoch: [84][  120/  207]    Overall Loss 0.039939    Objective Loss 0.039939                                        LR 0.001000    Time 0.078850    
Epoch: [84][  130/  207]    Overall Loss 0.040230    Objective Loss 0.040230                                        LR 0.001000    Time 0.078383    
Epoch: [84][  140/  207]    Overall Loss 0.040433    Objective Loss 0.040433                                        LR 0.001000    Time 0.078086    
Epoch: [84][  150/  207]    Overall Loss 0.041278    Objective Loss 0.041278                                        LR 0.001000    Time 0.077703    
Epoch: [84][  160/  207]    Overall Loss 0.043065    Objective Loss 0.043065                                        LR 0.001000    Time 0.077310    
Epoch: [84][  170/  207]    Overall Loss 0.046380    Objective Loss 0.046380                                        LR 0.001000    Time 0.076878    
Epoch: [84][  180/  207]    Overall Loss 0.050747    Objective Loss 0.050747                                        LR 0.001000    Time 0.076660    
Epoch: [84][  190/  207]    Overall Loss 0.055154    Objective Loss 0.055154                                        LR 0.001000    Time 0.076350    
Epoch: [84][  200/  207]    Overall Loss 0.059576    Objective Loss 0.059576                                        LR 0.001000    Time 0.076161    
Epoch: [84][  207/  207]    Overall Loss 0.062577    Objective Loss 0.062577    Top1 93.091733    Top5 100.000000    LR 0.001000    Time 0.075929    
--- validate (epoch=84)-----------
5136 samples (512 per mini-batch)
Epoch: [84][   10/   11]    Loss 0.624041    Top1 80.253906    Top5 99.531250    
Epoch: [84][   11/   11]    Loss 0.576709    Top1 80.295950    Top5 99.532710    
==> Top1: 80.296    Top5: 99.533    Loss: 0.577

==> Confusion:
[[280   7   4   3   0   1   0   5]
 [  5 277  15   2   0   0   0   1]
 [  6  55 234   2   0   2   0   1]
 [  1   6   0 792  26   9   2   1]
 [  2   0   0 104 743   1  14  15]
 [ 21  16  16  40  22 751  12  16]
 [  9   0   0  11  65   9 723  20]
 [ 61  37  24 121  79 123  20 324]]

==> Best [Top1: 84.307   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 83]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [85][   10/  207]    Overall Loss 0.249443    Objective Loss 0.249443                                        LR 0.001000    Time 0.134413    
Epoch: [85][   20/  207]    Overall Loss 0.263600    Objective Loss 0.263600                                        LR 0.001000    Time 0.107386    
Epoch: [85][   30/  207]    Overall Loss 0.243298    Objective Loss 0.243298                                        LR 0.001000    Time 0.098737    
Epoch: [85][   40/  207]    Overall Loss 0.226720    Objective Loss 0.226720                                        LR 0.001000    Time 0.112140    
Epoch: [85][   50/  207]    Overall Loss 0.216296    Objective Loss 0.216296                                        LR 0.001000    Time 0.108884    
Epoch: [85][   60/  207]    Overall Loss 0.210671    Objective Loss 0.210671                                        LR 0.001000    Time 0.103238    
Epoch: [85][   70/  207]    Overall Loss 0.206627    Objective Loss 0.206627                                        LR 0.001000    Time 0.098960    
Epoch: [85][   80/  207]    Overall Loss 0.204277    Objective Loss 0.204277                                        LR 0.001000    Time 0.098080    
Epoch: [85][   90/  207]    Overall Loss 0.199479    Objective Loss 0.199479                                        LR 0.001000    Time 0.097016    
Epoch: [85][  100/  207]    Overall Loss 0.195179    Objective Loss 0.195179                                        LR 0.001000    Time 0.094518    
Epoch: [85][  110/  207]    Overall Loss 0.191097    Objective Loss 0.191097                                        LR 0.001000    Time 0.093309    
Epoch: [85][  120/  207]    Overall Loss 0.188682    Objective Loss 0.188682                                        LR 0.001000    Time 0.091815    
Epoch: [85][  130/  207]    Overall Loss 0.183996    Objective Loss 0.183996                                        LR 0.001000    Time 0.090576    
Epoch: [85][  140/  207]    Overall Loss 0.181295    Objective Loss 0.181295                                        LR 0.001000    Time 0.089426    
Epoch: [85][  150/  207]    Overall Loss 0.177546    Objective Loss 0.177546                                        LR 0.001000    Time 0.088391    
Epoch: [85][  160/  207]    Overall Loss 0.175434    Objective Loss 0.175434                                        LR 0.001000    Time 0.087352    
Epoch: [85][  170/  207]    Overall Loss 0.174593    Objective Loss 0.174593                                        LR 0.001000    Time 0.086332    
Epoch: [85][  180/  207]    Overall Loss 0.172235    Objective Loss 0.172235                                        LR 0.001000    Time 0.085477    
Epoch: [85][  190/  207]    Overall Loss 0.170803    Objective Loss 0.170803                                        LR 0.001000    Time 0.084845    
Epoch: [85][  200/  207]    Overall Loss 0.169542    Objective Loss 0.169542                                        LR 0.001000    Time 0.084186    
Epoch: [85][  207/  207]    Overall Loss 0.169532    Objective Loss 0.169532    Top1 93.091733    Top5 99.886750    LR 0.001000    Time 0.083625    
--- validate (epoch=85)-----------
5136 samples (512 per mini-batch)
Epoch: [85][   10/   11]    Loss 0.561467    Top1 81.015625    Top5 99.453125    
Epoch: [85][   11/   11]    Loss 0.610593    Top1 81.016355    Top5 99.454829    
==> Top1: 81.016    Top5: 99.455    Loss: 0.611

==> Confusion:
[[283   4   2   1   0   5   0   5]
 [  8 253  34   0   0   2   0   3]
 [  4  26 267   0   0   1   0   2]
 [  1   4   1 733  50  29   0  19]
 [  4   0   3  39 767  32   9  25]
 [ 23   9   9   8  13 811   5  16]
 [  7   0   0   5  39  35 714  37]
 [ 53  26  29  48  72 217  11 333]]

==> Best [Top1: 84.307   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 83]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [86][   10/  207]    Overall Loss 0.183518    Objective Loss 0.183518                                        LR 0.001000    Time 0.131409    
Epoch: [86][   20/  207]    Overall Loss 0.191925    Objective Loss 0.191925                                        LR 0.001000    Time 0.103642    
Epoch: [86][   30/  207]    Overall Loss 0.188838    Objective Loss 0.188838                                        LR 0.001000    Time 0.093849    
Epoch: [86][   40/  207]    Overall Loss 0.188329    Objective Loss 0.188329                                        LR 0.001000    Time 0.088258    
Epoch: [86][   50/  207]    Overall Loss 0.180958    Objective Loss 0.180958                                        LR 0.001000    Time 0.085824    
Epoch: [86][   60/  207]    Overall Loss 0.170144    Objective Loss 0.170144                                        LR 0.001000    Time 0.085568    
Epoch: [86][   70/  207]    Overall Loss 0.160741    Objective Loss 0.160741                                        LR 0.001000    Time 0.083850    
Epoch: [86][   80/  207]    Overall Loss 0.151357    Objective Loss 0.151357                                        LR 0.001000    Time 0.082485    
Epoch: [86][   90/  207]    Overall Loss 0.145281    Objective Loss 0.145281                                        LR 0.001000    Time 0.082110    
Epoch: [86][  100/  207]    Overall Loss 0.139509    Objective Loss 0.139509                                        LR 0.001000    Time 0.081383    
Epoch: [86][  110/  207]    Overall Loss 0.134473    Objective Loss 0.134473                                        LR 0.001000    Time 0.080423    
Epoch: [86][  120/  207]    Overall Loss 0.131000    Objective Loss 0.131000                                        LR 0.001000    Time 0.080392    
Epoch: [86][  130/  207]    Overall Loss 0.127819    Objective Loss 0.127819                                        LR 0.001000    Time 0.079867    
Epoch: [86][  140/  207]    Overall Loss 0.125559    Objective Loss 0.125559                                        LR 0.001000    Time 0.079501    
Epoch: [86][  150/  207]    Overall Loss 0.122404    Objective Loss 0.122404                                        LR 0.001000    Time 0.078906    
Epoch: [86][  160/  207]    Overall Loss 0.120091    Objective Loss 0.120091                                        LR 0.001000    Time 0.078500    
Epoch: [86][  170/  207]    Overall Loss 0.117791    Objective Loss 0.117791                                        LR 0.001000    Time 0.078376    
Epoch: [86][  180/  207]    Overall Loss 0.116030    Objective Loss 0.116030                                        LR 0.001000    Time 0.078116    
Epoch: [86][  190/  207]    Overall Loss 0.115037    Objective Loss 0.115037                                        LR 0.001000    Time 0.078203    
Epoch: [86][  200/  207]    Overall Loss 0.113555    Objective Loss 0.113555                                        LR 0.001000    Time 0.078311    
Epoch: [86][  207/  207]    Overall Loss 0.114216    Objective Loss 0.114216    Top1 93.544734    Top5 100.000000    LR 0.001000    Time 0.078369    
--- validate (epoch=86)-----------
5136 samples (512 per mini-batch)
Epoch: [86][   10/   11]    Loss 0.530133    Top1 81.816406    Top5 99.570312    
Epoch: [86][   11/   11]    Loss 0.532758    Top1 81.834112    Top5 99.571651    
==> Top1: 81.834    Top5: 99.572    Loss: 0.533

==> Confusion:
[[282   7   1   2   0   0   1   7]
 [  7 275  14   1   0   0   0   3]
 [  7  54 235   0   0   0   0   4]
 [  1   5   1 756  46  10  11   7]
 [  5   0   0  47 769   2  33  23]
 [ 32  15  13  15  23 726  23  47]
 [  3   0   0   7  29   7 773  18]
 [ 75  41  14  57  69 103  43 387]]

==> Best [Top1: 84.307   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 83]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [87][   10/  207]    Overall Loss 0.095573    Objective Loss 0.095573                                        LR 0.001000    Time 0.135098    
Epoch: [87][   20/  207]    Overall Loss 0.107325    Objective Loss 0.107325                                        LR 0.001000    Time 0.105748    
Epoch: [87][   30/  207]    Overall Loss 0.103204    Objective Loss 0.103204                                        LR 0.001000    Time 0.096220    
Epoch: [87][   40/  207]    Overall Loss 0.098516    Objective Loss 0.098516                                        LR 0.001000    Time 0.090023    
Epoch: [87][   50/  207]    Overall Loss 0.095216    Objective Loss 0.095216                                        LR 0.001000    Time 0.086151    
Epoch: [87][   60/  207]    Overall Loss 0.091443    Objective Loss 0.091443                                        LR 0.001000    Time 0.083822    
Epoch: [87][   70/  207]    Overall Loss 0.087917    Objective Loss 0.087917                                        LR 0.001000    Time 0.082391    
Epoch: [87][   80/  207]    Overall Loss 0.084847    Objective Loss 0.084847                                        LR 0.001000    Time 0.081167    
Epoch: [87][   90/  207]    Overall Loss 0.083302    Objective Loss 0.083302                                        LR 0.001000    Time 0.081609    
Epoch: [87][  100/  207]    Overall Loss 0.081015    Objective Loss 0.081015                                        LR 0.001000    Time 0.082185    
Epoch: [87][  110/  207]    Overall Loss 0.079259    Objective Loss 0.079259                                        LR 0.001000    Time 0.081610    
Epoch: [87][  120/  207]    Overall Loss 0.078579    Objective Loss 0.078579                                        LR 0.001000    Time 0.081230    
Epoch: [87][  130/  207]    Overall Loss 0.078512    Objective Loss 0.078512                                        LR 0.001000    Time 0.080898    
Epoch: [87][  140/  207]    Overall Loss 0.077150    Objective Loss 0.077150                                        LR 0.001000    Time 0.080601    
Epoch: [87][  150/  207]    Overall Loss 0.076436    Objective Loss 0.076436                                        LR 0.001000    Time 0.080315    
Epoch: [87][  160/  207]    Overall Loss 0.074806    Objective Loss 0.074806                                        LR 0.001000    Time 0.080533    
Epoch: [87][  170/  207]    Overall Loss 0.073932    Objective Loss 0.073932                                        LR 0.001000    Time 0.080897    
Epoch: [87][  180/  207]    Overall Loss 0.073334    Objective Loss 0.073334                                        LR 0.001000    Time 0.081271    
Epoch: [87][  190/  207]    Overall Loss 0.072902    Objective Loss 0.072902                                        LR 0.001000    Time 0.081298    
Epoch: [87][  200/  207]    Overall Loss 0.072248    Objective Loss 0.072248                                        LR 0.001000    Time 0.080953    
Epoch: [87][  207/  207]    Overall Loss 0.071963    Objective Loss 0.071963    Top1 95.809740    Top5 99.886750    LR 0.001000    Time 0.080503    
--- validate (epoch=87)-----------
5136 samples (512 per mini-batch)
Epoch: [87][   10/   11]    Loss 0.587294    Top1 83.906250    Top5 99.746094    
Epoch: [87][   11/   11]    Loss 0.573143    Top1 83.897975    Top5 99.746885    
==> Top1: 83.898    Top5: 99.747    Loss: 0.573

==> Confusion:
[[235   9  13   2   0   4   2  35]
 [  1 261  31   1   1   1   0   4]
 [  0  15 281   1   0   0   0   3]
 [  0   6   0 760  44   7   7  13]
 [  1   0   0  47 781   6  22  22]
 [  2   7  20  14  28 763  13  47]
 [  1   0   0   8  42   4 757  25]
 [ 15  23  23  50  84 102  21 471]]

==> Best [Top1: 84.307   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 83]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [88][   10/  207]    Overall Loss 0.064264    Objective Loss 0.064264                                        LR 0.001000    Time 0.153003    
Epoch: [88][   20/  207]    Overall Loss 0.063489    Objective Loss 0.063489                                        LR 0.001000    Time 0.118638    
Epoch: [88][   30/  207]    Overall Loss 0.062460    Objective Loss 0.062460                                        LR 0.001000    Time 0.109372    
Epoch: [88][   40/  207]    Overall Loss 0.060648    Objective Loss 0.060648                                        LR 0.001000    Time 0.100961    
Epoch: [88][   50/  207]    Overall Loss 0.058201    Objective Loss 0.058201                                        LR 0.001000    Time 0.095512    
Epoch: [88][   60/  207]    Overall Loss 0.055619    Objective Loss 0.055619                                        LR 0.001000    Time 0.091467    
Epoch: [88][   70/  207]    Overall Loss 0.055043    Objective Loss 0.055043                                        LR 0.001000    Time 0.088969    
Epoch: [88][   80/  207]    Overall Loss 0.053555    Objective Loss 0.053555                                        LR 0.001000    Time 0.087077    
Epoch: [88][   90/  207]    Overall Loss 0.052447    Objective Loss 0.052447                                        LR 0.001000    Time 0.085585    
Epoch: [88][  100/  207]    Overall Loss 0.051533    Objective Loss 0.051533                                        LR 0.001000    Time 0.084224    
Epoch: [88][  110/  207]    Overall Loss 0.050958    Objective Loss 0.050958                                        LR 0.001000    Time 0.083328    
Epoch: [88][  120/  207]    Overall Loss 0.050259    Objective Loss 0.050259                                        LR 0.001000    Time 0.082632    
Epoch: [88][  130/  207]    Overall Loss 0.049949    Objective Loss 0.049949                                        LR 0.001000    Time 0.081825    
Epoch: [88][  140/  207]    Overall Loss 0.050143    Objective Loss 0.050143                                        LR 0.001000    Time 0.081089    
Epoch: [88][  150/  207]    Overall Loss 0.050176    Objective Loss 0.050176                                        LR 0.001000    Time 0.080447    
Epoch: [88][  160/  207]    Overall Loss 0.050074    Objective Loss 0.050074                                        LR 0.001000    Time 0.080084    
Epoch: [88][  170/  207]    Overall Loss 0.050253    Objective Loss 0.050253                                        LR 0.001000    Time 0.079619    
Epoch: [88][  180/  207]    Overall Loss 0.049883    Objective Loss 0.049883                                        LR 0.001000    Time 0.079213    
Epoch: [88][  190/  207]    Overall Loss 0.049456    Objective Loss 0.049456                                        LR 0.001000    Time 0.078915    
Epoch: [88][  200/  207]    Overall Loss 0.049377    Objective Loss 0.049377                                        LR 0.001000    Time 0.078657    
Epoch: [88][  207/  207]    Overall Loss 0.049013    Objective Loss 0.049013    Top1 95.696489    Top5 99.886750    LR 0.001000    Time 0.078284    
--- validate (epoch=88)-----------
5136 samples (512 per mini-batch)
Epoch: [88][   10/   11]    Loss 0.494949    Top1 84.453125    Top5 99.785156    
Epoch: [88][   11/   11]    Loss 0.484310    Top1 84.443146    Top5 99.785826    
==> Top1: 84.443    Top5: 99.786    Loss: 0.484

==> Confusion:
[[269   6   2   2   0   1   2  18]
 [  1 269  23   1   0   1   0   5]
 [  3  16 274   2   0   1   0   4]
 [  0   4   0 764  40  11   2  16]
 [  3   0   0  49 765  10  24  28]
 [  8   6   6  12  17 796  10  39]
 [  2   0   0  11  30  12 755  27]
 [ 28  14  18  65  63 131  25 445]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [89][   10/  207]    Overall Loss 0.036945    Objective Loss 0.036945                                        LR 0.001000    Time 0.112622    
Epoch: [89][   20/  207]    Overall Loss 0.036451    Objective Loss 0.036451                                        LR 0.001000    Time 0.092623    
Epoch: [89][   30/  207]    Overall Loss 0.035488    Objective Loss 0.035488                                        LR 0.001000    Time 0.085326    
Epoch: [89][   40/  207]    Overall Loss 0.035713    Objective Loss 0.035713                                        LR 0.001000    Time 0.082074    
Epoch: [89][   50/  207]    Overall Loss 0.036300    Objective Loss 0.036300                                        LR 0.001000    Time 0.080149    
Epoch: [89][   60/  207]    Overall Loss 0.035433    Objective Loss 0.035433                                        LR 0.001000    Time 0.079071    
Epoch: [89][   70/  207]    Overall Loss 0.034871    Objective Loss 0.034871                                        LR 0.001000    Time 0.078361    
Epoch: [89][   80/  207]    Overall Loss 0.035355    Objective Loss 0.035355                                        LR 0.001000    Time 0.077760    
Epoch: [89][   90/  207]    Overall Loss 0.035727    Objective Loss 0.035727                                        LR 0.001000    Time 0.078444    
Epoch: [89][  100/  207]    Overall Loss 0.036598    Objective Loss 0.036598                                        LR 0.001000    Time 0.078104    
Epoch: [89][  110/  207]    Overall Loss 0.036498    Objective Loss 0.036498                                        LR 0.001000    Time 0.077560    
Epoch: [89][  120/  207]    Overall Loss 0.037146    Objective Loss 0.037146                                        LR 0.001000    Time 0.077351    
Epoch: [89][  130/  207]    Overall Loss 0.037822    Objective Loss 0.037822                                        LR 0.001000    Time 0.077255    
Epoch: [89][  140/  207]    Overall Loss 0.038878    Objective Loss 0.038878                                        LR 0.001000    Time 0.077139    
Epoch: [89][  150/  207]    Overall Loss 0.039451    Objective Loss 0.039451                                        LR 0.001000    Time 0.077264    
Epoch: [89][  160/  207]    Overall Loss 0.039977    Objective Loss 0.039977                                        LR 0.001000    Time 0.077347    
Epoch: [89][  170/  207]    Overall Loss 0.040421    Objective Loss 0.040421                                        LR 0.001000    Time 0.077114    
Epoch: [89][  180/  207]    Overall Loss 0.040909    Objective Loss 0.040909                                        LR 0.001000    Time 0.076823    
Epoch: [89][  190/  207]    Overall Loss 0.041016    Objective Loss 0.041016                                        LR 0.001000    Time 0.076568    
Epoch: [89][  200/  207]    Overall Loss 0.040776    Objective Loss 0.040776                                        LR 0.001000    Time 0.076440    
Epoch: [89][  207/  207]    Overall Loss 0.040761    Objective Loss 0.040761    Top1 96.375991    Top5 99.886750    LR 0.001000    Time 0.076235    
--- validate (epoch=89)-----------
5136 samples (512 per mini-batch)
Epoch: [89][   10/   11]    Loss 0.490249    Top1 83.378906    Top5 99.667969    
Epoch: [89][   11/   11]    Loss 0.502777    Top1 83.391745    Top5 99.669003    
==> Top1: 83.392    Top5: 99.669    Loss: 0.503

==> Confusion:
[[264   5   4   3   0   5   2  17]
 [  1 268  22   1   0   4   0   4]
 [  3  18 272   0   0   3   0   4]
 [  0   2   0 759  40  16   9  11]
 [  2   0   0  58 745  16  35  23]
 [  5   8   8   6  10 827  13  17]
 [  1   0   0   6  19  15 782  14]
 [ 17  16  16  65  56 200  53 366]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [90][   10/  207]    Overall Loss 0.035571    Objective Loss 0.035571                                        LR 0.001000    Time 0.131340    
Epoch: [90][   20/  207]    Overall Loss 0.037037    Objective Loss 0.037037                                        LR 0.001000    Time 0.102335    
Epoch: [90][   30/  207]    Overall Loss 0.039670    Objective Loss 0.039670                                        LR 0.001000    Time 0.093953    
Epoch: [90][   40/  207]    Overall Loss 0.040820    Objective Loss 0.040820                                        LR 0.001000    Time 0.087984    
Epoch: [90][   50/  207]    Overall Loss 0.039438    Objective Loss 0.039438                                        LR 0.001000    Time 0.084489    
Epoch: [90][   60/  207]    Overall Loss 0.039473    Objective Loss 0.039473                                        LR 0.001000    Time 0.082376    
Epoch: [90][   70/  207]    Overall Loss 0.038734    Objective Loss 0.038734                                        LR 0.001000    Time 0.080988    
Epoch: [90][   80/  207]    Overall Loss 0.037581    Objective Loss 0.037581                                        LR 0.001000    Time 0.080115    
Epoch: [90][   90/  207]    Overall Loss 0.036899    Objective Loss 0.036899                                        LR 0.001000    Time 0.079346    
Epoch: [90][  100/  207]    Overall Loss 0.036604    Objective Loss 0.036604                                        LR 0.001000    Time 0.078549    
Epoch: [90][  110/  207]    Overall Loss 0.036383    Objective Loss 0.036383                                        LR 0.001000    Time 0.078139    
Epoch: [90][  120/  207]    Overall Loss 0.036389    Objective Loss 0.036389                                        LR 0.001000    Time 0.077554    
Epoch: [90][  130/  207]    Overall Loss 0.036467    Objective Loss 0.036467                                        LR 0.001000    Time 0.077023    
Epoch: [90][  140/  207]    Overall Loss 0.036313    Objective Loss 0.036313                                        LR 0.001000    Time 0.076691    
Epoch: [90][  150/  207]    Overall Loss 0.036198    Objective Loss 0.036198                                        LR 0.001000    Time 0.076292    
Epoch: [90][  160/  207]    Overall Loss 0.036351    Objective Loss 0.036351                                        LR 0.001000    Time 0.075924    
Epoch: [90][  170/  207]    Overall Loss 0.036985    Objective Loss 0.036985                                        LR 0.001000    Time 0.075560    
Epoch: [90][  180/  207]    Overall Loss 0.037685    Objective Loss 0.037685                                        LR 0.001000    Time 0.075290    
Epoch: [90][  190/  207]    Overall Loss 0.038678    Objective Loss 0.038678                                        LR 0.001000    Time 0.075123    
Epoch: [90][  200/  207]    Overall Loss 0.039248    Objective Loss 0.039248                                        LR 0.001000    Time 0.074974    
Epoch: [90][  207/  207]    Overall Loss 0.039474    Objective Loss 0.039474    Top1 97.055493    Top5 99.886750    LR 0.001000    Time 0.074731    
--- validate (epoch=90)-----------
5136 samples (512 per mini-batch)
Epoch: [90][   10/   11]    Loss 0.559521    Top1 84.023438    Top5 99.609375    
Epoch: [90][   11/   11]    Loss 0.587196    Top1 84.014798    Top5 99.610592    
==> Top1: 84.015    Top5: 99.611    Loss: 0.587

==> Confusion:
[[264   2   1   1   1   4   3  24]
 [  1 265  17   1   0   4   0  12]
 [  2  14 264   1   0   4   0  15]
 [  0   2   0 752  41  12  15  15]
 [  0   0   0  54 747  18  37  23]
 [  1   6   6   9  12 807  21  32]
 [  1   0   0   6  20  12 784  14]
 [ 18   9  16  49  63 146  56 432]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [91][   10/  207]    Overall Loss 0.040022    Objective Loss 0.040022                                        LR 0.001000    Time 0.130419    
Epoch: [91][   20/  207]    Overall Loss 0.038527    Objective Loss 0.038527                                        LR 0.001000    Time 0.101586    
Epoch: [91][   30/  207]    Overall Loss 0.039096    Objective Loss 0.039096                                        LR 0.001000    Time 0.092235    
Epoch: [91][   40/  207]    Overall Loss 0.038310    Objective Loss 0.038310                                        LR 0.001000    Time 0.087219    
Epoch: [91][   50/  207]    Overall Loss 0.037107    Objective Loss 0.037107                                        LR 0.001000    Time 0.084115    
Epoch: [91][   60/  207]    Overall Loss 0.036729    Objective Loss 0.036729                                        LR 0.001000    Time 0.082389    
Epoch: [91][   70/  207]    Overall Loss 0.036300    Objective Loss 0.036300                                        LR 0.001000    Time 0.080703    
Epoch: [91][   80/  207]    Overall Loss 0.036116    Objective Loss 0.036116                                        LR 0.001000    Time 0.079873    
Epoch: [91][   90/  207]    Overall Loss 0.036271    Objective Loss 0.036271                                        LR 0.001000    Time 0.079359    
Epoch: [91][  100/  207]    Overall Loss 0.036862    Objective Loss 0.036862                                        LR 0.001000    Time 0.078861    
Epoch: [91][  110/  207]    Overall Loss 0.037048    Objective Loss 0.037048                                        LR 0.001000    Time 0.079432    
Epoch: [91][  120/  207]    Overall Loss 0.037086    Objective Loss 0.037086                                        LR 0.001000    Time 0.079612    
Epoch: [91][  130/  207]    Overall Loss 0.037506    Objective Loss 0.037506                                        LR 0.001000    Time 0.079552    
Epoch: [91][  140/  207]    Overall Loss 0.038117    Objective Loss 0.038117                                        LR 0.001000    Time 0.079441    
Epoch: [91][  150/  207]    Overall Loss 0.038444    Objective Loss 0.038444                                        LR 0.001000    Time 0.079509    
Epoch: [91][  160/  207]    Overall Loss 0.038634    Objective Loss 0.038634                                        LR 0.001000    Time 0.079302    
Epoch: [91][  170/  207]    Overall Loss 0.038657    Objective Loss 0.038657                                        LR 0.001000    Time 0.079338    
Epoch: [91][  180/  207]    Overall Loss 0.038674    Objective Loss 0.038674                                        LR 0.001000    Time 0.079096    
Epoch: [91][  190/  207]    Overall Loss 0.038750    Objective Loss 0.038750                                        LR 0.001000    Time 0.078904    
Epoch: [91][  200/  207]    Overall Loss 0.038839    Objective Loss 0.038839                                        LR 0.001000    Time 0.078744    
Epoch: [91][  207/  207]    Overall Loss 0.039130    Objective Loss 0.039130    Top1 97.508494    Top5 100.000000    LR 0.001000    Time 0.078523    
--- validate (epoch=91)-----------
5136 samples (512 per mini-batch)
Epoch: [91][   10/   11]    Loss 0.541712    Top1 84.121094    Top5 99.726562    
Epoch: [91][   11/   11]    Loss 0.504880    Top1 84.092679    Top5 99.727414    
==> Top1: 84.093    Top5: 99.727    Loss: 0.505

==> Confusion:
[[271   2   4   3   1   3   2  14]
 [  2 261  26   1   0   4   1   5]
 [  2  16 273   1   0   3   0   5]
 [  0   0   0 744  62  10  11  10]
 [  1   0   0  33 804   3  14  24]
 [  5   6   5  13  27 780  23  35]
 [  1   0   0   6  47   5 767  11]
 [ 30  14  13  55  88 121  49 419]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [92][   10/  207]    Overall Loss 0.036604    Objective Loss 0.036604                                        LR 0.001000    Time 0.136759    
Epoch: [92][   20/  207]    Overall Loss 0.037659    Objective Loss 0.037659                                        LR 0.001000    Time 0.107223    
Epoch: [92][   30/  207]    Overall Loss 0.038866    Objective Loss 0.038866                                        LR 0.001000    Time 0.096966    
Epoch: [92][   40/  207]    Overall Loss 0.038193    Objective Loss 0.038193                                        LR 0.001000    Time 0.091313    
Epoch: [92][   50/  207]    Overall Loss 0.038678    Objective Loss 0.038678                                        LR 0.001000    Time 0.088881    
Epoch: [92][   60/  207]    Overall Loss 0.040112    Objective Loss 0.040112                                        LR 0.001000    Time 0.086751    
Epoch: [92][   70/  207]    Overall Loss 0.040924    Objective Loss 0.040924                                        LR 0.001000    Time 0.085143    
Epoch: [92][   80/  207]    Overall Loss 0.041990    Objective Loss 0.041990                                        LR 0.001000    Time 0.083436    
Epoch: [92][   90/  207]    Overall Loss 0.041966    Objective Loss 0.041966                                        LR 0.001000    Time 0.082040    
Epoch: [92][  100/  207]    Overall Loss 0.042519    Objective Loss 0.042519                                        LR 0.001000    Time 0.081195    
Epoch: [92][  110/  207]    Overall Loss 0.043538    Objective Loss 0.043538                                        LR 0.001000    Time 0.080403    
Epoch: [92][  120/  207]    Overall Loss 0.044073    Objective Loss 0.044073                                        LR 0.001000    Time 0.079850    
Epoch: [92][  130/  207]    Overall Loss 0.044441    Objective Loss 0.044441                                        LR 0.001000    Time 0.079215    
Epoch: [92][  140/  207]    Overall Loss 0.044140    Objective Loss 0.044140                                        LR 0.001000    Time 0.078716    
Epoch: [92][  150/  207]    Overall Loss 0.044057    Objective Loss 0.044057                                        LR 0.001000    Time 0.078448    
Epoch: [92][  160/  207]    Overall Loss 0.044136    Objective Loss 0.044136                                        LR 0.001000    Time 0.078068    
Epoch: [92][  170/  207]    Overall Loss 0.044720    Objective Loss 0.044720                                        LR 0.001000    Time 0.077607    
Epoch: [92][  180/  207]    Overall Loss 0.046456    Objective Loss 0.046456                                        LR 0.001000    Time 0.077339    
Epoch: [92][  190/  207]    Overall Loss 0.051087    Objective Loss 0.051087                                        LR 0.001000    Time 0.077111    
Epoch: [92][  200/  207]    Overall Loss 0.056157    Objective Loss 0.056157                                        LR 0.001000    Time 0.076947    
Epoch: [92][  207/  207]    Overall Loss 0.059302    Objective Loss 0.059302    Top1 93.544734    Top5 99.660249    LR 0.001000    Time 0.076602    
--- validate (epoch=92)-----------
5136 samples (512 per mini-batch)
Epoch: [92][   10/   11]    Loss 0.722984    Top1 79.648438    Top5 99.589844    
Epoch: [92][   11/   11]    Loss 0.666868    Top1 79.672897    Top5 99.591121    
==> Top1: 79.673    Top5: 99.591    Loss: 0.667

==> Confusion:
[[239   7  18   2   1   5   6  22]
 [  1 228  66   1   0   1   1   2]
 [  0  18 280   0   0   2   0   0]
 [  0   8   2 713  90   6  11   7]
 [  1   0   0  29 813   3  22  11]
 [  4   8  26  20  36 710  54  36]
 [  1   0   1   4  56   1 765   9]
 [ 17  23  35  79 117  96  78 344]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [93][   10/  207]    Overall Loss 0.164633    Objective Loss 0.164633                                        LR 0.001000    Time 0.129250    
Epoch: [93][   20/  207]    Overall Loss 0.172426    Objective Loss 0.172426                                        LR 0.001000    Time 0.099670    
Epoch: [93][   30/  207]    Overall Loss 0.164377    Objective Loss 0.164377                                        LR 0.001000    Time 0.090227    
Epoch: [93][   40/  207]    Overall Loss 0.163498    Objective Loss 0.163498                                        LR 0.001000    Time 0.085498    
Epoch: [93][   50/  207]    Overall Loss 0.158910    Objective Loss 0.158910                                        LR 0.001000    Time 0.083004    
Epoch: [93][   60/  207]    Overall Loss 0.157967    Objective Loss 0.157967                                        LR 0.001000    Time 0.081191    
Epoch: [93][   70/  207]    Overall Loss 0.159898    Objective Loss 0.159898                                        LR 0.001000    Time 0.079702    
Epoch: [93][   80/  207]    Overall Loss 0.160567    Objective Loss 0.160567                                        LR 0.001000    Time 0.078908    
Epoch: [93][   90/  207]    Overall Loss 0.161901    Objective Loss 0.161901                                        LR 0.001000    Time 0.078049    
Epoch: [93][  100/  207]    Overall Loss 0.163476    Objective Loss 0.163476                                        LR 0.001000    Time 0.077155    
Epoch: [93][  110/  207]    Overall Loss 0.165262    Objective Loss 0.165262                                        LR 0.001000    Time 0.076686    
Epoch: [93][  120/  207]    Overall Loss 0.165510    Objective Loss 0.165510                                        LR 0.001000    Time 0.076263    
Epoch: [93][  130/  207]    Overall Loss 0.165927    Objective Loss 0.165927                                        LR 0.001000    Time 0.075866    
Epoch: [93][  140/  207]    Overall Loss 0.167546    Objective Loss 0.167546                                        LR 0.001000    Time 0.075663    
Epoch: [93][  150/  207]    Overall Loss 0.165997    Objective Loss 0.165997                                        LR 0.001000    Time 0.075483    
Epoch: [93][  160/  207]    Overall Loss 0.164364    Objective Loss 0.164364                                        LR 0.001000    Time 0.075388    
Epoch: [93][  170/  207]    Overall Loss 0.161522    Objective Loss 0.161522                                        LR 0.001000    Time 0.075329    
Epoch: [93][  180/  207]    Overall Loss 0.157913    Objective Loss 0.157913                                        LR 0.001000    Time 0.075133    
Epoch: [93][  190/  207]    Overall Loss 0.154586    Objective Loss 0.154586                                        LR 0.001000    Time 0.075070    
Epoch: [93][  200/  207]    Overall Loss 0.151171    Objective Loss 0.151171                                        LR 0.001000    Time 0.074944    
Epoch: [93][  207/  207]    Overall Loss 0.149572    Objective Loss 0.149572    Top1 95.130238    Top5 100.000000    LR 0.001000    Time 0.074692    
--- validate (epoch=93)-----------
5136 samples (512 per mini-batch)
Epoch: [93][   10/   11]    Loss 0.560699    Top1 82.753906    Top5 99.687500    
Epoch: [93][   11/   11]    Loss 0.653186    Top1 82.729751    Top5 99.688474    
==> Top1: 82.730    Top5: 99.688    Loss: 0.653

==> Confusion:
[[270   6   1   1   0   8   2  12]
 [  4 247  32   0   1   6   0  10]
 [  3  18 259   1   0   9   0  10]
 [  0   3   1 768  37  14   5   9]
 [  1   0   0  89 739   9  21  20]
 [  4   4   4  18  21 811   7  25]
 [  1   0   0   8  37  19 749  23]
 [ 29   9  14  89  62 159  21 406]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [94][   10/  207]    Overall Loss 0.075686    Objective Loss 0.075686                                        LR 0.001000    Time 0.130364    
Epoch: [94][   20/  207]    Overall Loss 0.074104    Objective Loss 0.074104                                        LR 0.001000    Time 0.100745    
Epoch: [94][   30/  207]    Overall Loss 0.075242    Objective Loss 0.075242                                        LR 0.001000    Time 0.090953    
Epoch: [94][   40/  207]    Overall Loss 0.075815    Objective Loss 0.075815                                        LR 0.001000    Time 0.086044    
Epoch: [94][   50/  207]    Overall Loss 0.075863    Objective Loss 0.075863                                        LR 0.001000    Time 0.083082    
Epoch: [94][   60/  207]    Overall Loss 0.081324    Objective Loss 0.081324                                        LR 0.001000    Time 0.081161    
Epoch: [94][   70/  207]    Overall Loss 0.084024    Objective Loss 0.084024                                        LR 0.001000    Time 0.080076    
Epoch: [94][   80/  207]    Overall Loss 0.085136    Objective Loss 0.085136                                        LR 0.001000    Time 0.079116    
Epoch: [94][   90/  207]    Overall Loss 0.084979    Objective Loss 0.084979                                        LR 0.001000    Time 0.078260    
Epoch: [94][  100/  207]    Overall Loss 0.084677    Objective Loss 0.084677                                        LR 0.001000    Time 0.077882    
Epoch: [94][  110/  207]    Overall Loss 0.084000    Objective Loss 0.084000                                        LR 0.001000    Time 0.077462    
Epoch: [94][  120/  207]    Overall Loss 0.085050    Objective Loss 0.085050                                        LR 0.001000    Time 0.076942    
Epoch: [94][  130/  207]    Overall Loss 0.085470    Objective Loss 0.085470                                        LR 0.001000    Time 0.076446    
Epoch: [94][  140/  207]    Overall Loss 0.085350    Objective Loss 0.085350                                        LR 0.001000    Time 0.076190    
Epoch: [94][  150/  207]    Overall Loss 0.084679    Objective Loss 0.084679                                        LR 0.001000    Time 0.075995    
Epoch: [94][  160/  207]    Overall Loss 0.084056    Objective Loss 0.084056                                        LR 0.001000    Time 0.075697    
Epoch: [94][  170/  207]    Overall Loss 0.083250    Objective Loss 0.083250                                        LR 0.001000    Time 0.075440    
Epoch: [94][  180/  207]    Overall Loss 0.082353    Objective Loss 0.082353                                        LR 0.001000    Time 0.075270    
Epoch: [94][  190/  207]    Overall Loss 0.082570    Objective Loss 0.082570                                        LR 0.001000    Time 0.075141    
Epoch: [94][  200/  207]    Overall Loss 0.082638    Objective Loss 0.082638                                        LR 0.001000    Time 0.074986    
Epoch: [94][  207/  207]    Overall Loss 0.084057    Objective Loss 0.084057    Top1 90.713477    Top5 100.000000    LR 0.001000    Time 0.074739    
--- validate (epoch=94)-----------
5136 samples (512 per mini-batch)
Epoch: [94][   10/   11]    Loss 0.484652    Top1 82.988281    Top5 99.667969    
Epoch: [94][   11/   11]    Loss 0.480254    Top1 82.943925    Top5 99.669003    
==> Top1: 82.944    Top5: 99.669    Loss: 0.480

==> Confusion:
[[277   4   2   2   0   3   3   9]
 [  3 260  27   2   0   2   1   5]
 [  3  20 266   0   0   7   0   4]
 [  1   2   0 758  43  19   5   9]
 [  4   0   0  45 771  12  27  20]
 [ 13   6   7   9  18 812  12  17]
 [  4   0   0   6  30  20 763  14]
 [ 48  16  16  49  68 199  40 353]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [95][   10/  207]    Overall Loss 0.119043    Objective Loss 0.119043                                        LR 0.001000    Time 0.130418    
Epoch: [95][   20/  207]    Overall Loss 0.116461    Objective Loss 0.116461                                        LR 0.001000    Time 0.100318    
Epoch: [95][   30/  207]    Overall Loss 0.109752    Objective Loss 0.109752                                        LR 0.001000    Time 0.090286    
Epoch: [95][   40/  207]    Overall Loss 0.104227    Objective Loss 0.104227                                        LR 0.001000    Time 0.085212    
Epoch: [95][   50/  207]    Overall Loss 0.100926    Objective Loss 0.100926                                        LR 0.001000    Time 0.082530    
Epoch: [95][   60/  207]    Overall Loss 0.095599    Objective Loss 0.095599                                        LR 0.001000    Time 0.080637    
Epoch: [95][   70/  207]    Overall Loss 0.091718    Objective Loss 0.091718                                        LR 0.001000    Time 0.079313    
Epoch: [95][   80/  207]    Overall Loss 0.088617    Objective Loss 0.088617                                        LR 0.001000    Time 0.078606    
Epoch: [95][   90/  207]    Overall Loss 0.087090    Objective Loss 0.087090                                        LR 0.001000    Time 0.077896    
Epoch: [95][  100/  207]    Overall Loss 0.084060    Objective Loss 0.084060                                        LR 0.001000    Time 0.077262    
Epoch: [95][  110/  207]    Overall Loss 0.083010    Objective Loss 0.083010                                        LR 0.001000    Time 0.076782    
Epoch: [95][  120/  207]    Overall Loss 0.084855    Objective Loss 0.084855                                        LR 0.001000    Time 0.076308    
Epoch: [95][  130/  207]    Overall Loss 0.084213    Objective Loss 0.084213                                        LR 0.001000    Time 0.075891    
Epoch: [95][  140/  207]    Overall Loss 0.085263    Objective Loss 0.085263                                        LR 0.001000    Time 0.075620    
Epoch: [95][  150/  207]    Overall Loss 0.086302    Objective Loss 0.086302                                        LR 0.001000    Time 0.075359    
Epoch: [95][  160/  207]    Overall Loss 0.086005    Objective Loss 0.086005                                        LR 0.001000    Time 0.075252    
Epoch: [95][  170/  207]    Overall Loss 0.084935    Objective Loss 0.084935                                        LR 0.001000    Time 0.075074    
Epoch: [95][  180/  207]    Overall Loss 0.084760    Objective Loss 0.084760                                        LR 0.001000    Time 0.074812    
Epoch: [95][  190/  207]    Overall Loss 0.083912    Objective Loss 0.083912                                        LR 0.001000    Time 0.074633    
Epoch: [95][  200/  207]    Overall Loss 0.082782    Objective Loss 0.082782                                        LR 0.001000    Time 0.074487    
Epoch: [95][  207/  207]    Overall Loss 0.082921    Objective Loss 0.082921    Top1 96.715742    Top5 100.000000    LR 0.001000    Time 0.074257    
--- validate (epoch=95)-----------
5136 samples (512 per mini-batch)
Epoch: [95][   10/   11]    Loss 0.609817    Top1 83.652344    Top5 99.726562    
Epoch: [95][   11/   11]    Loss 0.632356    Top1 83.664330    Top5 99.727414    
==> Top1: 83.664    Top5: 99.727    Loss: 0.632

==> Confusion:
[[266   3   2   0   1   2   3  23]
 [  1 267  13   1   0   1   0  17]
 [  3  27 245   1   0   3   0  21]
 [  0   0   1 763  42   5   9  17]
 [  2   0   0  58 772   6  11  30]
 [  7   5   9  20  24 744  16  69]
 [  1   0   0   9  34   8 737  48]
 [ 24  14  13  57  71  89  18 503]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [96][   10/  207]    Overall Loss 0.061974    Objective Loss 0.061974                                        LR 0.001000    Time 0.128601    
Epoch: [96][   20/  207]    Overall Loss 0.062342    Objective Loss 0.062342                                        LR 0.001000    Time 0.099969    
Epoch: [96][   30/  207]    Overall Loss 0.062816    Objective Loss 0.062816                                        LR 0.001000    Time 0.091293    
Epoch: [96][   40/  207]    Overall Loss 0.058671    Objective Loss 0.058671                                        LR 0.001000    Time 0.086819    
Epoch: [96][   50/  207]    Overall Loss 0.057933    Objective Loss 0.057933                                        LR 0.001000    Time 0.083920    
Epoch: [96][   60/  207]    Overall Loss 0.058117    Objective Loss 0.058117                                        LR 0.001000    Time 0.081996    
Epoch: [96][   70/  207]    Overall Loss 0.055827    Objective Loss 0.055827                                        LR 0.001000    Time 0.080795    
Epoch: [96][   80/  207]    Overall Loss 0.054539    Objective Loss 0.054539                                        LR 0.001000    Time 0.079852    
Epoch: [96][   90/  207]    Overall Loss 0.053777    Objective Loss 0.053777                                        LR 0.001000    Time 0.079031    
Epoch: [96][  100/  207]    Overall Loss 0.053371    Objective Loss 0.053371                                        LR 0.001000    Time 0.078426    
Epoch: [96][  110/  207]    Overall Loss 0.053496    Objective Loss 0.053496                                        LR 0.001000    Time 0.077986    
Epoch: [96][  120/  207]    Overall Loss 0.054617    Objective Loss 0.054617                                        LR 0.001000    Time 0.077621    
Epoch: [96][  130/  207]    Overall Loss 0.055147    Objective Loss 0.055147                                        LR 0.001000    Time 0.077268    
Epoch: [96][  140/  207]    Overall Loss 0.056871    Objective Loss 0.056871                                        LR 0.001000    Time 0.077011    
Epoch: [96][  150/  207]    Overall Loss 0.057725    Objective Loss 0.057725                                        LR 0.001000    Time 0.076851    
Epoch: [96][  160/  207]    Overall Loss 0.060428    Objective Loss 0.060428                                        LR 0.001000    Time 0.076520    
Epoch: [96][  170/  207]    Overall Loss 0.063128    Objective Loss 0.063128                                        LR 0.001000    Time 0.076331    
Epoch: [96][  180/  207]    Overall Loss 0.066103    Objective Loss 0.066103                                        LR 0.001000    Time 0.076053    
Epoch: [96][  190/  207]    Overall Loss 0.067937    Objective Loss 0.067937                                        LR 0.001000    Time 0.075782    
Epoch: [96][  200/  207]    Overall Loss 0.069733    Objective Loss 0.069733                                        LR 0.001000    Time 0.075546    
Epoch: [96][  207/  207]    Overall Loss 0.070351    Objective Loss 0.070351    Top1 94.677237    Top5 99.886750    LR 0.001000    Time 0.075273    
--- validate (epoch=96)-----------
5136 samples (512 per mini-batch)
Epoch: [96][   10/   11]    Loss 0.498607    Top1 82.460938    Top5 99.687500    
Epoch: [96][   11/   11]    Loss 0.462433    Top1 82.476636    Top5 99.688474    
==> Top1: 82.477    Top5: 99.688    Loss: 0.462

==> Confusion:
[[275   3   1   4   0   2   1  14]
 [  2 265  20   4   0   3   0   6]
 [  3  20 274   1   0   0   0   2]
 [  0   2   1 742  64  12   8   8]
 [  4   0   0  43 793   4  25  10]
 [ 16   7  14  18  25 773  23  18]
 [  1   0   0  10  34   7 771  14]
 [ 37  12  22  79  90 154  52 343]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [97][   10/  207]    Overall Loss 0.066662    Objective Loss 0.066662                                        LR 0.001000    Time 0.127763    
Epoch: [97][   20/  207]    Overall Loss 0.070464    Objective Loss 0.070464                                        LR 0.001000    Time 0.099571    
Epoch: [97][   30/  207]    Overall Loss 0.071935    Objective Loss 0.071935                                        LR 0.001000    Time 0.090357    
Epoch: [97][   40/  207]    Overall Loss 0.068995    Objective Loss 0.068995                                        LR 0.001000    Time 0.085489    
Epoch: [97][   50/  207]    Overall Loss 0.066971    Objective Loss 0.066971                                        LR 0.001000    Time 0.082539    
Epoch: [97][   60/  207]    Overall Loss 0.066313    Objective Loss 0.066313                                        LR 0.001000    Time 0.080776    
Epoch: [97][   70/  207]    Overall Loss 0.066413    Objective Loss 0.066413                                        LR 0.001000    Time 0.079412    
Epoch: [97][   80/  207]    Overall Loss 0.064875    Objective Loss 0.064875                                        LR 0.001000    Time 0.078253    
Epoch: [97][   90/  207]    Overall Loss 0.063678    Objective Loss 0.063678                                        LR 0.001000    Time 0.077394    
Epoch: [97][  100/  207]    Overall Loss 0.062556    Objective Loss 0.062556                                        LR 0.001000    Time 0.076761    
Epoch: [97][  110/  207]    Overall Loss 0.062836    Objective Loss 0.062836                                        LR 0.001000    Time 0.076312    
Epoch: [97][  120/  207]    Overall Loss 0.063033    Objective Loss 0.063033                                        LR 0.001000    Time 0.075924    
Epoch: [97][  130/  207]    Overall Loss 0.063124    Objective Loss 0.063124                                        LR 0.001000    Time 0.075586    
Epoch: [97][  140/  207]    Overall Loss 0.063213    Objective Loss 0.063213                                        LR 0.001000    Time 0.075295    
Epoch: [97][  150/  207]    Overall Loss 0.063843    Objective Loss 0.063843                                        LR 0.001000    Time 0.074940    
Epoch: [97][  160/  207]    Overall Loss 0.063952    Objective Loss 0.063952                                        LR 0.001000    Time 0.074730    
Epoch: [97][  170/  207]    Overall Loss 0.064394    Objective Loss 0.064394                                        LR 0.001000    Time 0.074531    
Epoch: [97][  180/  207]    Overall Loss 0.064251    Objective Loss 0.064251                                        LR 0.001000    Time 0.074313    
Epoch: [97][  190/  207]    Overall Loss 0.063788    Objective Loss 0.063788                                        LR 0.001000    Time 0.074215    
Epoch: [97][  200/  207]    Overall Loss 0.063709    Objective Loss 0.063709                                        LR 0.001000    Time 0.074161    
Epoch: [97][  207/  207]    Overall Loss 0.063306    Objective Loss 0.063306    Top1 96.262741    Top5 100.000000    LR 0.001000    Time 0.073937    
--- validate (epoch=97)-----------
5136 samples (512 per mini-batch)
Epoch: [97][   10/   11]    Loss 0.520322    Top1 83.496094    Top5 99.746094    
Epoch: [97][   11/   11]    Loss 0.521539    Top1 83.528037    Top5 99.746885    
==> Top1: 83.528    Top5: 99.747    Loss: 0.522

==> Confusion:
[[284   3   1   0   0   3   0   9]
 [  5 245  42   1   0   3   1   3]
 [  6  12 275   0   0   5   0   2]
 [  0   1   0 750  48  19   7  12]
 [  3   0   0  49 771  13  19  24]
 [ 13   5   8   5  14 820   9  20]
 [  2   0   0   6  37  23 744  25]
 [ 42   9  17  51  74 176  19 401]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [98][   10/  207]    Overall Loss 0.058048    Objective Loss 0.058048                                        LR 0.001000    Time 0.130197    
Epoch: [98][   20/  207]    Overall Loss 0.055281    Objective Loss 0.055281                                        LR 0.001000    Time 0.101790    
Epoch: [98][   30/  207]    Overall Loss 0.052599    Objective Loss 0.052599                                        LR 0.001000    Time 0.091400    
Epoch: [98][   40/  207]    Overall Loss 0.050213    Objective Loss 0.050213                                        LR 0.001000    Time 0.086522    
Epoch: [98][   50/  207]    Overall Loss 0.050086    Objective Loss 0.050086                                        LR 0.001000    Time 0.083486    
Epoch: [98][   60/  207]    Overall Loss 0.050305    Objective Loss 0.050305                                        LR 0.001000    Time 0.081182    
Epoch: [98][   70/  207]    Overall Loss 0.050966    Objective Loss 0.050966                                        LR 0.001000    Time 0.079747    
Epoch: [98][   80/  207]    Overall Loss 0.050390    Objective Loss 0.050390                                        LR 0.001000    Time 0.078614    
Epoch: [98][   90/  207]    Overall Loss 0.051401    Objective Loss 0.051401                                        LR 0.001000    Time 0.077940    
Epoch: [98][  100/  207]    Overall Loss 0.052283    Objective Loss 0.052283                                        LR 0.001000    Time 0.077429    
Epoch: [98][  110/  207]    Overall Loss 0.052826    Objective Loss 0.052826                                        LR 0.001000    Time 0.076861    
Epoch: [98][  120/  207]    Overall Loss 0.052409    Objective Loss 0.052409                                        LR 0.001000    Time 0.076394    
Epoch: [98][  130/  207]    Overall Loss 0.052262    Objective Loss 0.052262                                        LR 0.001000    Time 0.076121    
Epoch: [98][  140/  207]    Overall Loss 0.052255    Objective Loss 0.052255                                        LR 0.001000    Time 0.075875    
Epoch: [98][  150/  207]    Overall Loss 0.052962    Objective Loss 0.052962                                        LR 0.001000    Time 0.075652    
Epoch: [98][  160/  207]    Overall Loss 0.053095    Objective Loss 0.053095                                        LR 0.001000    Time 0.075304    
Epoch: [98][  170/  207]    Overall Loss 0.053532    Objective Loss 0.053532                                        LR 0.001000    Time 0.075123    
Epoch: [98][  180/  207]    Overall Loss 0.054520    Objective Loss 0.054520                                        LR 0.001000    Time 0.074916    
Epoch: [98][  190/  207]    Overall Loss 0.055215    Objective Loss 0.055215                                        LR 0.001000    Time 0.074941    
Epoch: [98][  200/  207]    Overall Loss 0.056096    Objective Loss 0.056096                                        LR 0.001000    Time 0.074884    
Epoch: [98][  207/  207]    Overall Loss 0.057190    Objective Loss 0.057190    Top1 93.997735    Top5 100.000000    LR 0.001000    Time 0.074659    
--- validate (epoch=98)-----------
5136 samples (512 per mini-batch)
Epoch: [98][   10/   11]    Loss 0.594547    Top1 82.734375    Top5 99.667969    
Epoch: [98][   11/   11]    Loss 0.685644    Top1 82.749221    Top5 99.669003    
==> Top1: 82.749    Top5: 99.669    Loss: 0.686

==> Confusion:
[[261   4   2   3   2   1   1  26]
 [  1 255  33   1   1   3   1   5]
 [  3  19 265   2   1   3   0   7]
 [  0   2   0 767  48   5   9   6]
 [  1   0   0  62 776   1  18  21]
 [  7   7  10  32  34 725  26  53]
 [  1   0   0  10  40   2 761  23]
 [ 22  16  13  84  84  91  39 440]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [99][   10/  207]    Overall Loss 0.101018    Objective Loss 0.101018                                        LR 0.001000    Time 0.113211    
Epoch: [99][   20/  207]    Overall Loss 0.106461    Objective Loss 0.106461                                        LR 0.001000    Time 0.091595    
Epoch: [99][   30/  207]    Overall Loss 0.109311    Objective Loss 0.109311                                        LR 0.001000    Time 0.084767    
Epoch: [99][   40/  207]    Overall Loss 0.111621    Objective Loss 0.111621                                        LR 0.001000    Time 0.081211    
Epoch: [99][   50/  207]    Overall Loss 0.111660    Objective Loss 0.111660                                        LR 0.001000    Time 0.078963    
Epoch: [99][   60/  207]    Overall Loss 0.108191    Objective Loss 0.108191                                        LR 0.001000    Time 0.077738    
Epoch: [99][   70/  207]    Overall Loss 0.104630    Objective Loss 0.104630                                        LR 0.001000    Time 0.076903    
Epoch: [99][   80/  207]    Overall Loss 0.100655    Objective Loss 0.100655                                        LR 0.001000    Time 0.076427    
Epoch: [99][   90/  207]    Overall Loss 0.097585    Objective Loss 0.097585                                        LR 0.001000    Time 0.075788    
Epoch: [99][  100/  207]    Overall Loss 0.094940    Objective Loss 0.094940                                        LR 0.001000    Time 0.075212    
Epoch: [99][  110/  207]    Overall Loss 0.091368    Objective Loss 0.091368                                        LR 0.001000    Time 0.074831    
Epoch: [99][  120/  207]    Overall Loss 0.089555    Objective Loss 0.089555                                        LR 0.001000    Time 0.074506    
Epoch: [99][  130/  207]    Overall Loss 0.087768    Objective Loss 0.087768                                        LR 0.001000    Time 0.074327    
Epoch: [99][  140/  207]    Overall Loss 0.087351    Objective Loss 0.087351                                        LR 0.001000    Time 0.074095    
Epoch: [99][  150/  207]    Overall Loss 0.085719    Objective Loss 0.085719                                        LR 0.001000    Time 0.073982    
Epoch: [99][  160/  207]    Overall Loss 0.084077    Objective Loss 0.084077                                        LR 0.001000    Time 0.073897    
Epoch: [99][  170/  207]    Overall Loss 0.082516    Objective Loss 0.082516                                        LR 0.001000    Time 0.073711    
Epoch: [99][  180/  207]    Overall Loss 0.081011    Objective Loss 0.081011                                        LR 0.001000    Time 0.073678    
Epoch: [99][  190/  207]    Overall Loss 0.079869    Objective Loss 0.079869                                        LR 0.001000    Time 0.073484    
Epoch: [99][  200/  207]    Overall Loss 0.078449    Objective Loss 0.078449                                        LR 0.001000    Time 0.073389    
Epoch: [99][  207/  207]    Overall Loss 0.077478    Objective Loss 0.077478    Top1 95.809740    Top5 100.000000    LR 0.001000    Time 0.073194    
--- validate (epoch=99)-----------
5136 samples (512 per mini-batch)
Epoch: [99][   10/   11]    Loss 0.596160    Top1 84.101562    Top5 99.667969    
Epoch: [99][   11/   11]    Loss 0.547418    Top1 84.131620    Top5 99.669003    
==> Top1: 84.132    Top5: 99.669    Loss: 0.547

==> Confusion:
[[255   3   3   1   2   3   3  30]
 [  2 251  25   4   0   2   1  15]
 [  2  14 270   1   0   4   0   9]
 [  1   1   1 742  56   9   9  18]
 [  0   0   0  42 770   6  31  30]
 [  2   3   8  12  20 780  31  38]
 [  1   0   0   5  20   6 780  25]
 [ 17  13  15  45  69 115  42 473]]

==> Best [Top1: 84.443   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 88]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [100][   10/  207]    Overall Loss 0.043390    Objective Loss 0.043390                                        LR 0.000500    Time 0.130902    
Epoch: [100][   20/  207]    Overall Loss 0.041317    Objective Loss 0.041317                                        LR 0.000500    Time 0.101234    
Epoch: [100][   30/  207]    Overall Loss 0.039892    Objective Loss 0.039892                                        LR 0.000500    Time 0.090681    
Epoch: [100][   40/  207]    Overall Loss 0.037778    Objective Loss 0.037778                                        LR 0.000500    Time 0.086371    
Epoch: [100][   50/  207]    Overall Loss 0.037224    Objective Loss 0.037224                                        LR 0.000500    Time 0.083342    
Epoch: [100][   60/  207]    Overall Loss 0.036486    Objective Loss 0.036486                                        LR 0.000500    Time 0.081527    
Epoch: [100][   70/  207]    Overall Loss 0.035556    Objective Loss 0.035556                                        LR 0.000500    Time 0.080193    
Epoch: [100][   80/  207]    Overall Loss 0.035013    Objective Loss 0.035013                                        LR 0.000500    Time 0.079118    
Epoch: [100][   90/  207]    Overall Loss 0.034413    Objective Loss 0.034413                                        LR 0.000500    Time 0.078399    
Epoch: [100][  100/  207]    Overall Loss 0.034438    Objective Loss 0.034438                                        LR 0.000500    Time 0.077657    
Epoch: [100][  110/  207]    Overall Loss 0.033981    Objective Loss 0.033981                                        LR 0.000500    Time 0.077005    
Epoch: [100][  120/  207]    Overall Loss 0.034055    Objective Loss 0.034055                                        LR 0.000500    Time 0.076689    
Epoch: [100][  130/  207]    Overall Loss 0.034299    Objective Loss 0.034299                                        LR 0.000500    Time 0.076248    
Epoch: [100][  140/  207]    Overall Loss 0.034110    Objective Loss 0.034110                                        LR 0.000500    Time 0.075954    
Epoch: [100][  150/  207]    Overall Loss 0.034254    Objective Loss 0.034254                                        LR 0.000500    Time 0.075796    
Epoch: [100][  160/  207]    Overall Loss 0.034229    Objective Loss 0.034229                                        LR 0.000500    Time 0.075452    
Epoch: [100][  170/  207]    Overall Loss 0.034224    Objective Loss 0.034224                                        LR 0.000500    Time 0.075172    
Epoch: [100][  180/  207]    Overall Loss 0.033962    Objective Loss 0.033962                                        LR 0.000500    Time 0.075015    
Epoch: [100][  190/  207]    Overall Loss 0.033950    Objective Loss 0.033950                                        LR 0.000500    Time 0.074827    
Epoch: [100][  200/  207]    Overall Loss 0.033616    Objective Loss 0.033616                                        LR 0.000500    Time 0.074632    
Epoch: [100][  207/  207]    Overall Loss 0.033346    Objective Loss 0.033346    Top1 97.961495    Top5 100.000000    LR 0.000500    Time 0.074399    
--- validate (epoch=100)-----------
5136 samples (512 per mini-batch)
Epoch: [100][   10/   11]    Loss 0.541453    Top1 85.175781    Top5 99.765625    
Epoch: [100][   11/   11]    Loss 0.581887    Top1 85.124611    Top5 99.766355    
==> Top1: 85.125    Top5: 99.766    Loss: 0.582

==> Confusion:
[[262   2   2   1   1   1   3  28]
 [  1 256  24   1   0   2   1  15]
 [  3  14 268   1   0   2   0  12]
 [  0   0   0 741  57  14   8  17]
 [  0   0   0  37 783   7  21  31]
 [  2   5   7   8  18 790  18  46]
 [  1   0   0   7  28   5 765  31]
 [ 15  11  12  46  70  97  31 507]]

==> Best [Top1: 85.125   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 100]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [101][   10/  207]    Overall Loss 0.026714    Objective Loss 0.026714                                        LR 0.000500    Time 0.127025    
Epoch: [101][   20/  207]    Overall Loss 0.025465    Objective Loss 0.025465                                        LR 0.000500    Time 0.099695    
Epoch: [101][   30/  207]    Overall Loss 0.026392    Objective Loss 0.026392                                        LR 0.000500    Time 0.090935    
Epoch: [101][   40/  207]    Overall Loss 0.026243    Objective Loss 0.026243                                        LR 0.000500    Time 0.085796    
Epoch: [101][   50/  207]    Overall Loss 0.026243    Objective Loss 0.026243                                        LR 0.000500    Time 0.083798    
Epoch: [101][   60/  207]    Overall Loss 0.026303    Objective Loss 0.026303                                        LR 0.000500    Time 0.081828    
Epoch: [101][   70/  207]    Overall Loss 0.026270    Objective Loss 0.026270                                        LR 0.000500    Time 0.080341    
Epoch: [101][   80/  207]    Overall Loss 0.025759    Objective Loss 0.025759                                        LR 0.000500    Time 0.079494    
Epoch: [101][   90/  207]    Overall Loss 0.025961    Objective Loss 0.025961                                        LR 0.000500    Time 0.078608    
Epoch: [101][  100/  207]    Overall Loss 0.026082    Objective Loss 0.026082                                        LR 0.000500    Time 0.077934    
Epoch: [101][  110/  207]    Overall Loss 0.026069    Objective Loss 0.026069                                        LR 0.000500    Time 0.077583    
Epoch: [101][  120/  207]    Overall Loss 0.026193    Objective Loss 0.026193                                        LR 0.000500    Time 0.077037    
Epoch: [101][  130/  207]    Overall Loss 0.026481    Objective Loss 0.026481                                        LR 0.000500    Time 0.076656    
Epoch: [101][  140/  207]    Overall Loss 0.026215    Objective Loss 0.026215                                        LR 0.000500    Time 0.076188    
Epoch: [101][  150/  207]    Overall Loss 0.026151    Objective Loss 0.026151                                        LR 0.000500    Time 0.075835    
Epoch: [101][  160/  207]    Overall Loss 0.026331    Objective Loss 0.026331                                        LR 0.000500    Time 0.075560    
Epoch: [101][  170/  207]    Overall Loss 0.026342    Objective Loss 0.026342                                        LR 0.000500    Time 0.075347    
Epoch: [101][  180/  207]    Overall Loss 0.026467    Objective Loss 0.026467                                        LR 0.000500    Time 0.075180    
Epoch: [101][  190/  207]    Overall Loss 0.026531    Objective Loss 0.026531                                        LR 0.000500    Time 0.075121    
Epoch: [101][  200/  207]    Overall Loss 0.026270    Objective Loss 0.026270                                        LR 0.000500    Time 0.074993    
Epoch: [101][  207/  207]    Overall Loss 0.026327    Objective Loss 0.026327    Top1 97.055493    Top5 99.886750    LR 0.000500    Time 0.074760    
--- validate (epoch=101)-----------
5136 samples (512 per mini-batch)
Epoch: [101][   10/   11]    Loss 0.495839    Top1 85.156250    Top5 99.726562    
Epoch: [101][   11/   11]    Loss 0.459261    Top1 85.183022    Top5 99.727414    
==> Top1: 85.183    Top5: 99.727    Loss: 0.459

==> Confusion:
[[273   2   2   1   2   1   3  16]
 [  1 261  21   1   0   4   0  12]
 [  3  16 268   2   0   3   0   8]
 [  0   0   1 758  49  12   6  11]
 [  1   0   0  43 783   8  24  20]
 [  2   5   7  14  20 793  14  39]
 [  1   0   0   8  29   9 765  25]
 [ 25  13  10  54  69 117  27 474]]

==> Best [Top1: 85.183   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 101]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [102][   10/  207]    Overall Loss 0.022337    Objective Loss 0.022337                                        LR 0.000500    Time 0.126953    
Epoch: [102][   20/  207]    Overall Loss 0.021757    Objective Loss 0.021757                                        LR 0.000500    Time 0.098877    
Epoch: [102][   30/  207]    Overall Loss 0.021673    Objective Loss 0.021673                                        LR 0.000500    Time 0.094164    
Epoch: [102][   40/  207]    Overall Loss 0.023463    Objective Loss 0.023463                                        LR 0.000500    Time 0.088182    
Epoch: [102][   50/  207]    Overall Loss 0.023686    Objective Loss 0.023686                                        LR 0.000500    Time 0.084828    
Epoch: [102][   60/  207]    Overall Loss 0.023258    Objective Loss 0.023258                                        LR 0.000500    Time 0.082452    
Epoch: [102][   70/  207]    Overall Loss 0.022952    Objective Loss 0.022952                                        LR 0.000500    Time 0.081214    
Epoch: [102][   80/  207]    Overall Loss 0.023603    Objective Loss 0.023603                                        LR 0.000500    Time 0.080059    
Epoch: [102][   90/  207]    Overall Loss 0.023715    Objective Loss 0.023715                                        LR 0.000500    Time 0.079165    
Epoch: [102][  100/  207]    Overall Loss 0.023819    Objective Loss 0.023819                                        LR 0.000500    Time 0.078523    
Epoch: [102][  110/  207]    Overall Loss 0.023985    Objective Loss 0.023985                                        LR 0.000500    Time 0.077836    
Epoch: [102][  120/  207]    Overall Loss 0.024171    Objective Loss 0.024171                                        LR 0.000500    Time 0.077268    
Epoch: [102][  130/  207]    Overall Loss 0.024069    Objective Loss 0.024069                                        LR 0.000500    Time 0.076937    
Epoch: [102][  140/  207]    Overall Loss 0.024137    Objective Loss 0.024137                                        LR 0.000500    Time 0.076712    
Epoch: [102][  150/  207]    Overall Loss 0.024103    Objective Loss 0.024103                                        LR 0.000500    Time 0.076367    
Epoch: [102][  160/  207]    Overall Loss 0.024010    Objective Loss 0.024010                                        LR 0.000500    Time 0.076129    
Epoch: [102][  170/  207]    Overall Loss 0.024035    Objective Loss 0.024035                                        LR 0.000500    Time 0.075989    
Epoch: [102][  180/  207]    Overall Loss 0.024143    Objective Loss 0.024143                                        LR 0.000500    Time 0.075888    
Epoch: [102][  190/  207]    Overall Loss 0.024266    Objective Loss 0.024266                                        LR 0.000500    Time 0.075730    
Epoch: [102][  200/  207]    Overall Loss 0.024491    Objective Loss 0.024491                                        LR 0.000500    Time 0.075537    
Epoch: [102][  207/  207]    Overall Loss 0.024559    Objective Loss 0.024559    Top1 98.301246    Top5 100.000000    LR 0.000500    Time 0.075279    
--- validate (epoch=102)-----------
5136 samples (512 per mini-batch)
Epoch: [102][   10/   11]    Loss 0.525535    Top1 85.429688    Top5 99.746094    
Epoch: [102][   11/   11]    Loss 0.581840    Top1 85.377726    Top5 99.746885    
==> Top1: 85.378    Top5: 99.747    Loss: 0.582

==> Confusion:
[[274   2   2   1   1   0   3  17]
 [  1 266  16   1   0   1   1  14]
 [  4  15 266   1   0   0   0  14]
 [  0   1   0 752  49  10   7  18]
 [  2   0   0  46 773   4  25  29]
 [  4   6   7  13  21 768  17  58]
 [  1   0   0   8  23   3 774  28]
 [ 24  13  10  51  63  87  29 512]]

==> Best [Top1: 85.378   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 102]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [103][   10/  207]    Overall Loss 0.021974    Objective Loss 0.021974                                        LR 0.000500    Time 0.112318    
Epoch: [103][   20/  207]    Overall Loss 0.020656    Objective Loss 0.020656                                        LR 0.000500    Time 0.092027    
Epoch: [103][   30/  207]    Overall Loss 0.020410    Objective Loss 0.020410                                        LR 0.000500    Time 0.084641    
Epoch: [103][   40/  207]    Overall Loss 0.021461    Objective Loss 0.021461                                        LR 0.000500    Time 0.081593    
Epoch: [103][   50/  207]    Overall Loss 0.022168    Objective Loss 0.022168                                        LR 0.000500    Time 0.079756    
Epoch: [103][   60/  207]    Overall Loss 0.022097    Objective Loss 0.022097                                        LR 0.000500    Time 0.078369    
Epoch: [103][   70/  207]    Overall Loss 0.021870    Objective Loss 0.021870                                        LR 0.000500    Time 0.077348    
Epoch: [103][   80/  207]    Overall Loss 0.022557    Objective Loss 0.022557                                        LR 0.000500    Time 0.076687    
Epoch: [103][   90/  207]    Overall Loss 0.022674    Objective Loss 0.022674                                        LR 0.000500    Time 0.076014    
Epoch: [103][  100/  207]    Overall Loss 0.022882    Objective Loss 0.022882                                        LR 0.000500    Time 0.075588    
Epoch: [103][  110/  207]    Overall Loss 0.022779    Objective Loss 0.022779                                        LR 0.000500    Time 0.075183    
Epoch: [103][  120/  207]    Overall Loss 0.022799    Objective Loss 0.022799                                        LR 0.000500    Time 0.074977    
Epoch: [103][  130/  207]    Overall Loss 0.022942    Objective Loss 0.022942                                        LR 0.000500    Time 0.074746    
Epoch: [103][  140/  207]    Overall Loss 0.022861    Objective Loss 0.022861                                        LR 0.000500    Time 0.074602    
Epoch: [103][  150/  207]    Overall Loss 0.022784    Objective Loss 0.022784                                        LR 0.000500    Time 0.074423    
Epoch: [103][  160/  207]    Overall Loss 0.022758    Objective Loss 0.022758                                        LR 0.000500    Time 0.074312    
Epoch: [103][  170/  207]    Overall Loss 0.022936    Objective Loss 0.022936                                        LR 0.000500    Time 0.074362    
Epoch: [103][  180/  207]    Overall Loss 0.022964    Objective Loss 0.022964                                        LR 0.000500    Time 0.074266    
Epoch: [103][  190/  207]    Overall Loss 0.023196    Objective Loss 0.023196                                        LR 0.000500    Time 0.074168    
Epoch: [103][  200/  207]    Overall Loss 0.023121    Objective Loss 0.023121                                        LR 0.000500    Time 0.074036    
Epoch: [103][  207/  207]    Overall Loss 0.022995    Objective Loss 0.022995    Top1 98.754247    Top5 100.000000    LR 0.000500    Time 0.073813    
--- validate (epoch=103)-----------
5136 samples (512 per mini-batch)
Epoch: [103][   10/   11]    Loss 0.534352    Top1 85.371094    Top5 99.765625    
Epoch: [103][   11/   11]    Loss 0.510826    Top1 85.338785    Top5 99.766355    
==> Top1: 85.339    Top5: 99.766    Loss: 0.511

==> Confusion:
[[269   3   2   1   1   1   2  21]
 [  1 266  17   1   0   1   1  13]
 [  4  17 269   1   0   2   0   7]
 [  0   1   0 747  49  13   7  20]
 [  1   0   0  36 784   5  18  35]
 [  2   6   8   9  21 786  11  51]
 [  1   0   0   6  32   9 753  36]
 [ 21  11  13  45  66 101  23 509]]

==> Best [Top1: 85.378   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 102]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [104][   10/  207]    Overall Loss 0.022887    Objective Loss 0.022887                                        LR 0.000500    Time 0.130007    
Epoch: [104][   20/  207]    Overall Loss 0.023310    Objective Loss 0.023310                                        LR 0.000500    Time 0.100748    
Epoch: [104][   30/  207]    Overall Loss 0.022806    Objective Loss 0.022806                                        LR 0.000500    Time 0.091102    
Epoch: [104][   40/  207]    Overall Loss 0.021943    Objective Loss 0.021943                                        LR 0.000500    Time 0.086323    
Epoch: [104][   50/  207]    Overall Loss 0.021677    Objective Loss 0.021677                                        LR 0.000500    Time 0.082977    
Epoch: [104][   60/  207]    Overall Loss 0.021489    Objective Loss 0.021489                                        LR 0.000500    Time 0.080937    
Epoch: [104][   70/  207]    Overall Loss 0.021346    Objective Loss 0.021346                                        LR 0.000500    Time 0.079410    
Epoch: [104][   80/  207]    Overall Loss 0.021816    Objective Loss 0.021816                                        LR 0.000500    Time 0.078286    
Epoch: [104][   90/  207]    Overall Loss 0.021797    Objective Loss 0.021797                                        LR 0.000500    Time 0.077557    
Epoch: [104][  100/  207]    Overall Loss 0.021744    Objective Loss 0.021744                                        LR 0.000500    Time 0.077069    
Epoch: [104][  110/  207]    Overall Loss 0.021822    Objective Loss 0.021822                                        LR 0.000500    Time 0.076435    
Epoch: [104][  120/  207]    Overall Loss 0.022168    Objective Loss 0.022168                                        LR 0.000500    Time 0.076064    
Epoch: [104][  130/  207]    Overall Loss 0.021928    Objective Loss 0.021928                                        LR 0.000500    Time 0.075699    
Epoch: [104][  140/  207]    Overall Loss 0.021979    Objective Loss 0.021979                                        LR 0.000500    Time 0.075303    
Epoch: [104][  150/  207]    Overall Loss 0.021952    Objective Loss 0.021952                                        LR 0.000500    Time 0.074966    
Epoch: [104][  160/  207]    Overall Loss 0.022054    Objective Loss 0.022054                                        LR 0.000500    Time 0.074706    
Epoch: [104][  170/  207]    Overall Loss 0.022051    Objective Loss 0.022051                                        LR 0.000500    Time 0.074522    
Epoch: [104][  180/  207]    Overall Loss 0.021923    Objective Loss 0.021923                                        LR 0.000500    Time 0.074317    
Epoch: [104][  190/  207]    Overall Loss 0.021911    Objective Loss 0.021911                                        LR 0.000500    Time 0.074138    
Epoch: [104][  200/  207]    Overall Loss 0.021953    Objective Loss 0.021953                                        LR 0.000500    Time 0.074026    
Epoch: [104][  207/  207]    Overall Loss 0.022042    Objective Loss 0.022042    Top1 97.961495    Top5 100.000000    LR 0.000500    Time 0.073834    
--- validate (epoch=104)-----------
5136 samples (512 per mini-batch)
Epoch: [104][   10/   11]    Loss 0.543653    Top1 85.625000    Top5 99.765625    
Epoch: [104][   11/   11]    Loss 0.640258    Top1 85.572430    Top5 99.766355    
==> Top1: 85.572    Top5: 99.766    Loss: 0.640

==> Confusion:
[[266   2   3   1   1   1   3  23]
 [  1 264  17   1   0   2   1  14]
 [  2  13 269   1   0   3   0  12]
 [  0   2   0 749  46  14   6  20]
 [  1   0   0  37 779   7  21  34]
 [  3   5   8   9  19 792  15  43]
 [  1   0   0   5  27   5 766  33]
 [ 16  10  12  40  64 110  27 510]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [105][   10/  207]    Overall Loss 0.021009    Objective Loss 0.021009                                        LR 0.000500    Time 0.129793    
Epoch: [105][   20/  207]    Overall Loss 0.021883    Objective Loss 0.021883                                        LR 0.000500    Time 0.099872    
Epoch: [105][   30/  207]    Overall Loss 0.021864    Objective Loss 0.021864                                        LR 0.000500    Time 0.090343    
Epoch: [105][   40/  207]    Overall Loss 0.021509    Objective Loss 0.021509                                        LR 0.000500    Time 0.085439    
Epoch: [105][   50/  207]    Overall Loss 0.021964    Objective Loss 0.021964                                        LR 0.000500    Time 0.082387    
Epoch: [105][   60/  207]    Overall Loss 0.022038    Objective Loss 0.022038                                        LR 0.000500    Time 0.080487    
Epoch: [105][   70/  207]    Overall Loss 0.022300    Objective Loss 0.022300                                        LR 0.000500    Time 0.079092    
Epoch: [105][   80/  207]    Overall Loss 0.022121    Objective Loss 0.022121                                        LR 0.000500    Time 0.078156    
Epoch: [105][   90/  207]    Overall Loss 0.021847    Objective Loss 0.021847                                        LR 0.000500    Time 0.077412    
Epoch: [105][  100/  207]    Overall Loss 0.021784    Objective Loss 0.021784                                        LR 0.000500    Time 0.076886    
Epoch: [105][  110/  207]    Overall Loss 0.021719    Objective Loss 0.021719                                        LR 0.000500    Time 0.076565    
Epoch: [105][  120/  207]    Overall Loss 0.021583    Objective Loss 0.021583                                        LR 0.000500    Time 0.076418    
Epoch: [105][  130/  207]    Overall Loss 0.021548    Objective Loss 0.021548                                        LR 0.000500    Time 0.075988    
Epoch: [105][  140/  207]    Overall Loss 0.021634    Objective Loss 0.021634                                        LR 0.000500    Time 0.075621    
Epoch: [105][  150/  207]    Overall Loss 0.021750    Objective Loss 0.021750                                        LR 0.000500    Time 0.075457    
Epoch: [105][  160/  207]    Overall Loss 0.021690    Objective Loss 0.021690                                        LR 0.000500    Time 0.075138    
Epoch: [105][  170/  207]    Overall Loss 0.021571    Objective Loss 0.021571                                        LR 0.000500    Time 0.075119    
Epoch: [105][  180/  207]    Overall Loss 0.021533    Objective Loss 0.021533                                        LR 0.000500    Time 0.074892    
Epoch: [105][  190/  207]    Overall Loss 0.021525    Objective Loss 0.021525                                        LR 0.000500    Time 0.074683    
Epoch: [105][  200/  207]    Overall Loss 0.021588    Objective Loss 0.021588                                        LR 0.000500    Time 0.074499    
Epoch: [105][  207/  207]    Overall Loss 0.021644    Objective Loss 0.021644    Top1 98.074745    Top5 100.000000    LR 0.000500    Time 0.074249    
--- validate (epoch=105)-----------
5136 samples (512 per mini-batch)
Epoch: [105][   10/   11]    Loss 0.512479    Top1 85.527344    Top5 99.765625    
Epoch: [105][   11/   11]    Loss 0.565629    Top1 85.514019    Top5 99.766355    
==> Top1: 85.514    Top5: 99.766    Loss: 0.566

==> Confusion:
[[273   3   4   0   1   0   3  16]
 [  1 266  18   1   0   2   1  11]
 [  3  14 272   1   0   2   0   8]
 [  0   2   0 753  47  11   7  17]
 [  2   0   0  40 784   6  18  29]
 [  2   6   7   8  21 794  14  42]
 [  1   0   0   5  28   6 768  29]
 [ 25  14  14  46  68 116  24 482]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [106][   10/  207]    Overall Loss 0.019447    Objective Loss 0.019447                                        LR 0.000500    Time 0.133362    
Epoch: [106][   20/  207]    Overall Loss 0.019285    Objective Loss 0.019285                                        LR 0.000500    Time 0.101848    
Epoch: [106][   30/  207]    Overall Loss 0.019677    Objective Loss 0.019677                                        LR 0.000500    Time 0.091503    
Epoch: [106][   40/  207]    Overall Loss 0.019829    Objective Loss 0.019829                                        LR 0.000500    Time 0.086275    
Epoch: [106][   50/  207]    Overall Loss 0.019565    Objective Loss 0.019565                                        LR 0.000500    Time 0.083280    
Epoch: [106][   60/  207]    Overall Loss 0.019361    Objective Loss 0.019361                                        LR 0.000500    Time 0.081790    
Epoch: [106][   70/  207]    Overall Loss 0.019489    Objective Loss 0.019489                                        LR 0.000500    Time 0.080448    
Epoch: [106][   80/  207]    Overall Loss 0.019842    Objective Loss 0.019842                                        LR 0.000500    Time 0.079215    
Epoch: [106][   90/  207]    Overall Loss 0.019971    Objective Loss 0.019971                                        LR 0.000500    Time 0.078569    
Epoch: [106][  100/  207]    Overall Loss 0.020183    Objective Loss 0.020183                                        LR 0.000500    Time 0.078010    
Epoch: [106][  110/  207]    Overall Loss 0.020273    Objective Loss 0.020273                                        LR 0.000500    Time 0.077461    
Epoch: [106][  120/  207]    Overall Loss 0.020768    Objective Loss 0.020768                                        LR 0.000500    Time 0.077185    
Epoch: [106][  130/  207]    Overall Loss 0.020820    Objective Loss 0.020820                                        LR 0.000500    Time 0.076754    
Epoch: [106][  140/  207]    Overall Loss 0.021071    Objective Loss 0.021071                                        LR 0.000500    Time 0.076518    
Epoch: [106][  150/  207]    Overall Loss 0.021231    Objective Loss 0.021231                                        LR 0.000500    Time 0.076131    
Epoch: [106][  160/  207]    Overall Loss 0.021478    Objective Loss 0.021478                                        LR 0.000500    Time 0.075973    
Epoch: [106][  170/  207]    Overall Loss 0.021551    Objective Loss 0.021551                                        LR 0.000500    Time 0.075702    
Epoch: [106][  180/  207]    Overall Loss 0.021498    Objective Loss 0.021498                                        LR 0.000500    Time 0.075564    
Epoch: [106][  190/  207]    Overall Loss 0.021491    Objective Loss 0.021491                                        LR 0.000500    Time 0.075341    
Epoch: [106][  200/  207]    Overall Loss 0.021566    Objective Loss 0.021566                                        LR 0.000500    Time 0.075168    
Epoch: [106][  207/  207]    Overall Loss 0.021711    Objective Loss 0.021711    Top1 97.508494    Top5 100.000000    LR 0.000500    Time 0.074930    
--- validate (epoch=106)-----------
5136 samples (512 per mini-batch)
Epoch: [106][   10/   11]    Loss 0.522258    Top1 84.863281    Top5 99.746094    
Epoch: [106][   11/   11]    Loss 0.487723    Top1 84.871495    Top5 99.746885    
==> Top1: 84.871    Top5: 99.747    Loss: 0.488

==> Confusion:
[[270   2   3   3   2   1   2  17]
 [  1 266  18   1   0   2   0  12]
 [  3  16 272   1   0   2   0   6]
 [  0   1   1 764  41  13   5  12]
 [  2   0   0  71 758   6  17  25]
 [  2   7   7  11  17 803  10  37]
 [  1   0   0  10  34   7 754  31]
 [ 19  14  12  62  66 123  21 472]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [107][   10/  207]    Overall Loss 0.018521    Objective Loss 0.018521                                        LR 0.000500    Time 0.128929    
Epoch: [107][   20/  207]    Overall Loss 0.019283    Objective Loss 0.019283                                        LR 0.000500    Time 0.100739    
Epoch: [107][   30/  207]    Overall Loss 0.019876    Objective Loss 0.019876                                        LR 0.000500    Time 0.090891    
Epoch: [107][   40/  207]    Overall Loss 0.020321    Objective Loss 0.020321                                        LR 0.000500    Time 0.086695    
Epoch: [107][   50/  207]    Overall Loss 0.020366    Objective Loss 0.020366                                        LR 0.000500    Time 0.083827    
Epoch: [107][   60/  207]    Overall Loss 0.020655    Objective Loss 0.020655                                        LR 0.000500    Time 0.082114    
Epoch: [107][   70/  207]    Overall Loss 0.021261    Objective Loss 0.021261                                        LR 0.000500    Time 0.080619    
Epoch: [107][   80/  207]    Overall Loss 0.021461    Objective Loss 0.021461                                        LR 0.000500    Time 0.079549    
Epoch: [107][   90/  207]    Overall Loss 0.022268    Objective Loss 0.022268                                        LR 0.000500    Time 0.078604    
Epoch: [107][  100/  207]    Overall Loss 0.022892    Objective Loss 0.022892                                        LR 0.000500    Time 0.077907    
Epoch: [107][  110/  207]    Overall Loss 0.022814    Objective Loss 0.022814                                        LR 0.000500    Time 0.077411    
Epoch: [107][  120/  207]    Overall Loss 0.022890    Objective Loss 0.022890                                        LR 0.000500    Time 0.076865    
Epoch: [107][  130/  207]    Overall Loss 0.023078    Objective Loss 0.023078                                        LR 0.000500    Time 0.076534    
Epoch: [107][  140/  207]    Overall Loss 0.023121    Objective Loss 0.023121                                        LR 0.000500    Time 0.076124    
Epoch: [107][  150/  207]    Overall Loss 0.023359    Objective Loss 0.023359                                        LR 0.000500    Time 0.075871    
Epoch: [107][  160/  207]    Overall Loss 0.023344    Objective Loss 0.023344                                        LR 0.000500    Time 0.075628    
Epoch: [107][  170/  207]    Overall Loss 0.023373    Objective Loss 0.023373                                        LR 0.000500    Time 0.075366    
Epoch: [107][  180/  207]    Overall Loss 0.023506    Objective Loss 0.023506                                        LR 0.000500    Time 0.075235    
Epoch: [107][  190/  207]    Overall Loss 0.023606    Objective Loss 0.023606                                        LR 0.000500    Time 0.075140    
Epoch: [107][  200/  207]    Overall Loss 0.023868    Objective Loss 0.023868                                        LR 0.000500    Time 0.074946    
Epoch: [107][  207/  207]    Overall Loss 0.023822    Objective Loss 0.023822    Top1 97.734994    Top5 100.000000    LR 0.000500    Time 0.074710    
--- validate (epoch=107)-----------
5136 samples (512 per mini-batch)
Epoch: [107][   10/   11]    Loss 0.521022    Top1 85.234375    Top5 99.687500    
Epoch: [107][   11/   11]    Loss 0.533692    Top1 85.221963    Top5 99.688474    
==> Top1: 85.222    Top5: 99.688    Loss: 0.534

==> Confusion:
[[268   2   3   3   1   1   3  19]
 [  1 262  23   1   0   2   1  10]
 [  3  12 275   1   0   3   0   6]
 [  0   4   0 746  56   8   8  15]
 [  2   0   0  40 787   5  21  24]
 [  3   5   7   8  22 799  17  33]
 [  1   0   0   5  31   8 770  22]
 [ 18  15  14  50  74 121  27 470]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [108][   10/  207]    Overall Loss 0.018526    Objective Loss 0.018526                                        LR 0.000500    Time 0.130030    
Epoch: [108][   20/  207]    Overall Loss 0.020073    Objective Loss 0.020073                                        LR 0.000500    Time 0.100816    
Epoch: [108][   30/  207]    Overall Loss 0.020123    Objective Loss 0.020123                                        LR 0.000500    Time 0.091042    
Epoch: [108][   40/  207]    Overall Loss 0.020824    Objective Loss 0.020824                                        LR 0.000500    Time 0.086045    
Epoch: [108][   50/  207]    Overall Loss 0.020660    Objective Loss 0.020660                                        LR 0.000500    Time 0.083413    
Epoch: [108][   60/  207]    Overall Loss 0.020532    Objective Loss 0.020532                                        LR 0.000500    Time 0.081652    
Epoch: [108][   70/  207]    Overall Loss 0.020840    Objective Loss 0.020840                                        LR 0.000500    Time 0.080150    
Epoch: [108][   80/  207]    Overall Loss 0.020978    Objective Loss 0.020978                                        LR 0.000500    Time 0.078973    
Epoch: [108][   90/  207]    Overall Loss 0.021372    Objective Loss 0.021372                                        LR 0.000500    Time 0.078172    
Epoch: [108][  100/  207]    Overall Loss 0.021289    Objective Loss 0.021289                                        LR 0.000500    Time 0.077417    
Epoch: [108][  110/  207]    Overall Loss 0.021141    Objective Loss 0.021141                                        LR 0.000500    Time 0.077036    
Epoch: [108][  120/  207]    Overall Loss 0.021254    Objective Loss 0.021254                                        LR 0.000500    Time 0.076844    
Epoch: [108][  130/  207]    Overall Loss 0.021421    Objective Loss 0.021421                                        LR 0.000500    Time 0.076500    
Epoch: [108][  140/  207]    Overall Loss 0.021482    Objective Loss 0.021482                                        LR 0.000500    Time 0.076128    
Epoch: [108][  150/  207]    Overall Loss 0.021672    Objective Loss 0.021672                                        LR 0.000500    Time 0.075806    
Epoch: [108][  160/  207]    Overall Loss 0.021731    Objective Loss 0.021731                                        LR 0.000500    Time 0.075539    
Epoch: [108][  170/  207]    Overall Loss 0.021665    Objective Loss 0.021665                                        LR 0.000500    Time 0.075349    
Epoch: [108][  180/  207]    Overall Loss 0.021850    Objective Loss 0.021850                                        LR 0.000500    Time 0.075173    
Epoch: [108][  190/  207]    Overall Loss 0.021923    Objective Loss 0.021923                                        LR 0.000500    Time 0.075029    
Epoch: [108][  200/  207]    Overall Loss 0.022194    Objective Loss 0.022194                                        LR 0.000500    Time 0.074927    
Epoch: [108][  207/  207]    Overall Loss 0.022204    Objective Loss 0.022204    Top1 98.187995    Top5 100.000000    LR 0.000500    Time 0.074687    
--- validate (epoch=108)-----------
5136 samples (512 per mini-batch)
Epoch: [108][   10/   11]    Loss 0.488771    Top1 84.707031    Top5 99.726562    
Epoch: [108][   11/   11]    Loss 0.487164    Top1 84.715732    Top5 99.727414    
==> Top1: 84.716    Top5: 99.727    Loss: 0.487

==> Confusion:
[[279   2   2   3   0   3   1  10]
 [  2 264  22   1   0   4   0   7]
 [  4  15 272   1   0   5   0   3]
 [  0   1   0 752  51  17   3  13]
 [  2   0   0  44 782  13  16  22]
 [  3   5   4   6  19 823   8  26]
 [  2   0   0   5  31  19 758  22]
 [ 24  14  16  53  72 164  25 421]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [109][   10/  207]    Overall Loss 0.021773    Objective Loss 0.021773                                        LR 0.000500    Time 0.128562    
Epoch: [109][   20/  207]    Overall Loss 0.020454    Objective Loss 0.020454                                        LR 0.000500    Time 0.099341    
Epoch: [109][   30/  207]    Overall Loss 0.019985    Objective Loss 0.019985                                        LR 0.000500    Time 0.090472    
Epoch: [109][   40/  207]    Overall Loss 0.020311    Objective Loss 0.020311                                        LR 0.000500    Time 0.085678    
Epoch: [109][   50/  207]    Overall Loss 0.020495    Objective Loss 0.020495                                        LR 0.000500    Time 0.082915    
Epoch: [109][   60/  207]    Overall Loss 0.021219    Objective Loss 0.021219                                        LR 0.000500    Time 0.081117    
Epoch: [109][   70/  207]    Overall Loss 0.021881    Objective Loss 0.021881                                        LR 0.000500    Time 0.079758    
Epoch: [109][   80/  207]    Overall Loss 0.022053    Objective Loss 0.022053                                        LR 0.000500    Time 0.078764    
Epoch: [109][   90/  207]    Overall Loss 0.021998    Objective Loss 0.021998                                        LR 0.000500    Time 0.077740    
Epoch: [109][  100/  207]    Overall Loss 0.021882    Objective Loss 0.021882                                        LR 0.000500    Time 0.077084    
Epoch: [109][  110/  207]    Overall Loss 0.022099    Objective Loss 0.022099                                        LR 0.000500    Time 0.076711    
Epoch: [109][  120/  207]    Overall Loss 0.021737    Objective Loss 0.021737                                        LR 0.000500    Time 0.076334    
Epoch: [109][  130/  207]    Overall Loss 0.021819    Objective Loss 0.021819                                        LR 0.000500    Time 0.075998    
Epoch: [109][  140/  207]    Overall Loss 0.021730    Objective Loss 0.021730                                        LR 0.000500    Time 0.075594    
Epoch: [109][  150/  207]    Overall Loss 0.021608    Objective Loss 0.021608                                        LR 0.000500    Time 0.075455    
Epoch: [109][  160/  207]    Overall Loss 0.021913    Objective Loss 0.021913                                        LR 0.000500    Time 0.075236    
Epoch: [109][  170/  207]    Overall Loss 0.021978    Objective Loss 0.021978                                        LR 0.000500    Time 0.074960    
Epoch: [109][  180/  207]    Overall Loss 0.021986    Objective Loss 0.021986                                        LR 0.000500    Time 0.074862    
Epoch: [109][  190/  207]    Overall Loss 0.021986    Objective Loss 0.021986                                        LR 0.000500    Time 0.074672    
Epoch: [109][  200/  207]    Overall Loss 0.022209    Objective Loss 0.022209                                        LR 0.000500    Time 0.074482    
Epoch: [109][  207/  207]    Overall Loss 0.022349    Objective Loss 0.022349    Top1 99.093998    Top5 100.000000    LR 0.000500    Time 0.074263    
--- validate (epoch=109)-----------
5136 samples (512 per mini-batch)
Epoch: [109][   10/   11]    Loss 0.574438    Top1 84.980469    Top5 99.667969    
Epoch: [109][   11/   11]    Loss 0.580738    Top1 84.929907    Top5 99.669003    
==> Top1: 84.930    Top5: 99.669    Loss: 0.581

==> Confusion:
[[269   3   9   3   1   1   1  13]
 [  1 249  35   2   0   2   0  11]
 [  1  12 282   1   0   0   0   4]
 [  0   3   1 758  47  10   4  14]
 [  2   0   0  50 778   5  14  30]
 [  5   4  14  14  25 772   9  51]
 [  3   0   0   7  31   9 758  29]
 [ 18  15  22  52  71  94  21 496]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [110][   10/  207]    Overall Loss 0.026692    Objective Loss 0.026692                                        LR 0.000500    Time 0.127908    
Epoch: [110][   20/  207]    Overall Loss 0.025104    Objective Loss 0.025104                                        LR 0.000500    Time 0.099310    
Epoch: [110][   30/  207]    Overall Loss 0.024920    Objective Loss 0.024920                                        LR 0.000500    Time 0.089693    
Epoch: [110][   40/  207]    Overall Loss 0.024418    Objective Loss 0.024418                                        LR 0.000500    Time 0.085943    
Epoch: [110][   50/  207]    Overall Loss 0.025264    Objective Loss 0.025264                                        LR 0.000500    Time 0.082874    
Epoch: [110][   60/  207]    Overall Loss 0.024905    Objective Loss 0.024905                                        LR 0.000500    Time 0.081367    
Epoch: [110][   70/  207]    Overall Loss 0.024226    Objective Loss 0.024226                                        LR 0.000500    Time 0.079979    
Epoch: [110][   80/  207]    Overall Loss 0.023787    Objective Loss 0.023787                                        LR 0.000500    Time 0.079027    
Epoch: [110][   90/  207]    Overall Loss 0.023414    Objective Loss 0.023414                                        LR 0.000500    Time 0.078188    
Epoch: [110][  100/  207]    Overall Loss 0.023275    Objective Loss 0.023275                                        LR 0.000500    Time 0.077546    
Epoch: [110][  110/  207]    Overall Loss 0.023386    Objective Loss 0.023386                                        LR 0.000500    Time 0.076953    
Epoch: [110][  120/  207]    Overall Loss 0.023263    Objective Loss 0.023263                                        LR 0.000500    Time 0.076619    
Epoch: [110][  130/  207]    Overall Loss 0.023208    Objective Loss 0.023208                                        LR 0.000500    Time 0.076415    
Epoch: [110][  140/  207]    Overall Loss 0.023149    Objective Loss 0.023149                                        LR 0.000500    Time 0.076175    
Epoch: [110][  150/  207]    Overall Loss 0.023438    Objective Loss 0.023438                                        LR 0.000500    Time 0.075985    
Epoch: [110][  160/  207]    Overall Loss 0.023515    Objective Loss 0.023515                                        LR 0.000500    Time 0.075646    
Epoch: [110][  170/  207]    Overall Loss 0.023831    Objective Loss 0.023831                                        LR 0.000500    Time 0.075409    
Epoch: [110][  180/  207]    Overall Loss 0.024035    Objective Loss 0.024035                                        LR 0.000500    Time 0.075165    
Epoch: [110][  190/  207]    Overall Loss 0.024677    Objective Loss 0.024677                                        LR 0.000500    Time 0.074893    
Epoch: [110][  200/  207]    Overall Loss 0.025204    Objective Loss 0.025204                                        LR 0.000500    Time 0.074629    
Epoch: [110][  207/  207]    Overall Loss 0.025628    Objective Loss 0.025628    Top1 97.508494    Top5 100.000000    LR 0.000500    Time 0.074412    
--- validate (epoch=110)-----------
5136 samples (512 per mini-batch)
Epoch: [110][   10/   11]    Loss 0.703320    Top1 84.277344    Top5 99.746094    
Epoch: [110][   11/   11]    Loss 0.642665    Top1 84.306854    Top5 99.746885    
==> Top1: 84.307    Top5: 99.747    Loss: 0.643

==> Confusion:
[[246   1   3   2   1   4   1  42]
 [  1 240  27   2   0   5   1  24]
 [  1  10 257   1   0   6   0  25]
 [  0   0   0 749  44  14   7  23]
 [  1   0   0  47 774   9  16  32]
 [  0   2   6   4  13 819  12  38]
 [  1   0   0   7  33  15 748  33]
 [  5   5   9  42  65 147  19 497]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [111][   10/  207]    Overall Loss 0.039363    Objective Loss 0.039363                                        LR 0.000500    Time 0.129445    
Epoch: [111][   20/  207]    Overall Loss 0.035438    Objective Loss 0.035438                                        LR 0.000500    Time 0.100364    
Epoch: [111][   30/  207]    Overall Loss 0.034714    Objective Loss 0.034714                                        LR 0.000500    Time 0.091166    
Epoch: [111][   40/  207]    Overall Loss 0.033354    Objective Loss 0.033354                                        LR 0.000500    Time 0.086922    
Epoch: [111][   50/  207]    Overall Loss 0.032478    Objective Loss 0.032478                                        LR 0.000500    Time 0.083952    
Epoch: [111][   60/  207]    Overall Loss 0.032520    Objective Loss 0.032520                                        LR 0.000500    Time 0.082255    
Epoch: [111][   70/  207]    Overall Loss 0.032198    Objective Loss 0.032198                                        LR 0.000500    Time 0.081139    
Epoch: [111][   80/  207]    Overall Loss 0.033630    Objective Loss 0.033630                                        LR 0.000500    Time 0.079980    
Epoch: [111][   90/  207]    Overall Loss 0.041368    Objective Loss 0.041368                                        LR 0.000500    Time 0.079314    
Epoch: [111][  100/  207]    Overall Loss 0.043067    Objective Loss 0.043067                                        LR 0.000500    Time 0.078536    
Epoch: [111][  110/  207]    Overall Loss 0.046691    Objective Loss 0.046691                                        LR 0.000500    Time 0.078129    
Epoch: [111][  120/  207]    Overall Loss 0.048825    Objective Loss 0.048825                                        LR 0.000500    Time 0.077767    
Epoch: [111][  130/  207]    Overall Loss 0.053287    Objective Loss 0.053287                                        LR 0.000500    Time 0.077392    
Epoch: [111][  140/  207]    Overall Loss 0.057540    Objective Loss 0.057540                                        LR 0.000500    Time 0.076966    
Epoch: [111][  150/  207]    Overall Loss 0.060155    Objective Loss 0.060155                                        LR 0.000500    Time 0.076712    
Epoch: [111][  160/  207]    Overall Loss 0.062447    Objective Loss 0.062447                                        LR 0.000500    Time 0.076340    
Epoch: [111][  170/  207]    Overall Loss 0.063546    Objective Loss 0.063546                                        LR 0.000500    Time 0.076179    
Epoch: [111][  180/  207]    Overall Loss 0.063867    Objective Loss 0.063867                                        LR 0.000500    Time 0.075899    
Epoch: [111][  190/  207]    Overall Loss 0.064220    Objective Loss 0.064220                                        LR 0.000500    Time 0.075696    
Epoch: [111][  200/  207]    Overall Loss 0.065847    Objective Loss 0.065847                                        LR 0.000500    Time 0.075620    
Epoch: [111][  207/  207]    Overall Loss 0.066792    Objective Loss 0.066792    Top1 95.809740    Top5 100.000000    LR 0.000500    Time 0.075372    
--- validate (epoch=111)-----------
5136 samples (512 per mini-batch)
Epoch: [111][   10/   11]    Loss 0.566304    Top1 83.554688    Top5 99.707031    
Epoch: [111][   11/   11]    Loss 0.521613    Top1 83.566978    Top5 99.707944    
==> Top1: 83.567    Top5: 99.708    Loss: 0.522

==> Confusion:
[[281   3   2   0   0   0   1  13]
 [  3 275  11   1   0   3   0   7]
 [  5  30 255   1   0   2   0   7]
 [  1   3   1 723  76  10   6  17]
 [  2   0   0  29 800   5  19  24]
 [ 17   8   8  10  30 774  11  36]
 [  3   1   0   5  40  14 754  20]
 [ 36  22  15  39  96 131  20 430]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [112][   10/  207]    Overall Loss 0.062276    Objective Loss 0.062276                                        LR 0.000500    Time 0.127902    
Epoch: [112][   20/  207]    Overall Loss 0.063874    Objective Loss 0.063874                                        LR 0.000500    Time 0.098722    
Epoch: [112][   30/  207]    Overall Loss 0.061536    Objective Loss 0.061536                                        LR 0.000500    Time 0.089826    
Epoch: [112][   40/  207]    Overall Loss 0.060976    Objective Loss 0.060976                                        LR 0.000500    Time 0.085155    
Epoch: [112][   50/  207]    Overall Loss 0.061311    Objective Loss 0.061311                                        LR 0.000500    Time 0.082666    
Epoch: [112][   60/  207]    Overall Loss 0.058980    Objective Loss 0.058980                                        LR 0.000500    Time 0.080715    
Epoch: [112][   70/  207]    Overall Loss 0.060010    Objective Loss 0.060010                                        LR 0.000500    Time 0.079470    
Epoch: [112][   80/  207]    Overall Loss 0.059248    Objective Loss 0.059248                                        LR 0.000500    Time 0.078675    
Epoch: [112][   90/  207]    Overall Loss 0.061674    Objective Loss 0.061674                                        LR 0.000500    Time 0.077941    
Epoch: [112][  100/  207]    Overall Loss 0.060947    Objective Loss 0.060947                                        LR 0.000500    Time 0.077457    
Epoch: [112][  110/  207]    Overall Loss 0.060539    Objective Loss 0.060539                                        LR 0.000500    Time 0.077047    
Epoch: [112][  120/  207]    Overall Loss 0.059704    Objective Loss 0.059704                                        LR 0.000500    Time 0.076641    
Epoch: [112][  130/  207]    Overall Loss 0.058296    Objective Loss 0.058296                                        LR 0.000500    Time 0.076352    
Epoch: [112][  140/  207]    Overall Loss 0.057129    Objective Loss 0.057129                                        LR 0.000500    Time 0.076060    
Epoch: [112][  150/  207]    Overall Loss 0.055917    Objective Loss 0.055917                                        LR 0.000500    Time 0.075668    
Epoch: [112][  160/  207]    Overall Loss 0.054842    Objective Loss 0.054842                                        LR 0.000500    Time 0.075293    
Epoch: [112][  170/  207]    Overall Loss 0.054058    Objective Loss 0.054058                                        LR 0.000500    Time 0.075063    
Epoch: [112][  180/  207]    Overall Loss 0.054192    Objective Loss 0.054192                                        LR 0.000500    Time 0.074983    
Epoch: [112][  190/  207]    Overall Loss 0.053486    Objective Loss 0.053486                                        LR 0.000500    Time 0.074767    
Epoch: [112][  200/  207]    Overall Loss 0.052866    Objective Loss 0.052866                                        LR 0.000500    Time 0.074691    
Epoch: [112][  207/  207]    Overall Loss 0.052130    Objective Loss 0.052130    Top1 95.809740    Top5 100.000000    LR 0.000500    Time 0.074456    
--- validate (epoch=112)-----------
5136 samples (512 per mini-batch)
Epoch: [112][   10/   11]    Loss 0.495888    Top1 84.414062    Top5 99.589844    
Epoch: [112][   11/   11]    Loss 0.462494    Top1 84.423676    Top5 99.591121    
==> Top1: 84.424    Top5: 99.591    Loss: 0.462

==> Confusion:
[[277   3   3   0   2   2   2  11]
 [  2 266  26   0   0   1   0   5]
 [  3  20 272   0   0   3   0   2]
 [  0   6   1 737  69   4   9  11]
 [  2   0   0  34 799   1  20  23]
 [ 12  10  13  12  25 765  21  36]
 [  2   0   0   6  37   6 768  18]
 [ 36  24  18  50  84  99  26 452]]

==> Best [Top1: 85.572   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 104]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [113][   10/  207]    Overall Loss 0.031192    Objective Loss 0.031192                                        LR 0.000500    Time 0.131270    
Epoch: [113][   20/  207]    Overall Loss 0.031073    Objective Loss 0.031073                                        LR 0.000500    Time 0.100491    
Epoch: [113][   30/  207]    Overall Loss 0.028374    Objective Loss 0.028374                                        LR 0.000500    Time 0.090820    
Epoch: [113][   40/  207]    Overall Loss 0.028002    Objective Loss 0.028002                                        LR 0.000500    Time 0.086112    
Epoch: [113][   50/  207]    Overall Loss 0.027884    Objective Loss 0.027884                                        LR 0.000500    Time 0.083008    
Epoch: [113][   60/  207]    Overall Loss 0.026881    Objective Loss 0.026881                                        LR 0.000500    Time 0.081245    
Epoch: [113][   70/  207]    Overall Loss 0.027667    Objective Loss 0.027667                                        LR 0.000500    Time 0.079820    
Epoch: [113][   80/  207]    Overall Loss 0.027064    Objective Loss 0.027064                                        LR 0.000500    Time 0.078710    
Epoch: [113][   90/  207]    Overall Loss 0.026730    Objective Loss 0.026730                                        LR 0.000500    Time 0.077815    
Epoch: [113][  100/  207]    Overall Loss 0.026598    Objective Loss 0.026598                                        LR 0.000500    Time 0.077090    
Epoch: [113][  110/  207]    Overall Loss 0.026299    Objective Loss 0.026299                                        LR 0.000500    Time 0.076492    
Epoch: [113][  120/  207]    Overall Loss 0.026119    Objective Loss 0.026119                                        LR 0.000500    Time 0.076053    
Epoch: [113][  130/  207]    Overall Loss 0.025854    Objective Loss 0.025854                                        LR 0.000500    Time 0.075723    
Epoch: [113][  140/  207]    Overall Loss 0.025729    Objective Loss 0.025729                                        LR 0.000500    Time 0.075456    
Epoch: [113][  150/  207]    Overall Loss 0.025400    Objective Loss 0.025400                                        LR 0.000500    Time 0.075257    
Epoch: [113][  160/  207]    Overall Loss 0.025105    Objective Loss 0.025105                                        LR 0.000500    Time 0.075081    
Epoch: [113][  170/  207]    Overall Loss 0.024913    Objective Loss 0.024913                                        LR 0.000500    Time 0.074920    
Epoch: [113][  180/  207]    Overall Loss 0.024978    Objective Loss 0.024978                                        LR 0.000500    Time 0.074718    
Epoch: [113][  190/  207]    Overall Loss 0.025043    Objective Loss 0.025043                                        LR 0.000500    Time 0.074645    
Epoch: [113][  200/  207]    Overall Loss 0.024997    Objective Loss 0.024997                                        LR 0.000500    Time 0.074520    
Epoch: [113][  207/  207]    Overall Loss 0.024913    Objective Loss 0.024913    Top1 98.527746    Top5 100.000000    LR 0.000500    Time 0.074293    
--- validate (epoch=113)-----------
5136 samples (512 per mini-batch)
Epoch: [113][   10/   11]    Loss 0.531890    Top1 85.703125    Top5 99.648438    
Epoch: [113][   11/   11]    Loss 0.582983    Top1 85.630841    Top5 99.630062    
==> Top1: 85.631    Top5: 99.630    Loss: 0.583

==> Confusion:
[[273   2   2   2   1   1   2  17]
 [  1 269  18   1   0   3   1   7]
 [  3  19 271   1   0   2   0   4]
 [  0   1   1 753  42   7  16  17]
 [  2   0   0  42 768   2  35  30]
 [  6   6   7  11  17 781  23  43]
 [  1   0   0   6  20   8 784  18]
 [ 16  11  11  43  61 100  48 499]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [114][   10/  207]    Overall Loss 0.019853    Objective Loss 0.019853                                        LR 0.000500    Time 0.129250    
Epoch: [114][   20/  207]    Overall Loss 0.020334    Objective Loss 0.020334                                        LR 0.000500    Time 0.100829    
Epoch: [114][   30/  207]    Overall Loss 0.020667    Objective Loss 0.020667                                        LR 0.000500    Time 0.090376    
Epoch: [114][   40/  207]    Overall Loss 0.020430    Objective Loss 0.020430                                        LR 0.000500    Time 0.085824    
Epoch: [114][   50/  207]    Overall Loss 0.020505    Objective Loss 0.020505                                        LR 0.000500    Time 0.083364    
Epoch: [114][   60/  207]    Overall Loss 0.020577    Objective Loss 0.020577                                        LR 0.000500    Time 0.081252    
Epoch: [114][   70/  207]    Overall Loss 0.020512    Objective Loss 0.020512                                        LR 0.000500    Time 0.079845    
Epoch: [114][   80/  207]    Overall Loss 0.020714    Objective Loss 0.020714                                        LR 0.000500    Time 0.078810    
Epoch: [114][   90/  207]    Overall Loss 0.020212    Objective Loss 0.020212                                        LR 0.000500    Time 0.078085    
Epoch: [114][  100/  207]    Overall Loss 0.020038    Objective Loss 0.020038                                        LR 0.000500    Time 0.077693    
Epoch: [114][  110/  207]    Overall Loss 0.020008    Objective Loss 0.020008                                        LR 0.000500    Time 0.077231    
Epoch: [114][  120/  207]    Overall Loss 0.020079    Objective Loss 0.020079                                        LR 0.000500    Time 0.076724    
Epoch: [114][  130/  207]    Overall Loss 0.020152    Objective Loss 0.020152                                        LR 0.000500    Time 0.076402    
Epoch: [114][  140/  207]    Overall Loss 0.020161    Objective Loss 0.020161                                        LR 0.000500    Time 0.076096    
Epoch: [114][  150/  207]    Overall Loss 0.020179    Objective Loss 0.020179                                        LR 0.000500    Time 0.075776    
Epoch: [114][  160/  207]    Overall Loss 0.020262    Objective Loss 0.020262                                        LR 0.000500    Time 0.075485    
Epoch: [114][  170/  207]    Overall Loss 0.020507    Objective Loss 0.020507                                        LR 0.000500    Time 0.075226    
Epoch: [114][  180/  207]    Overall Loss 0.020479    Objective Loss 0.020479                                        LR 0.000500    Time 0.075009    
Epoch: [114][  190/  207]    Overall Loss 0.020697    Objective Loss 0.020697                                        LR 0.000500    Time 0.074805    
Epoch: [114][  200/  207]    Overall Loss 0.020636    Objective Loss 0.020636                                        LR 0.000500    Time 0.074660    
Epoch: [114][  207/  207]    Overall Loss 0.020514    Objective Loss 0.020514    Top1 97.508494    Top5 100.000000    LR 0.000500    Time 0.074424    
--- validate (epoch=114)-----------
5136 samples (512 per mini-batch)
Epoch: [114][   10/   11]    Loss 0.587305    Top1 85.507812    Top5 99.726562    
Epoch: [114][   11/   11]    Loss 0.581870    Top1 85.514019    Top5 99.727414    
==> Top1: 85.514    Top5: 99.727    Loss: 0.582

==> Confusion:
[[274   2   3   0   1   1   1  18]
 [  1 262  21   0   0   2   0  14]
 [  3  17 258   1   0   2   0  19]
 [  0   1   1 748  47   8  14  18]
 [  2   0   0  39 767   5  29  37]
 [  4   6   6   8  18 759  19  74]
 [  1   0   0   5  22   7 779  23]
 [ 17  10  11  38  63  71  34 545]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [115][   10/  207]    Overall Loss 0.018359    Objective Loss 0.018359                                        LR 0.000500    Time 0.130663    
Epoch: [115][   20/  207]    Overall Loss 0.018221    Objective Loss 0.018221                                        LR 0.000500    Time 0.101027    
Epoch: [115][   30/  207]    Overall Loss 0.018851    Objective Loss 0.018851                                        LR 0.000500    Time 0.091197    
Epoch: [115][   40/  207]    Overall Loss 0.019201    Objective Loss 0.019201                                        LR 0.000500    Time 0.086473    
Epoch: [115][   50/  207]    Overall Loss 0.018986    Objective Loss 0.018986                                        LR 0.000500    Time 0.083448    
Epoch: [115][   60/  207]    Overall Loss 0.019357    Objective Loss 0.019357                                        LR 0.000500    Time 0.081607    
Epoch: [115][   70/  207]    Overall Loss 0.019205    Objective Loss 0.019205                                        LR 0.000500    Time 0.080333    
Epoch: [115][   80/  207]    Overall Loss 0.019102    Objective Loss 0.019102                                        LR 0.000500    Time 0.079258    
Epoch: [115][   90/  207]    Overall Loss 0.019126    Objective Loss 0.019126                                        LR 0.000500    Time 0.078348    
Epoch: [115][  100/  207]    Overall Loss 0.019185    Objective Loss 0.019185                                        LR 0.000500    Time 0.077817    
Epoch: [115][  110/  207]    Overall Loss 0.019184    Objective Loss 0.019184                                        LR 0.000500    Time 0.077408    
Epoch: [115][  120/  207]    Overall Loss 0.019069    Objective Loss 0.019069                                        LR 0.000500    Time 0.077118    
Epoch: [115][  130/  207]    Overall Loss 0.019200    Objective Loss 0.019200                                        LR 0.000500    Time 0.076899    
Epoch: [115][  140/  207]    Overall Loss 0.019236    Objective Loss 0.019236                                        LR 0.000500    Time 0.076530    
Epoch: [115][  150/  207]    Overall Loss 0.019181    Objective Loss 0.019181                                        LR 0.000500    Time 0.076142    
Epoch: [115][  160/  207]    Overall Loss 0.019259    Objective Loss 0.019259                                        LR 0.000500    Time 0.075983    
Epoch: [115][  170/  207]    Overall Loss 0.019459    Objective Loss 0.019459                                        LR 0.000500    Time 0.075864    
Epoch: [115][  180/  207]    Overall Loss 0.019454    Objective Loss 0.019454                                        LR 0.000500    Time 0.075608    
Epoch: [115][  190/  207]    Overall Loss 0.019443    Objective Loss 0.019443                                        LR 0.000500    Time 0.075385    
Epoch: [115][  200/  207]    Overall Loss 0.019555    Objective Loss 0.019555                                        LR 0.000500    Time 0.075165    
Epoch: [115][  207/  207]    Overall Loss 0.019581    Objective Loss 0.019581    Top1 98.187995    Top5 100.000000    LR 0.000500    Time 0.074921    
--- validate (epoch=115)-----------
5136 samples (512 per mini-batch)
Epoch: [115][   10/   11]    Loss 0.579163    Top1 85.566406    Top5 99.746094    
Epoch: [115][   11/   11]    Loss 0.540220    Top1 85.591900    Top5 99.746885    
==> Top1: 85.592    Top5: 99.747    Loss: 0.540

==> Confusion:
[[270   2   5   2   1   1   1  18]
 [  1 266  17   0   0   1   1  14]
 [  3  19 268   1   0   2   0   7]
 [  0   1   0 745  56   9   8  18]
 [  2   0   0  36 785   3  22  31]
 [  4   6   6   8  21 767  19  63]
 [  1   0   0   5  23   7 768  33]
 [ 16  10  12  40  66  90  28 527]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [116][   10/  207]    Overall Loss 0.016487    Objective Loss 0.016487                                        LR 0.000500    Time 0.129353    
Epoch: [116][   20/  207]    Overall Loss 0.017067    Objective Loss 0.017067                                        LR 0.000500    Time 0.099594    
Epoch: [116][   30/  207]    Overall Loss 0.017450    Objective Loss 0.017450                                        LR 0.000500    Time 0.089845    
Epoch: [116][   40/  207]    Overall Loss 0.018346    Objective Loss 0.018346                                        LR 0.000500    Time 0.085421    
Epoch: [116][   50/  207]    Overall Loss 0.018614    Objective Loss 0.018614                                        LR 0.000500    Time 0.082731    
Epoch: [116][   60/  207]    Overall Loss 0.018756    Objective Loss 0.018756                                        LR 0.000500    Time 0.081213    
Epoch: [116][   70/  207]    Overall Loss 0.018752    Objective Loss 0.018752                                        LR 0.000500    Time 0.079677    
Epoch: [116][   80/  207]    Overall Loss 0.018842    Objective Loss 0.018842                                        LR 0.000500    Time 0.078557    
Epoch: [116][   90/  207]    Overall Loss 0.018712    Objective Loss 0.018712                                        LR 0.000500    Time 0.077685    
Epoch: [116][  100/  207]    Overall Loss 0.019431    Objective Loss 0.019431                                        LR 0.000500    Time 0.077158    
Epoch: [116][  110/  207]    Overall Loss 0.019372    Objective Loss 0.019372                                        LR 0.000500    Time 0.076635    
Epoch: [116][  120/  207]    Overall Loss 0.019599    Objective Loss 0.019599                                        LR 0.000500    Time 0.076135    
Epoch: [116][  130/  207]    Overall Loss 0.019467    Objective Loss 0.019467                                        LR 0.000500    Time 0.075653    
Epoch: [116][  140/  207]    Overall Loss 0.019370    Objective Loss 0.019370                                        LR 0.000500    Time 0.075282    
Epoch: [116][  150/  207]    Overall Loss 0.019357    Objective Loss 0.019357                                        LR 0.000500    Time 0.074996    
Epoch: [116][  160/  207]    Overall Loss 0.019519    Objective Loss 0.019519                                        LR 0.000500    Time 0.074665    
Epoch: [116][  170/  207]    Overall Loss 0.019724    Objective Loss 0.019724                                        LR 0.000500    Time 0.074432    
Epoch: [116][  180/  207]    Overall Loss 0.019647    Objective Loss 0.019647                                        LR 0.000500    Time 0.074437    
Epoch: [116][  190/  207]    Overall Loss 0.019602    Objective Loss 0.019602                                        LR 0.000500    Time 0.074317    
Epoch: [116][  200/  207]    Overall Loss 0.019689    Objective Loss 0.019689                                        LR 0.000500    Time 0.074145    
Epoch: [116][  207/  207]    Overall Loss 0.019774    Objective Loss 0.019774    Top1 98.074745    Top5 99.886750    LR 0.000500    Time 0.073998    
--- validate (epoch=116)-----------
5136 samples (512 per mini-batch)
Epoch: [116][   10/   11]    Loss 0.526936    Top1 85.605469    Top5 99.746094    
Epoch: [116][   11/   11]    Loss 0.511668    Top1 85.611371    Top5 99.746885    
==> Top1: 85.611    Top5: 99.747    Loss: 0.512

==> Confusion:
[[276   2   3   2   1   2   1  13]
 [  1 262  24   0   0   3   0  10]
 [  3  15 277   1   0   2   0   2]
 [  0   2   0 747  48  13   6  21]
 [  2   0   0  41 777   9  24  26]
 [  5   7   7   5  18 810  13  29]
 [  2   1   0   7  29  17 756  25]
 [ 26  12  14  34  68 117  26 492]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [117][   10/  207]    Overall Loss 0.017248    Objective Loss 0.017248                                        LR 0.000500    Time 0.130156    
Epoch: [117][   20/  207]    Overall Loss 0.018347    Objective Loss 0.018347                                        LR 0.000500    Time 0.100476    
Epoch: [117][   30/  207]    Overall Loss 0.017815    Objective Loss 0.017815                                        LR 0.000500    Time 0.090338    
Epoch: [117][   40/  207]    Overall Loss 0.018456    Objective Loss 0.018456                                        LR 0.000500    Time 0.085682    
Epoch: [117][   50/  207]    Overall Loss 0.018167    Objective Loss 0.018167                                        LR 0.000500    Time 0.082780    
Epoch: [117][   60/  207]    Overall Loss 0.018305    Objective Loss 0.018305                                        LR 0.000500    Time 0.080898    
Epoch: [117][   70/  207]    Overall Loss 0.018174    Objective Loss 0.018174                                        LR 0.000500    Time 0.079512    
Epoch: [117][   80/  207]    Overall Loss 0.018429    Objective Loss 0.018429                                        LR 0.000500    Time 0.078730    
Epoch: [117][   90/  207]    Overall Loss 0.018237    Objective Loss 0.018237                                        LR 0.000500    Time 0.077830    
Epoch: [117][  100/  207]    Overall Loss 0.018095    Objective Loss 0.018095                                        LR 0.000500    Time 0.077152    
Epoch: [117][  110/  207]    Overall Loss 0.017860    Objective Loss 0.017860                                        LR 0.000500    Time 0.076577    
Epoch: [117][  120/  207]    Overall Loss 0.018121    Objective Loss 0.018121                                        LR 0.000500    Time 0.076190    
Epoch: [117][  130/  207]    Overall Loss 0.018132    Objective Loss 0.018132                                        LR 0.000500    Time 0.075918    
Epoch: [117][  140/  207]    Overall Loss 0.018005    Objective Loss 0.018005                                        LR 0.000500    Time 0.075592    
Epoch: [117][  150/  207]    Overall Loss 0.017945    Objective Loss 0.017945                                        LR 0.000500    Time 0.075229    
Epoch: [117][  160/  207]    Overall Loss 0.018039    Objective Loss 0.018039                                        LR 0.000500    Time 0.074906    
Epoch: [117][  170/  207]    Overall Loss 0.018063    Objective Loss 0.018063                                        LR 0.000500    Time 0.074843    
Epoch: [117][  180/  207]    Overall Loss 0.018077    Objective Loss 0.018077                                        LR 0.000500    Time 0.074637    
Epoch: [117][  190/  207]    Overall Loss 0.018199    Objective Loss 0.018199                                        LR 0.000500    Time 0.074457    
Epoch: [117][  200/  207]    Overall Loss 0.018179    Objective Loss 0.018179                                        LR 0.000500    Time 0.074304    
Epoch: [117][  207/  207]    Overall Loss 0.018366    Objective Loss 0.018366    Top1 98.640997    Top5 100.000000    LR 0.000500    Time 0.074073    
--- validate (epoch=117)-----------
5136 samples (512 per mini-batch)
Epoch: [117][   10/   11]    Loss 0.544111    Top1 85.097656    Top5 99.785156    
Epoch: [117][   11/   11]    Loss 0.594990    Top1 85.085670    Top5 99.785826    
==> Top1: 85.086    Top5: 99.786    Loss: 0.595

==> Confusion:
[[279   3   3   3   0   0   1  11]
 [  2 274  17   1   0   2   0   4]
 [  4  20 270   1   0   2   0   3]
 [  0   3   0 760  37  12   3  22]
 [  2   0   0  58 764   5  17  33]
 [ 13  11  10  10  16 772   3  59]
 [  2   1   0   6  32  10 747  39]
 [ 26  19  19  48  64  93  16 504]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [118][   10/  207]    Overall Loss 0.019191    Objective Loss 0.019191                                        LR 0.000500    Time 0.131499    
Epoch: [118][   20/  207]    Overall Loss 0.022836    Objective Loss 0.022836                                        LR 0.000500    Time 0.101339    
Epoch: [118][   30/  207]    Overall Loss 0.025784    Objective Loss 0.025784                                        LR 0.000500    Time 0.091692    
Epoch: [118][   40/  207]    Overall Loss 0.029321    Objective Loss 0.029321                                        LR 0.000500    Time 0.086620    
Epoch: [118][   50/  207]    Overall Loss 0.032344    Objective Loss 0.032344                                        LR 0.000500    Time 0.083292    
Epoch: [118][   60/  207]    Overall Loss 0.033412    Objective Loss 0.033412                                        LR 0.000500    Time 0.081220    
Epoch: [118][   70/  207]    Overall Loss 0.034325    Objective Loss 0.034325                                        LR 0.000500    Time 0.080024    
Epoch: [118][   80/  207]    Overall Loss 0.035621    Objective Loss 0.035621                                        LR 0.000500    Time 0.079175    
Epoch: [118][   90/  207]    Overall Loss 0.038792    Objective Loss 0.038792                                        LR 0.000500    Time 0.078259    
Epoch: [118][  100/  207]    Overall Loss 0.042569    Objective Loss 0.042569                                        LR 0.000500    Time 0.077657    
Epoch: [118][  110/  207]    Overall Loss 0.049182    Objective Loss 0.049182                                        LR 0.000500    Time 0.076956    
Epoch: [118][  120/  207]    Overall Loss 0.056200    Objective Loss 0.056200                                        LR 0.000500    Time 0.076601    
Epoch: [118][  130/  207]    Overall Loss 0.061094    Objective Loss 0.061094                                        LR 0.000500    Time 0.076267    
Epoch: [118][  140/  207]    Overall Loss 0.065402    Objective Loss 0.065402                                        LR 0.000500    Time 0.075855    
Epoch: [118][  150/  207]    Overall Loss 0.068038    Objective Loss 0.068038                                        LR 0.000500    Time 0.075723    
Epoch: [118][  160/  207]    Overall Loss 0.069343    Objective Loss 0.069343                                        LR 0.000500    Time 0.075432    
Epoch: [118][  170/  207]    Overall Loss 0.070279    Objective Loss 0.070279                                        LR 0.000500    Time 0.075119    
Epoch: [118][  180/  207]    Overall Loss 0.070723    Objective Loss 0.070723                                        LR 0.000500    Time 0.074880    
Epoch: [118][  190/  207]    Overall Loss 0.070996    Objective Loss 0.070996                                        LR 0.000500    Time 0.074652    
Epoch: [118][  200/  207]    Overall Loss 0.070950    Objective Loss 0.070950                                        LR 0.000500    Time 0.074530    
Epoch: [118][  207/  207]    Overall Loss 0.070498    Objective Loss 0.070498    Top1 95.243488    Top5 99.886750    LR 0.000500    Time 0.074319    
--- validate (epoch=118)-----------
5136 samples (512 per mini-batch)
Epoch: [118][   10/   11]    Loss 0.619170    Top1 83.769531    Top5 99.667969    
Epoch: [118][   11/   11]    Loss 0.605031    Top1 83.761682    Top5 99.669003    
==> Top1: 83.762    Top5: 99.669    Loss: 0.605

==> Confusion:
[[255   5   3   2   3   1   0  31]
 [  1 266  19   0   0   2   0  12]
 [  5  26 261   1   0   1   0   6]
 [  0   6   0 717  77   6  10  21]
 [  2   0   0  27 803   5  21  21]
 [  4   7  12   8  29 765  16  53]
 [  1   1   0   4  35  12 763  21]
 [ 16  19  19  46  94  95  28 472]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [119][   10/  207]    Overall Loss 0.069225    Objective Loss 0.069225                                        LR 0.000500    Time 0.129256    
Epoch: [119][   20/  207]    Overall Loss 0.061807    Objective Loss 0.061807                                        LR 0.000500    Time 0.100648    
Epoch: [119][   30/  207]    Overall Loss 0.057883    Objective Loss 0.057883                                        LR 0.000500    Time 0.091141    
Epoch: [119][   40/  207]    Overall Loss 0.054197    Objective Loss 0.054197                                        LR 0.000500    Time 0.087073    
Epoch: [119][   50/  207]    Overall Loss 0.052062    Objective Loss 0.052062                                        LR 0.000500    Time 0.084599    
Epoch: [119][   60/  207]    Overall Loss 0.051988    Objective Loss 0.051988                                        LR 0.000500    Time 0.083010    
Epoch: [119][   70/  207]    Overall Loss 0.052163    Objective Loss 0.052163                                        LR 0.000500    Time 0.081698    
Epoch: [119][   80/  207]    Overall Loss 0.051741    Objective Loss 0.051741                                        LR 0.000500    Time 0.080604    
Epoch: [119][   90/  207]    Overall Loss 0.051442    Objective Loss 0.051442                                        LR 0.000500    Time 0.079676    
Epoch: [119][  100/  207]    Overall Loss 0.050468    Objective Loss 0.050468                                        LR 0.000500    Time 0.078915    
Epoch: [119][  110/  207]    Overall Loss 0.049070    Objective Loss 0.049070                                        LR 0.000500    Time 0.078286    
Epoch: [119][  120/  207]    Overall Loss 0.048015    Objective Loss 0.048015                                        LR 0.000500    Time 0.077997    
Epoch: [119][  130/  207]    Overall Loss 0.047107    Objective Loss 0.047107                                        LR 0.000500    Time 0.077651    
Epoch: [119][  140/  207]    Overall Loss 0.046174    Objective Loss 0.046174                                        LR 0.000500    Time 0.077184    
Epoch: [119][  150/  207]    Overall Loss 0.045033    Objective Loss 0.045033                                        LR 0.000500    Time 0.076725    
Epoch: [119][  160/  207]    Overall Loss 0.044219    Objective Loss 0.044219                                        LR 0.000500    Time 0.076405    
Epoch: [119][  170/  207]    Overall Loss 0.043608    Objective Loss 0.043608                                        LR 0.000500    Time 0.076191    
Epoch: [119][  180/  207]    Overall Loss 0.042834    Objective Loss 0.042834                                        LR 0.000500    Time 0.076053    
Epoch: [119][  190/  207]    Overall Loss 0.042169    Objective Loss 0.042169                                        LR 0.000500    Time 0.075872    
Epoch: [119][  200/  207]    Overall Loss 0.041573    Objective Loss 0.041573                                        LR 0.000500    Time 0.075643    
Epoch: [119][  207/  207]    Overall Loss 0.041219    Objective Loss 0.041219    Top1 96.489241    Top5 100.000000    LR 0.000500    Time 0.075375    
--- validate (epoch=119)-----------
5136 samples (512 per mini-batch)
Epoch: [119][   10/   11]    Loss 0.499717    Top1 84.570312    Top5 99.687500    
Epoch: [119][   11/   11]    Loss 0.456813    Top1 84.618380    Top5 99.688474    
==> Top1: 84.618    Top5: 99.688    Loss: 0.457

==> Confusion:
[[282   2   1   2   0   0   2  11]
 [  3 259  31   1   0   2   1   3]
 [  4  16 273   1   0   2   0   4]
 [  0   1   0 760  49   8   8  11]
 [  3   0   0  45 780   5  19  27]
 [ 13   7  10  12  24 782  19  27]
 [  3   0   0   6  31  11 770  16]
 [ 34  16  19  64  77 112  27 440]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [120][   10/  207]    Overall Loss 0.024724    Objective Loss 0.024724                                        LR 0.000500    Time 0.129868    
Epoch: [120][   20/  207]    Overall Loss 0.024356    Objective Loss 0.024356                                        LR 0.000500    Time 0.101546    
Epoch: [120][   30/  207]    Overall Loss 0.024304    Objective Loss 0.024304                                        LR 0.000500    Time 0.091967    
Epoch: [120][   40/  207]    Overall Loss 0.024136    Objective Loss 0.024136                                        LR 0.000500    Time 0.086466    
Epoch: [120][   50/  207]    Overall Loss 0.024037    Objective Loss 0.024037                                        LR 0.000500    Time 0.083931    
Epoch: [120][   60/  207]    Overall Loss 0.023705    Objective Loss 0.023705                                        LR 0.000500    Time 0.082102    
Epoch: [120][   70/  207]    Overall Loss 0.023881    Objective Loss 0.023881                                        LR 0.000500    Time 0.080921    
Epoch: [120][   80/  207]    Overall Loss 0.023786    Objective Loss 0.023786                                        LR 0.000500    Time 0.079970    
Epoch: [120][   90/  207]    Overall Loss 0.024020    Objective Loss 0.024020                                        LR 0.000500    Time 0.079233    
Epoch: [120][  100/  207]    Overall Loss 0.023883    Objective Loss 0.023883                                        LR 0.000500    Time 0.078419    
Epoch: [120][  110/  207]    Overall Loss 0.023700    Objective Loss 0.023700                                        LR 0.000500    Time 0.077768    
Epoch: [120][  120/  207]    Overall Loss 0.023680    Objective Loss 0.023680                                        LR 0.000500    Time 0.077172    
Epoch: [120][  130/  207]    Overall Loss 0.023671    Objective Loss 0.023671                                        LR 0.000500    Time 0.076860    
Epoch: [120][  140/  207]    Overall Loss 0.023509    Objective Loss 0.023509                                        LR 0.000500    Time 0.076447    
Epoch: [120][  150/  207]    Overall Loss 0.023267    Objective Loss 0.023267                                        LR 0.000500    Time 0.076105    
Epoch: [120][  160/  207]    Overall Loss 0.023087    Objective Loss 0.023087                                        LR 0.000500    Time 0.075797    
Epoch: [120][  170/  207]    Overall Loss 0.023125    Objective Loss 0.023125                                        LR 0.000500    Time 0.075692    
Epoch: [120][  180/  207]    Overall Loss 0.023043    Objective Loss 0.023043                                        LR 0.000500    Time 0.075398    
Epoch: [120][  190/  207]    Overall Loss 0.023238    Objective Loss 0.023238                                        LR 0.000500    Time 0.075211    
Epoch: [120][  200/  207]    Overall Loss 0.023234    Objective Loss 0.023234                                        LR 0.000500    Time 0.075018    
Epoch: [120][  207/  207]    Overall Loss 0.023130    Objective Loss 0.023130    Top1 97.848245    Top5 100.000000    LR 0.000500    Time 0.074741    
--- validate (epoch=120)-----------
5136 samples (512 per mini-batch)
Epoch: [120][   10/   11]    Loss 0.595329    Top1 85.273438    Top5 99.765625    
Epoch: [120][   11/   11]    Loss 0.591860    Top1 85.260903    Top5 99.766355    
==> Top1: 85.261    Top5: 99.766    Loss: 0.592

==> Confusion:
[[268   2   2   3   2   0   2  21]
 [  1 262  23   0   0   1   1  12]
 [  3  15 270   1   0   4   0   7]
 [  0   0   0 751  46  12   8  20]
 [  2   0   0  47 766   7  22  35]
 [  4   7   8  14  16 775  10  60]
 [  1   0   0   6  25   6 770  29]
 [ 13   8  13  42  67 104  25 517]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [121][   10/  207]    Overall Loss 0.015519    Objective Loss 0.015519                                        LR 0.000500    Time 0.130391    
Epoch: [121][   20/  207]    Overall Loss 0.017638    Objective Loss 0.017638                                        LR 0.000500    Time 0.100073    
Epoch: [121][   30/  207]    Overall Loss 0.017649    Objective Loss 0.017649                                        LR 0.000500    Time 0.090237    
Epoch: [121][   40/  207]    Overall Loss 0.016899    Objective Loss 0.016899                                        LR 0.000500    Time 0.085817    
Epoch: [121][   50/  207]    Overall Loss 0.016871    Objective Loss 0.016871                                        LR 0.000500    Time 0.083192    
Epoch: [121][   60/  207]    Overall Loss 0.017192    Objective Loss 0.017192                                        LR 0.000500    Time 0.081238    
Epoch: [121][   70/  207]    Overall Loss 0.017514    Objective Loss 0.017514                                        LR 0.000500    Time 0.079846    
Epoch: [121][   80/  207]    Overall Loss 0.018033    Objective Loss 0.018033                                        LR 0.000500    Time 0.078734    
Epoch: [121][   90/  207]    Overall Loss 0.017975    Objective Loss 0.017975                                        LR 0.000500    Time 0.078115    
Epoch: [121][  100/  207]    Overall Loss 0.017944    Objective Loss 0.017944                                        LR 0.000500    Time 0.077494    
Epoch: [121][  110/  207]    Overall Loss 0.017863    Objective Loss 0.017863                                        LR 0.000500    Time 0.076911    
Epoch: [121][  120/  207]    Overall Loss 0.018061    Objective Loss 0.018061                                        LR 0.000500    Time 0.076697    
Epoch: [121][  130/  207]    Overall Loss 0.018335    Objective Loss 0.018335                                        LR 0.000500    Time 0.076159    
Epoch: [121][  140/  207]    Overall Loss 0.018449    Objective Loss 0.018449                                        LR 0.000500    Time 0.075771    
Epoch: [121][  150/  207]    Overall Loss 0.018575    Objective Loss 0.018575                                        LR 0.000500    Time 0.075596    
Epoch: [121][  160/  207]    Overall Loss 0.018527    Objective Loss 0.018527                                        LR 0.000500    Time 0.075231    
Epoch: [121][  170/  207]    Overall Loss 0.018675    Objective Loss 0.018675                                        LR 0.000500    Time 0.074928    
Epoch: [121][  180/  207]    Overall Loss 0.018819    Objective Loss 0.018819                                        LR 0.000500    Time 0.074754    
Epoch: [121][  190/  207]    Overall Loss 0.018963    Objective Loss 0.018963                                        LR 0.000500    Time 0.074638    
Epoch: [121][  200/  207]    Overall Loss 0.018997    Objective Loss 0.018997                                        LR 0.000500    Time 0.074426    
Epoch: [121][  207/  207]    Overall Loss 0.018981    Objective Loss 0.018981    Top1 97.734994    Top5 100.000000    LR 0.000500    Time 0.074201    
--- validate (epoch=121)-----------
5136 samples (512 per mini-batch)
Epoch: [121][   10/   11]    Loss 0.554290    Top1 85.488281    Top5 99.785156    
Epoch: [121][   11/   11]    Loss 0.558150    Top1 85.455607    Top5 99.785826    
==> Top1: 85.456    Top5: 99.786    Loss: 0.558

==> Confusion:
[[273   2   2   3   1   1   2  16]
 [  1 264  21   0   0   2   1  11]
 [  2  15 274   1   0   2   0   6]
 [  0   1   0 750  44  12   8  22]
 [  2   0   0  43 767   9  23  35]
 [  7   6   6   9  15 794   9  48]
 [  1   0   0   5  22  13 759  37]
 [ 20  13  13  40  61 112  22 508]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [122][   10/  207]    Overall Loss 0.017069    Objective Loss 0.017069                                        LR 0.000500    Time 0.127819    
Epoch: [122][   20/  207]    Overall Loss 0.017772    Objective Loss 0.017772                                        LR 0.000500    Time 0.099027    
Epoch: [122][   30/  207]    Overall Loss 0.017993    Objective Loss 0.017993                                        LR 0.000500    Time 0.090156    
Epoch: [122][   40/  207]    Overall Loss 0.017893    Objective Loss 0.017893                                        LR 0.000500    Time 0.086190    
Epoch: [122][   50/  207]    Overall Loss 0.017595    Objective Loss 0.017595                                        LR 0.000500    Time 0.083918    
Epoch: [122][   60/  207]    Overall Loss 0.017350    Objective Loss 0.017350                                        LR 0.000500    Time 0.081806    
Epoch: [122][   70/  207]    Overall Loss 0.017115    Objective Loss 0.017115                                        LR 0.000500    Time 0.080168    
Epoch: [122][   80/  207]    Overall Loss 0.017059    Objective Loss 0.017059                                        LR 0.000500    Time 0.079210    
Epoch: [122][   90/  207]    Overall Loss 0.017067    Objective Loss 0.017067                                        LR 0.000500    Time 0.078401    
Epoch: [122][  100/  207]    Overall Loss 0.016890    Objective Loss 0.016890                                        LR 0.000500    Time 0.077746    
Epoch: [122][  110/  207]    Overall Loss 0.017056    Objective Loss 0.017056                                        LR 0.000500    Time 0.077168    
Epoch: [122][  120/  207]    Overall Loss 0.017218    Objective Loss 0.017218                                        LR 0.000500    Time 0.076868    
Epoch: [122][  130/  207]    Overall Loss 0.017437    Objective Loss 0.017437                                        LR 0.000500    Time 0.076336    
Epoch: [122][  140/  207]    Overall Loss 0.017409    Objective Loss 0.017409                                        LR 0.000500    Time 0.075934    
Epoch: [122][  150/  207]    Overall Loss 0.017445    Objective Loss 0.017445                                        LR 0.000500    Time 0.075784    
Epoch: [122][  160/  207]    Overall Loss 0.017464    Objective Loss 0.017464                                        LR 0.000500    Time 0.075552    
Epoch: [122][  170/  207]    Overall Loss 0.017587    Objective Loss 0.017587                                        LR 0.000500    Time 0.075401    
Epoch: [122][  180/  207]    Overall Loss 0.017771    Objective Loss 0.017771                                        LR 0.000500    Time 0.075241    
Epoch: [122][  190/  207]    Overall Loss 0.017792    Objective Loss 0.017792                                        LR 0.000500    Time 0.075885    
Epoch: [122][  200/  207]    Overall Loss 0.017694    Objective Loss 0.017694                                        LR 0.000500    Time 0.075679    
Epoch: [122][  207/  207]    Overall Loss 0.017691    Objective Loss 0.017691    Top1 98.074745    Top5 100.000000    LR 0.000500    Time 0.075401    
--- validate (epoch=122)-----------
5136 samples (512 per mini-batch)
Epoch: [122][   10/   11]    Loss 0.548084    Top1 85.566406    Top5 99.726562    
Epoch: [122][   11/   11]    Loss 0.566306    Top1 85.533489    Top5 99.727414    
==> Top1: 85.533    Top5: 99.727    Loss: 0.566

==> Confusion:
[[272   2   3   3   1   1   3  15]
 [  1 262  21   0   0   4   1  11]
 [  1  17 275   1   0   3   0   3]
 [  0   0   0 752  50  11   6  18]
 [  1   0   0  41 783   8  15  31]
 [  4   6   9  11  18 795  10  41]
 [  1   0   0   5  27  13 764  27]
 [ 17  13  15  44  75 116  19 490]]

==> Best [Top1: 85.631   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 113]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [123][   10/  207]    Overall Loss 0.015721    Objective Loss 0.015721                                        LR 0.000500    Time 0.130517    
Epoch: [123][   20/  207]    Overall Loss 0.016201    Objective Loss 0.016201                                        LR 0.000500    Time 0.100836    
Epoch: [123][   30/  207]    Overall Loss 0.015201    Objective Loss 0.015201                                        LR 0.000500    Time 0.091241    
Epoch: [123][   40/  207]    Overall Loss 0.015523    Objective Loss 0.015523                                        LR 0.000500    Time 0.085940    
Epoch: [123][   50/  207]    Overall Loss 0.016111    Objective Loss 0.016111                                        LR 0.000500    Time 0.083287    
Epoch: [123][   60/  207]    Overall Loss 0.016690    Objective Loss 0.016690                                        LR 0.000500    Time 0.081237    
Epoch: [123][   70/  207]    Overall Loss 0.016825    Objective Loss 0.016825                                        LR 0.000500    Time 0.079941    
Epoch: [123][   80/  207]    Overall Loss 0.017090    Objective Loss 0.017090                                        LR 0.000500    Time 0.079089    
Epoch: [123][   90/  207]    Overall Loss 0.016993    Objective Loss 0.016993                                        LR 0.000500    Time 0.078403    
Epoch: [123][  100/  207]    Overall Loss 0.017129    Objective Loss 0.017129                                        LR 0.000500    Time 0.077731    
Epoch: [123][  110/  207]    Overall Loss 0.017145    Objective Loss 0.017145                                        LR 0.000500    Time 0.077411    
Epoch: [123][  120/  207]    Overall Loss 0.017197    Objective Loss 0.017197                                        LR 0.000500    Time 0.076957    
Epoch: [123][  130/  207]    Overall Loss 0.017544    Objective Loss 0.017544                                        LR 0.000500    Time 0.076434    
Epoch: [123][  140/  207]    Overall Loss 0.017539    Objective Loss 0.017539                                        LR 0.000500    Time 0.076096    
Epoch: [123][  150/  207]    Overall Loss 0.017750    Objective Loss 0.017750                                        LR 0.000500    Time 0.075719    
Epoch: [123][  160/  207]    Overall Loss 0.017978    Objective Loss 0.017978                                        LR 0.000500    Time 0.075510    
Epoch: [123][  170/  207]    Overall Loss 0.017945    Objective Loss 0.017945                                        LR 0.000500    Time 0.075199    
Epoch: [123][  180/  207]    Overall Loss 0.017842    Objective Loss 0.017842                                        LR 0.000500    Time 0.074956    
Epoch: [123][  190/  207]    Overall Loss 0.017690    Objective Loss 0.017690                                        LR 0.000500    Time 0.074778    
Epoch: [123][  200/  207]    Overall Loss 0.017680    Objective Loss 0.017680                                        LR 0.000500    Time 0.074616    
Epoch: [123][  207/  207]    Overall Loss 0.017788    Objective Loss 0.017788    Top1 97.961495    Top5 100.000000    LR 0.000500    Time 0.074386    
--- validate (epoch=123)-----------
5136 samples (512 per mini-batch)
Epoch: [123][   10/   11]    Loss 0.553180    Top1 85.800781    Top5 99.765625    
Epoch: [123][   11/   11]    Loss 0.682606    Top1 85.767134    Top5 99.766355    
==> Top1: 85.767    Top5: 99.766    Loss: 0.683

==> Confusion:
[[276   1   2   3   1   0   2  15]
 [  1 261  24   0   0   2   1  11]
 [  3  17 269   1   0   3   0   7]
 [  0   1   1 752  50   8   9  16]
 [  2   0   0  46 766   4  27  34]
 [  5   4   6  11  13 800  14  41]
 [  1   0   0   4  25  12 774  21]
 [ 20   9  16  40  65 105  27 507]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [124][   10/  207]    Overall Loss 0.014194    Objective Loss 0.014194                                        LR 0.000500    Time 0.128968    
Epoch: [124][   20/  207]    Overall Loss 0.014893    Objective Loss 0.014893                                        LR 0.000500    Time 0.100719    
Epoch: [124][   30/  207]    Overall Loss 0.014713    Objective Loss 0.014713                                        LR 0.000500    Time 0.091151    
Epoch: [124][   40/  207]    Overall Loss 0.015213    Objective Loss 0.015213                                        LR 0.000500    Time 0.086068    
Epoch: [124][   50/  207]    Overall Loss 0.015272    Objective Loss 0.015272                                        LR 0.000500    Time 0.083069    
Epoch: [124][   60/  207]    Overall Loss 0.015764    Objective Loss 0.015764                                        LR 0.000500    Time 0.081140    
Epoch: [124][   70/  207]    Overall Loss 0.016164    Objective Loss 0.016164                                        LR 0.000500    Time 0.079744    
Epoch: [124][   80/  207]    Overall Loss 0.016619    Objective Loss 0.016619                                        LR 0.000500    Time 0.078776    
Epoch: [124][   90/  207]    Overall Loss 0.016898    Objective Loss 0.016898                                        LR 0.000500    Time 0.078107    
Epoch: [124][  100/  207]    Overall Loss 0.016986    Objective Loss 0.016986                                        LR 0.000500    Time 0.077568    
Epoch: [124][  110/  207]    Overall Loss 0.016957    Objective Loss 0.016957                                        LR 0.000500    Time 0.077114    
Epoch: [124][  120/  207]    Overall Loss 0.017130    Objective Loss 0.017130                                        LR 0.000500    Time 0.076622    
Epoch: [124][  130/  207]    Overall Loss 0.017763    Objective Loss 0.017763                                        LR 0.000500    Time 0.076271    
Epoch: [124][  140/  207]    Overall Loss 0.018251    Objective Loss 0.018251                                        LR 0.000500    Time 0.075931    
Epoch: [124][  150/  207]    Overall Loss 0.018602    Objective Loss 0.018602                                        LR 0.000500    Time 0.075622    
Epoch: [124][  160/  207]    Overall Loss 0.018709    Objective Loss 0.018709                                        LR 0.000500    Time 0.075388    
Epoch: [124][  170/  207]    Overall Loss 0.018874    Objective Loss 0.018874                                        LR 0.000500    Time 0.075104    
Epoch: [124][  180/  207]    Overall Loss 0.018890    Objective Loss 0.018890                                        LR 0.000500    Time 0.074834    
Epoch: [124][  190/  207]    Overall Loss 0.018992    Objective Loss 0.018992                                        LR 0.000500    Time 0.074677    
Epoch: [124][  200/  207]    Overall Loss 0.018949    Objective Loss 0.018949                                        LR 0.000500    Time 0.074635    
Epoch: [124][  207/  207]    Overall Loss 0.018943    Objective Loss 0.018943    Top1 98.754247    Top5 100.000000    LR 0.000500    Time 0.074404    
--- validate (epoch=124)-----------
5136 samples (512 per mini-batch)
Epoch: [124][   10/   11]    Loss 0.570117    Top1 84.843750    Top5 99.726562    
Epoch: [124][   11/   11]    Loss 0.537966    Top1 84.852025    Top5 99.727414    
==> Top1: 84.852    Top5: 99.727    Loss: 0.538

==> Confusion:
[[274   2   3   2   2   0   1  16]
 [  1 260  26   0   0   1   1  11]
 [  2  16 276   1   0   2   0   3]
 [  0   3   1 744  52   7  10  20]
 [  2   0   0  47 770   3  25  32]
 [  8   7  11  11  23 765  13  56]
 [  1   0   0   5  30   6 767  28]
 [ 21  14  15  43  79  92  23 502]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [125][   10/  207]    Overall Loss 0.016670    Objective Loss 0.016670                                        LR 0.000500    Time 0.131466    
Epoch: [125][   20/  207]    Overall Loss 0.016558    Objective Loss 0.016558                                        LR 0.000500    Time 0.101335    
Epoch: [125][   30/  207]    Overall Loss 0.015913    Objective Loss 0.015913                                        LR 0.000500    Time 0.091452    
Epoch: [125][   40/  207]    Overall Loss 0.016474    Objective Loss 0.016474                                        LR 0.000500    Time 0.087055    
Epoch: [125][   50/  207]    Overall Loss 0.016304    Objective Loss 0.016304                                        LR 0.000500    Time 0.084438    
Epoch: [125][   60/  207]    Overall Loss 0.016276    Objective Loss 0.016276                                        LR 0.000500    Time 0.082385    
Epoch: [125][   70/  207]    Overall Loss 0.016739    Objective Loss 0.016739                                        LR 0.000500    Time 0.080650    
Epoch: [125][   80/  207]    Overall Loss 0.016977    Objective Loss 0.016977                                        LR 0.000500    Time 0.079584    
Epoch: [125][   90/  207]    Overall Loss 0.017269    Objective Loss 0.017269                                        LR 0.000500    Time 0.078847    
Epoch: [125][  100/  207]    Overall Loss 0.017348    Objective Loss 0.017348                                        LR 0.000500    Time 0.078130    
Epoch: [125][  110/  207]    Overall Loss 0.017713    Objective Loss 0.017713                                        LR 0.000500    Time 0.077391    
Epoch: [125][  120/  207]    Overall Loss 0.017792    Objective Loss 0.017792                                        LR 0.000500    Time 0.076877    
Epoch: [125][  130/  207]    Overall Loss 0.018052    Objective Loss 0.018052                                        LR 0.000500    Time 0.076507    
Epoch: [125][  140/  207]    Overall Loss 0.018177    Objective Loss 0.018177                                        LR 0.000500    Time 0.076153    
Epoch: [125][  150/  207]    Overall Loss 0.018541    Objective Loss 0.018541                                        LR 0.000500    Time 0.075844    
Epoch: [125][  160/  207]    Overall Loss 0.018894    Objective Loss 0.018894                                        LR 0.000500    Time 0.075492    
Epoch: [125][  170/  207]    Overall Loss 0.019050    Objective Loss 0.019050                                        LR 0.000500    Time 0.075213    
Epoch: [125][  180/  207]    Overall Loss 0.019556    Objective Loss 0.019556                                        LR 0.000500    Time 0.075054    
Epoch: [125][  190/  207]    Overall Loss 0.020158    Objective Loss 0.020158                                        LR 0.000500    Time 0.074811    
Epoch: [125][  200/  207]    Overall Loss 0.020294    Objective Loss 0.020294                                        LR 0.000500    Time 0.074616    
Epoch: [125][  207/  207]    Overall Loss 0.020503    Objective Loss 0.020503    Top1 98.527746    Top5 100.000000    LR 0.000500    Time 0.074381    
--- validate (epoch=125)-----------
5136 samples (512 per mini-batch)
Epoch: [125][   10/   11]    Loss 0.576024    Top1 84.941406    Top5 99.707031    
Epoch: [125][   11/   11]    Loss 0.587152    Top1 84.890966    Top5 99.707944    
==> Top1: 84.891    Top5: 99.708    Loss: 0.587

==> Confusion:
[[274   1   3   1   1   1   2  17]
 [  1 262  20   0   0   2   1  14]
 [  2  19 268   1   0   3   0   7]
 [  0   2   1 748  44  11  11  20]
 [  2   0   0  56 757   5  23  36]
 [  4  10   7   7  16 774  18  58]
 [  1   0   0   6  26  10 769  25]
 [ 14  10  15  46  60 100  36 508]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [126][   10/  207]    Overall Loss 0.023137    Objective Loss 0.023137                                        LR 0.000500    Time 0.130819    
Epoch: [126][   20/  207]    Overall Loss 0.023708    Objective Loss 0.023708                                        LR 0.000500    Time 0.100715    
Epoch: [126][   30/  207]    Overall Loss 0.024534    Objective Loss 0.024534                                        LR 0.000500    Time 0.090692    
Epoch: [126][   40/  207]    Overall Loss 0.025751    Objective Loss 0.025751                                        LR 0.000500    Time 0.086109    
Epoch: [126][   50/  207]    Overall Loss 0.025473    Objective Loss 0.025473                                        LR 0.000500    Time 0.083019    
Epoch: [126][   60/  207]    Overall Loss 0.026018    Objective Loss 0.026018                                        LR 0.000500    Time 0.080858    
Epoch: [126][   70/  207]    Overall Loss 0.026397    Objective Loss 0.026397                                        LR 0.000500    Time 0.079704    
Epoch: [126][   80/  207]    Overall Loss 0.026798    Objective Loss 0.026798                                        LR 0.000500    Time 0.078575    
Epoch: [126][   90/  207]    Overall Loss 0.028716    Objective Loss 0.028716                                        LR 0.000500    Time 0.077833    
Epoch: [126][  100/  207]    Overall Loss 0.028916    Objective Loss 0.028916                                        LR 0.000500    Time 0.077158    
Epoch: [126][  110/  207]    Overall Loss 0.029446    Objective Loss 0.029446                                        LR 0.000500    Time 0.076600    
Epoch: [126][  120/  207]    Overall Loss 0.030137    Objective Loss 0.030137                                        LR 0.000500    Time 0.076056    
Epoch: [126][  130/  207]    Overall Loss 0.030876    Objective Loss 0.030876                                        LR 0.000500    Time 0.075640    
Epoch: [126][  140/  207]    Overall Loss 0.031510    Objective Loss 0.031510                                        LR 0.000500    Time 0.075325    
Epoch: [126][  150/  207]    Overall Loss 0.032380    Objective Loss 0.032380                                        LR 0.000500    Time 0.075205    
Epoch: [126][  160/  207]    Overall Loss 0.032676    Objective Loss 0.032676                                        LR 0.000500    Time 0.075056    
Epoch: [126][  170/  207]    Overall Loss 0.033314    Objective Loss 0.033314                                        LR 0.000500    Time 0.074905    
Epoch: [126][  180/  207]    Overall Loss 0.034658    Objective Loss 0.034658                                        LR 0.000500    Time 0.074664    
Epoch: [126][  190/  207]    Overall Loss 0.036498    Objective Loss 0.036498                                        LR 0.000500    Time 0.074556    
Epoch: [126][  200/  207]    Overall Loss 0.038748    Objective Loss 0.038748                                        LR 0.000500    Time 0.074384    
Epoch: [126][  207/  207]    Overall Loss 0.041528    Objective Loss 0.041528    Top1 93.771234    Top5 100.000000    LR 0.000500    Time 0.074164    
--- validate (epoch=126)-----------
5136 samples (512 per mini-batch)
Epoch: [126][   10/   11]    Loss 0.838211    Top1 80.957031    Top5 99.589844    
Epoch: [126][   11/   11]    Loss 0.810535    Top1 80.938474    Top5 99.591121    
==> Top1: 80.938    Top5: 99.591    Loss: 0.811

==> Confusion:
[[231   4   1   3   4  17   3  37]
 [  2 242  21  11   0   9   1  14]
 [  2  18 213   4   0  42   0  21]
 [  0   0   0 775  44  11   4   3]
 [  0   0   0  78 766   4  14  17]
 [  0   4   0  21  30 811  16  12]
 [  0   0   0  12  48  12 751  14]
 [  6   7   8  94  88 182  36 368]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [127][   10/  207]    Overall Loss 0.102316    Objective Loss 0.102316                                        LR 0.000500    Time 0.130656    
Epoch: [127][   20/  207]    Overall Loss 0.103592    Objective Loss 0.103592                                        LR 0.000500    Time 0.100951    
Epoch: [127][   30/  207]    Overall Loss 0.094925    Objective Loss 0.094925                                        LR 0.000500    Time 0.090927    
Epoch: [127][   40/  207]    Overall Loss 0.091158    Objective Loss 0.091158                                        LR 0.000500    Time 0.086412    
Epoch: [127][   50/  207]    Overall Loss 0.085819    Objective Loss 0.085819                                        LR 0.000500    Time 0.083165    
Epoch: [127][   60/  207]    Overall Loss 0.080725    Objective Loss 0.080725                                        LR 0.000500    Time 0.081448    
Epoch: [127][   70/  207]    Overall Loss 0.076353    Objective Loss 0.076353                                        LR 0.000500    Time 0.080192    
Epoch: [127][   80/  207]    Overall Loss 0.072633    Objective Loss 0.072633                                        LR 0.000500    Time 0.079396    
Epoch: [127][   90/  207]    Overall Loss 0.070830    Objective Loss 0.070830                                        LR 0.000500    Time 0.078609    
Epoch: [127][  100/  207]    Overall Loss 0.068917    Objective Loss 0.068917                                        LR 0.000500    Time 0.078038    
Epoch: [127][  110/  207]    Overall Loss 0.068025    Objective Loss 0.068025                                        LR 0.000500    Time 0.077478    
Epoch: [127][  120/  207]    Overall Loss 0.066625    Objective Loss 0.066625                                        LR 0.000500    Time 0.077080    
Epoch: [127][  130/  207]    Overall Loss 0.065104    Objective Loss 0.065104                                        LR 0.000500    Time 0.076683    
Epoch: [127][  140/  207]    Overall Loss 0.063837    Objective Loss 0.063837                                        LR 0.000500    Time 0.076271    
Epoch: [127][  150/  207]    Overall Loss 0.062194    Objective Loss 0.062194                                        LR 0.000500    Time 0.075916    
Epoch: [127][  160/  207]    Overall Loss 0.060937    Objective Loss 0.060937                                        LR 0.000500    Time 0.075671    
Epoch: [127][  170/  207]    Overall Loss 0.059531    Objective Loss 0.059531                                        LR 0.000500    Time 0.075476    
Epoch: [127][  180/  207]    Overall Loss 0.058155    Objective Loss 0.058155                                        LR 0.000500    Time 0.075257    
Epoch: [127][  190/  207]    Overall Loss 0.056889    Objective Loss 0.056889                                        LR 0.000500    Time 0.075017    
Epoch: [127][  200/  207]    Overall Loss 0.055929    Objective Loss 0.055929                                        LR 0.000500    Time 0.074780    
Epoch: [127][  207/  207]    Overall Loss 0.055134    Objective Loss 0.055134    Top1 97.395243    Top5 100.000000    LR 0.000500    Time 0.074541    
--- validate (epoch=127)-----------
5136 samples (512 per mini-batch)
Epoch: [127][   10/   11]    Loss 0.577155    Top1 84.531250    Top5 99.765625    
Epoch: [127][   11/   11]    Loss 0.557597    Top1 84.521028    Top5 99.766355    
==> Top1: 84.521    Top5: 99.766    Loss: 0.558

==> Confusion:
[[268   2   1   2   2   1   3  21]
 [  1 256  24   2   0   4   1  12]
 [  4  19 260   0   0   5   0  12]
 [  0   2   1 761  40   9   7  17]
 [  1   0   0  56 767   3  24  28]
 [  3   6   6  12  21 779  18  49]
 [  1   1   0   6  33   5 763  28]
 [ 19   8  10  61  62 110  32 487]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [128][   10/  207]    Overall Loss 0.024052    Objective Loss 0.024052                                        LR 0.000500    Time 0.135715    
Epoch: [128][   20/  207]    Overall Loss 0.025299    Objective Loss 0.025299                                        LR 0.000500    Time 0.103530    
Epoch: [128][   30/  207]    Overall Loss 0.025133    Objective Loss 0.025133                                        LR 0.000500    Time 0.092765    
Epoch: [128][   40/  207]    Overall Loss 0.025728    Objective Loss 0.025728                                        LR 0.000500    Time 0.087692    
Epoch: [128][   50/  207]    Overall Loss 0.025356    Objective Loss 0.025356                                        LR 0.000500    Time 0.084615    
Epoch: [128][   60/  207]    Overall Loss 0.025169    Objective Loss 0.025169                                        LR 0.000500    Time 0.082464    
Epoch: [128][   70/  207]    Overall Loss 0.025838    Objective Loss 0.025838                                        LR 0.000500    Time 0.080835    
Epoch: [128][   80/  207]    Overall Loss 0.026023    Objective Loss 0.026023                                        LR 0.000500    Time 0.080155    
Epoch: [128][   90/  207]    Overall Loss 0.026038    Objective Loss 0.026038                                        LR 0.000500    Time 0.079148    
Epoch: [128][  100/  207]    Overall Loss 0.026066    Objective Loss 0.026066                                        LR 0.000500    Time 0.078317    
Epoch: [128][  110/  207]    Overall Loss 0.026051    Objective Loss 0.026051                                        LR 0.000500    Time 0.077966    
Epoch: [128][  120/  207]    Overall Loss 0.026113    Objective Loss 0.026113                                        LR 0.000500    Time 0.077388    
Epoch: [128][  130/  207]    Overall Loss 0.026038    Objective Loss 0.026038                                        LR 0.000500    Time 0.076978    
Epoch: [128][  140/  207]    Overall Loss 0.026129    Objective Loss 0.026129                                        LR 0.000500    Time 0.076712    
Epoch: [128][  150/  207]    Overall Loss 0.026606    Objective Loss 0.026606                                        LR 0.000500    Time 0.076495    
Epoch: [128][  160/  207]    Overall Loss 0.027620    Objective Loss 0.027620                                        LR 0.000500    Time 0.076205    
Epoch: [128][  170/  207]    Overall Loss 0.028806    Objective Loss 0.028806                                        LR 0.000500    Time 0.075885    
Epoch: [128][  180/  207]    Overall Loss 0.029079    Objective Loss 0.029079                                        LR 0.000500    Time 0.075591    
Epoch: [128][  190/  207]    Overall Loss 0.029965    Objective Loss 0.029965                                        LR 0.000500    Time 0.075339    
Epoch: [128][  200/  207]    Overall Loss 0.030294    Objective Loss 0.030294                                        LR 0.000500    Time 0.075089    
Epoch: [128][  207/  207]    Overall Loss 0.030216    Objective Loss 0.030216    Top1 97.395243    Top5 100.000000    LR 0.000500    Time 0.074861    
--- validate (epoch=128)-----------
5136 samples (512 per mini-batch)
Epoch: [128][   10/   11]    Loss 0.542185    Top1 84.082031    Top5 99.785156    
Epoch: [128][   11/   11]    Loss 0.683101    Top1 84.053738    Top5 99.785826    
==> Top1: 84.054    Top5: 99.786    Loss: 0.683

==> Confusion:
[[276   2   3   2   2   1   2  12]
 [  1 255  31   1   0   2   0  10]
 [  5  15 272   0   0   2   0   6]
 [  0   2   1 749  58   6   8  13]
 [  2   0   0  48 790   4  14  21]
 [ 14   8  13  11  25 770  15  38]
 [  1   0   0   5  38   7 761  25]
 [ 37  20  20  61  77 103  27 444]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [129][   10/  207]    Overall Loss 0.028840    Objective Loss 0.028840                                        LR 0.000500    Time 0.129908    
Epoch: [129][   20/  207]    Overall Loss 0.027881    Objective Loss 0.027881                                        LR 0.000500    Time 0.100889    
Epoch: [129][   30/  207]    Overall Loss 0.026456    Objective Loss 0.026456                                        LR 0.000500    Time 0.090929    
Epoch: [129][   40/  207]    Overall Loss 0.026906    Objective Loss 0.026906                                        LR 0.000500    Time 0.085764    
Epoch: [129][   50/  207]    Overall Loss 0.027067    Objective Loss 0.027067                                        LR 0.000500    Time 0.083130    
Epoch: [129][   60/  207]    Overall Loss 0.026487    Objective Loss 0.026487                                        LR 0.000500    Time 0.080985    
Epoch: [129][   70/  207]    Overall Loss 0.026072    Objective Loss 0.026072                                        LR 0.000500    Time 0.079664    
Epoch: [129][   80/  207]    Overall Loss 0.025631    Objective Loss 0.025631                                        LR 0.000500    Time 0.078661    
Epoch: [129][   90/  207]    Overall Loss 0.025858    Objective Loss 0.025858                                        LR 0.000500    Time 0.077770    
Epoch: [129][  100/  207]    Overall Loss 0.026702    Objective Loss 0.026702                                        LR 0.000500    Time 0.077169    
Epoch: [129][  110/  207]    Overall Loss 0.027224    Objective Loss 0.027224                                        LR 0.000500    Time 0.076800    
Epoch: [129][  120/  207]    Overall Loss 0.028620    Objective Loss 0.028620                                        LR 0.000500    Time 0.076361    
Epoch: [129][  130/  207]    Overall Loss 0.030525    Objective Loss 0.030525                                        LR 0.000500    Time 0.075863    
Epoch: [129][  140/  207]    Overall Loss 0.032315    Objective Loss 0.032315                                        LR 0.000500    Time 0.075470    
Epoch: [129][  150/  207]    Overall Loss 0.034279    Objective Loss 0.034279                                        LR 0.000500    Time 0.075212    
Epoch: [129][  160/  207]    Overall Loss 0.034867    Objective Loss 0.034867                                        LR 0.000500    Time 0.074905    
Epoch: [129][  170/  207]    Overall Loss 0.035801    Objective Loss 0.035801                                        LR 0.000500    Time 0.074749    
Epoch: [129][  180/  207]    Overall Loss 0.036014    Objective Loss 0.036014                                        LR 0.000500    Time 0.074504    
Epoch: [129][  190/  207]    Overall Loss 0.036642    Objective Loss 0.036642                                        LR 0.000500    Time 0.074303    
Epoch: [129][  200/  207]    Overall Loss 0.036839    Objective Loss 0.036839                                        LR 0.000500    Time 0.074191    
Epoch: [129][  207/  207]    Overall Loss 0.036806    Objective Loss 0.036806    Top1 97.508494    Top5 100.000000    LR 0.000500    Time 0.073954    
--- validate (epoch=129)-----------
5136 samples (512 per mini-batch)
Epoch: [129][   10/   11]    Loss 0.598821    Top1 84.511719    Top5 99.667969    
Epoch: [129][   11/   11]    Loss 0.559025    Top1 84.521028    Top5 99.669003    
==> Top1: 84.521    Top5: 99.669    Loss: 0.559

==> Confusion:
[[263   2   2   4   3   3   3  20]
 [  3 244  30   2   0   5   1  15]
 [  2  15 261   1   0   8   0  13]
 [  0   0   0 756  51  12   6  12]
 [  0   0   0  49 773   7  26  24]
 [  3   4   3   6  21 825  17  15]
 [  1   0   0   5  30  11 767  23]
 [ 14   9   9  48  68 153  36 452]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [130][   10/  207]    Overall Loss 0.031656    Objective Loss 0.031656                                        LR 0.000500    Time 0.131591    
Epoch: [130][   20/  207]    Overall Loss 0.028956    Objective Loss 0.028956                                        LR 0.000500    Time 0.102094    
Epoch: [130][   30/  207]    Overall Loss 0.027239    Objective Loss 0.027239                                        LR 0.000500    Time 0.091979    
Epoch: [130][   40/  207]    Overall Loss 0.026770    Objective Loss 0.026770                                        LR 0.000500    Time 0.087286    
Epoch: [130][   50/  207]    Overall Loss 0.027962    Objective Loss 0.027962                                        LR 0.000500    Time 0.084135    
Epoch: [130][   60/  207]    Overall Loss 0.027201    Objective Loss 0.027201                                        LR 0.000500    Time 0.081951    
Epoch: [130][   70/  207]    Overall Loss 0.027000    Objective Loss 0.027000                                        LR 0.000500    Time 0.080342    
Epoch: [130][   80/  207]    Overall Loss 0.026469    Objective Loss 0.026469                                        LR 0.000500    Time 0.079085    
Epoch: [130][   90/  207]    Overall Loss 0.026064    Objective Loss 0.026064                                        LR 0.000500    Time 0.078090    
Epoch: [130][  100/  207]    Overall Loss 0.026077    Objective Loss 0.026077                                        LR 0.000500    Time 0.077449    
Epoch: [130][  110/  207]    Overall Loss 0.025900    Objective Loss 0.025900                                        LR 0.000500    Time 0.076932    
Epoch: [130][  120/  207]    Overall Loss 0.025683    Objective Loss 0.025683                                        LR 0.000500    Time 0.076401    
Epoch: [130][  130/  207]    Overall Loss 0.025524    Objective Loss 0.025524                                        LR 0.000500    Time 0.075967    
Epoch: [130][  140/  207]    Overall Loss 0.026303    Objective Loss 0.026303                                        LR 0.000500    Time 0.075687    
Epoch: [130][  150/  207]    Overall Loss 0.026208    Objective Loss 0.026208                                        LR 0.000500    Time 0.075557    
Epoch: [130][  160/  207]    Overall Loss 0.025998    Objective Loss 0.025998                                        LR 0.000500    Time 0.075353    
Epoch: [130][  170/  207]    Overall Loss 0.025847    Objective Loss 0.025847                                        LR 0.000500    Time 0.075160    
Epoch: [130][  180/  207]    Overall Loss 0.025699    Objective Loss 0.025699                                        LR 0.000500    Time 0.075150    
Epoch: [130][  190/  207]    Overall Loss 0.025581    Objective Loss 0.025581                                        LR 0.000500    Time 0.075116    
Epoch: [130][  200/  207]    Overall Loss 0.025388    Objective Loss 0.025388                                        LR 0.000500    Time 0.075042    
Epoch: [130][  207/  207]    Overall Loss 0.025294    Objective Loss 0.025294    Top1 97.734994    Top5 100.000000    LR 0.000500    Time 0.074790    
--- validate (epoch=130)-----------
5136 samples (512 per mini-batch)
Epoch: [130][   10/   11]    Loss 0.584125    Top1 85.527344    Top5 99.707031    
Epoch: [130][   11/   11]    Loss 0.562947    Top1 85.514019    Top5 99.707944    
==> Top1: 85.514    Top5: 99.708    Loss: 0.563

==> Confusion:
[[273   2   4   1   1   0   1  18]
 [  2 257  22   0   0   3   0  16]
 [  4  14 266   0   0   5   0  11]
 [  0   1   1 745  45  12   9  24]
 [  2   0   0  45 763   6  29  34]
 [  7   8   7  10  16 785  13  48]
 [  2   0   0   5  21  10 770  29]
 [ 20   9  16  38  61  87  25 533]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [131][   10/  207]    Overall Loss 0.018494    Objective Loss 0.018494                                        LR 0.000500    Time 0.112031    
Epoch: [131][   20/  207]    Overall Loss 0.017985    Objective Loss 0.017985                                        LR 0.000500    Time 0.091189    
Epoch: [131][   30/  207]    Overall Loss 0.018466    Objective Loss 0.018466                                        LR 0.000500    Time 0.084497    
Epoch: [131][   40/  207]    Overall Loss 0.018312    Objective Loss 0.018312                                        LR 0.000500    Time 0.081473    
Epoch: [131][   50/  207]    Overall Loss 0.018792    Objective Loss 0.018792                                        LR 0.000500    Time 0.079395    
Epoch: [131][   60/  207]    Overall Loss 0.018910    Objective Loss 0.018910                                        LR 0.000500    Time 0.077869    
Epoch: [131][   70/  207]    Overall Loss 0.018926    Objective Loss 0.018926                                        LR 0.000500    Time 0.076982    
Epoch: [131][   80/  207]    Overall Loss 0.018839    Objective Loss 0.018839                                        LR 0.000500    Time 0.076415    
Epoch: [131][   90/  207]    Overall Loss 0.018769    Objective Loss 0.018769                                        LR 0.000500    Time 0.076117    
Epoch: [131][  100/  207]    Overall Loss 0.018795    Objective Loss 0.018795                                        LR 0.000500    Time 0.075623    
Epoch: [131][  110/  207]    Overall Loss 0.018763    Objective Loss 0.018763                                        LR 0.000500    Time 0.075183    
Epoch: [131][  120/  207]    Overall Loss 0.018877    Objective Loss 0.018877                                        LR 0.000500    Time 0.074758    
Epoch: [131][  130/  207]    Overall Loss 0.018901    Objective Loss 0.018901                                        LR 0.000500    Time 0.074450    
Epoch: [131][  140/  207]    Overall Loss 0.018664    Objective Loss 0.018664                                        LR 0.000500    Time 0.074275    
Epoch: [131][  150/  207]    Overall Loss 0.018623    Objective Loss 0.018623                                        LR 0.000500    Time 0.074063    
Epoch: [131][  160/  207]    Overall Loss 0.018609    Objective Loss 0.018609                                        LR 0.000500    Time 0.073987    
Epoch: [131][  170/  207]    Overall Loss 0.018700    Objective Loss 0.018700                                        LR 0.000500    Time 0.073830    
Epoch: [131][  180/  207]    Overall Loss 0.018682    Objective Loss 0.018682                                        LR 0.000500    Time 0.073749    
Epoch: [131][  190/  207]    Overall Loss 0.018857    Objective Loss 0.018857                                        LR 0.000500    Time 0.073775    
Epoch: [131][  200/  207]    Overall Loss 0.018932    Objective Loss 0.018932                                        LR 0.000500    Time 0.073670    
Epoch: [131][  207/  207]    Overall Loss 0.019137    Objective Loss 0.019137    Top1 98.187995    Top5 99.886750    LR 0.000500    Time 0.073480    
--- validate (epoch=131)-----------
5136 samples (512 per mini-batch)
Epoch: [131][   10/   11]    Loss 0.491574    Top1 85.000000    Top5 99.726562    
Epoch: [131][   11/   11]    Loss 0.448865    Top1 85.027259    Top5 99.727414    
==> Top1: 85.027    Top5: 99.727    Loss: 0.449

==> Confusion:
[[276   4   3   2   1   1   2  11]
 [  2 277  17   0   0   1   1   2]
 [  2  20 270   1   0   3   0   4]
 [  0   3   1 774  38   6   4  11]
 [  2   0   0  70 758   6  23  20]
 [  7   8  13  16  18 796  11  25]
 [  2   1   0   8  30  10 767  19]
 [ 24  23  21  69  62 113  28 449]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [132][   10/  207]    Overall Loss 0.026876    Objective Loss 0.026876                                        LR 0.000500    Time 0.128351    
Epoch: [132][   20/  207]    Overall Loss 0.022957    Objective Loss 0.022957                                        LR 0.000500    Time 0.100124    
Epoch: [132][   30/  207]    Overall Loss 0.022620    Objective Loss 0.022620                                        LR 0.000500    Time 0.090639    
Epoch: [132][   40/  207]    Overall Loss 0.021474    Objective Loss 0.021474                                        LR 0.000500    Time 0.085826    
Epoch: [132][   50/  207]    Overall Loss 0.020298    Objective Loss 0.020298                                        LR 0.000500    Time 0.083386    
Epoch: [132][   60/  207]    Overall Loss 0.019840    Objective Loss 0.019840                                        LR 0.000500    Time 0.081171    
Epoch: [132][   70/  207]    Overall Loss 0.019405    Objective Loss 0.019405                                        LR 0.000500    Time 0.079797    
Epoch: [132][   80/  207]    Overall Loss 0.019405    Objective Loss 0.019405                                        LR 0.000500    Time 0.078717    
Epoch: [132][   90/  207]    Overall Loss 0.018786    Objective Loss 0.018786                                        LR 0.000500    Time 0.077885    
Epoch: [132][  100/  207]    Overall Loss 0.018856    Objective Loss 0.018856                                        LR 0.000500    Time 0.077472    
Epoch: [132][  110/  207]    Overall Loss 0.018775    Objective Loss 0.018775                                        LR 0.000500    Time 0.077170    
Epoch: [132][  120/  207]    Overall Loss 0.018840    Objective Loss 0.018840                                        LR 0.000500    Time 0.076870    
Epoch: [132][  130/  207]    Overall Loss 0.018746    Objective Loss 0.018746                                        LR 0.000500    Time 0.076382    
Epoch: [132][  140/  207]    Overall Loss 0.018632    Objective Loss 0.018632                                        LR 0.000500    Time 0.076053    
Epoch: [132][  150/  207]    Overall Loss 0.018687    Objective Loss 0.018687                                        LR 0.000500    Time 0.075690    
Epoch: [132][  160/  207]    Overall Loss 0.018602    Objective Loss 0.018602                                        LR 0.000500    Time 0.075385    
Epoch: [132][  170/  207]    Overall Loss 0.018743    Objective Loss 0.018743                                        LR 0.000500    Time 0.075252    
Epoch: [132][  180/  207]    Overall Loss 0.018855    Objective Loss 0.018855                                        LR 0.000500    Time 0.075004    
Epoch: [132][  190/  207]    Overall Loss 0.018827    Objective Loss 0.018827                                        LR 0.000500    Time 0.074816    
Epoch: [132][  200/  207]    Overall Loss 0.018724    Objective Loss 0.018724                                        LR 0.000500    Time 0.074694    
Epoch: [132][  207/  207]    Overall Loss 0.018645    Objective Loss 0.018645    Top1 98.414496    Top5 100.000000    LR 0.000500    Time 0.074466    
--- validate (epoch=132)-----------
5136 samples (512 per mini-batch)
Epoch: [132][   10/   11]    Loss 0.582796    Top1 85.625000    Top5 99.707031    
Epoch: [132][   11/   11]    Loss 0.544762    Top1 85.650312    Top5 99.707944    
==> Top1: 85.650    Top5: 99.708    Loss: 0.545

==> Confusion:
[[265   2   3   3   0   1   3  23]
 [  1 257  24   0   0   3   1  14]
 [  2  17 266   1   0   3   0  11]
 [  0   0   1 764  42  11   4  15]
 [  1   0   0  53 757   6  26  36]
 [  3   5   7  15  16 784  14  50]
 [  1   0   0   6  25   8 766  31]
 [ 12   6  15  44  54  95  23 540]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [133][   10/  207]    Overall Loss 0.013463    Objective Loss 0.013463                                        LR 0.000500    Time 0.131320    
Epoch: [133][   20/  207]    Overall Loss 0.013891    Objective Loss 0.013891                                        LR 0.000500    Time 0.100950    
Epoch: [133][   30/  207]    Overall Loss 0.013935    Objective Loss 0.013935                                        LR 0.000500    Time 0.090882    
Epoch: [133][   40/  207]    Overall Loss 0.014126    Objective Loss 0.014126                                        LR 0.000500    Time 0.086402    
Epoch: [133][   50/  207]    Overall Loss 0.014249    Objective Loss 0.014249                                        LR 0.000500    Time 0.083301    
Epoch: [133][   60/  207]    Overall Loss 0.014542    Objective Loss 0.014542                                        LR 0.000500    Time 0.081246    
Epoch: [133][   70/  207]    Overall Loss 0.014873    Objective Loss 0.014873                                        LR 0.000500    Time 0.079841    
Epoch: [133][   80/  207]    Overall Loss 0.014973    Objective Loss 0.014973                                        LR 0.000500    Time 0.078934    
Epoch: [133][   90/  207]    Overall Loss 0.015240    Objective Loss 0.015240                                        LR 0.000500    Time 0.078203    
Epoch: [133][  100/  207]    Overall Loss 0.015182    Objective Loss 0.015182                                        LR 0.000500    Time 0.077613    
Epoch: [133][  110/  207]    Overall Loss 0.015245    Objective Loss 0.015245                                        LR 0.000500    Time 0.077004    
Epoch: [133][  120/  207]    Overall Loss 0.015501    Objective Loss 0.015501                                        LR 0.000500    Time 0.076644    
Epoch: [133][  130/  207]    Overall Loss 0.015718    Objective Loss 0.015718                                        LR 0.000500    Time 0.076098    
Epoch: [133][  140/  207]    Overall Loss 0.016084    Objective Loss 0.016084                                        LR 0.000500    Time 0.075752    
Epoch: [133][  150/  207]    Overall Loss 0.016227    Objective Loss 0.016227                                        LR 0.000500    Time 0.075375    
Epoch: [133][  160/  207]    Overall Loss 0.016223    Objective Loss 0.016223                                        LR 0.000500    Time 0.075137    
Epoch: [133][  170/  207]    Overall Loss 0.016060    Objective Loss 0.016060                                        LR 0.000500    Time 0.075014    
Epoch: [133][  180/  207]    Overall Loss 0.016158    Objective Loss 0.016158                                        LR 0.000500    Time 0.075003    
Epoch: [133][  190/  207]    Overall Loss 0.016104    Objective Loss 0.016104                                        LR 0.000500    Time 0.074975    
Epoch: [133][  200/  207]    Overall Loss 0.016155    Objective Loss 0.016155                                        LR 0.000500    Time 0.074786    
Epoch: [133][  207/  207]    Overall Loss 0.016212    Objective Loss 0.016212    Top1 98.867497    Top5 100.000000    LR 0.000500    Time 0.074561    
--- validate (epoch=133)-----------
5136 samples (512 per mini-batch)
Epoch: [133][   10/   11]    Loss 0.535771    Top1 85.390625    Top5 99.707031    
Epoch: [133][   11/   11]    Loss 0.533472    Top1 85.397196    Top5 99.707944    
==> Top1: 85.397    Top5: 99.708    Loss: 0.533

==> Confusion:
[[274   2   5   3   0   1   2  13]
 [  1 259  27   0   0   4   1   8]
 [  2  15 274   0   0   4   0   5]
 [  0   1   2 759  43  12   4  16]
 [  2   0   0  42 771   6  23  35]
 [  2   6   5   8  15 821  12  25]
 [  1   0   0   6  28  11 765  26]
 [ 17   9  17  52  68 135  28 463]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [134][   10/  207]    Overall Loss 0.018825    Objective Loss 0.018825                                        LR 0.000500    Time 0.128562    
Epoch: [134][   20/  207]    Overall Loss 0.017339    Objective Loss 0.017339                                        LR 0.000500    Time 0.099383    
Epoch: [134][   30/  207]    Overall Loss 0.016582    Objective Loss 0.016582                                        LR 0.000500    Time 0.090056    
Epoch: [134][   40/  207]    Overall Loss 0.017348    Objective Loss 0.017348                                        LR 0.000500    Time 0.085197    
Epoch: [134][   50/  207]    Overall Loss 0.017343    Objective Loss 0.017343                                        LR 0.000500    Time 0.082905    
Epoch: [134][   60/  207]    Overall Loss 0.017297    Objective Loss 0.017297                                        LR 0.000500    Time 0.081550    
Epoch: [134][   70/  207]    Overall Loss 0.017711    Objective Loss 0.017711                                        LR 0.000500    Time 0.080125    
Epoch: [134][   80/  207]    Overall Loss 0.018165    Objective Loss 0.018165                                        LR 0.000500    Time 0.079245    
Epoch: [134][   90/  207]    Overall Loss 0.018102    Objective Loss 0.018102                                        LR 0.000500    Time 0.078264    
Epoch: [134][  100/  207]    Overall Loss 0.017908    Objective Loss 0.017908                                        LR 0.000500    Time 0.077423    
Epoch: [134][  110/  207]    Overall Loss 0.017824    Objective Loss 0.017824                                        LR 0.000500    Time 0.076869    
Epoch: [134][  120/  207]    Overall Loss 0.017852    Objective Loss 0.017852                                        LR 0.000500    Time 0.076364    
Epoch: [134][  130/  207]    Overall Loss 0.017792    Objective Loss 0.017792                                        LR 0.000500    Time 0.075990    
Epoch: [134][  140/  207]    Overall Loss 0.017906    Objective Loss 0.017906                                        LR 0.000500    Time 0.075578    
Epoch: [134][  150/  207]    Overall Loss 0.018006    Objective Loss 0.018006                                        LR 0.000500    Time 0.075263    
Epoch: [134][  160/  207]    Overall Loss 0.017971    Objective Loss 0.017971                                        LR 0.000500    Time 0.074904    
Epoch: [134][  170/  207]    Overall Loss 0.017881    Objective Loss 0.017881                                        LR 0.000500    Time 0.074685    
Epoch: [134][  180/  207]    Overall Loss 0.017845    Objective Loss 0.017845                                        LR 0.000500    Time 0.074506    
Epoch: [134][  190/  207]    Overall Loss 0.017733    Objective Loss 0.017733                                        LR 0.000500    Time 0.074476    
Epoch: [134][  200/  207]    Overall Loss 0.017989    Objective Loss 0.017989                                        LR 0.000500    Time 0.074326    
Epoch: [134][  207/  207]    Overall Loss 0.018184    Objective Loss 0.018184    Top1 98.527746    Top5 100.000000    LR 0.000500    Time 0.074101    
--- validate (epoch=134)-----------
5136 samples (512 per mini-batch)
Epoch: [134][   10/   11]    Loss 0.604835    Top1 85.507812    Top5 99.765625    
Epoch: [134][   11/   11]    Loss 0.565365    Top1 85.533489    Top5 99.766355    
==> Top1: 85.533    Top5: 99.766    Loss: 0.565

==> Confusion:
[[267   2   4   2   1   1   1  22]
 [  1 262  22   0   0   1   1  13]
 [  3  18 265   1   0   2   0  11]
 [  0   0   0 774  32   9   5  17]
 [  2   0   0  63 749   7  22  36]
 [  3   5  12  15  15 774  10  60]
 [  1   0   0   7  24  14 756  35]
 [ 12   8  13  55  51  81  23 546]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [135][   10/  207]    Overall Loss 0.018048    Objective Loss 0.018048                                        LR 0.000500    Time 0.139985    
Epoch: [135][   20/  207]    Overall Loss 0.018748    Objective Loss 0.018748                                        LR 0.000500    Time 0.105714    
Epoch: [135][   30/  207]    Overall Loss 0.020927    Objective Loss 0.020927                                        LR 0.000500    Time 0.094184    
Epoch: [135][   40/  207]    Overall Loss 0.021557    Objective Loss 0.021557                                        LR 0.000500    Time 0.088909    
Epoch: [135][   50/  207]    Overall Loss 0.021794    Objective Loss 0.021794                                        LR 0.000500    Time 0.085335    
Epoch: [135][   60/  207]    Overall Loss 0.020945    Objective Loss 0.020945                                        LR 0.000500    Time 0.082762    
Epoch: [135][   70/  207]    Overall Loss 0.020660    Objective Loss 0.020660                                        LR 0.000500    Time 0.081365    
Epoch: [135][   80/  207]    Overall Loss 0.020254    Objective Loss 0.020254                                        LR 0.000500    Time 0.080012    
Epoch: [135][   90/  207]    Overall Loss 0.020043    Objective Loss 0.020043                                        LR 0.000500    Time 0.079164    
Epoch: [135][  100/  207]    Overall Loss 0.019671    Objective Loss 0.019671                                        LR 0.000500    Time 0.078381    
Epoch: [135][  110/  207]    Overall Loss 0.019463    Objective Loss 0.019463                                        LR 0.000500    Time 0.077816    
Epoch: [135][  120/  207]    Overall Loss 0.019464    Objective Loss 0.019464                                        LR 0.000500    Time 0.077239    
Epoch: [135][  130/  207]    Overall Loss 0.019325    Objective Loss 0.019325                                        LR 0.000500    Time 0.076834    
Epoch: [135][  140/  207]    Overall Loss 0.020521    Objective Loss 0.020521                                        LR 0.000500    Time 0.076491    
Epoch: [135][  150/  207]    Overall Loss 0.021292    Objective Loss 0.021292                                        LR 0.000500    Time 0.076108    
Epoch: [135][  160/  207]    Overall Loss 0.021884    Objective Loss 0.021884                                        LR 0.000500    Time 0.075815    
Epoch: [135][  170/  207]    Overall Loss 0.022609    Objective Loss 0.022609                                        LR 0.000500    Time 0.075576    
Epoch: [135][  180/  207]    Overall Loss 0.023300    Objective Loss 0.023300                                        LR 0.000500    Time 0.075321    
Epoch: [135][  190/  207]    Overall Loss 0.025128    Objective Loss 0.025128                                        LR 0.000500    Time 0.075049    
Epoch: [135][  200/  207]    Overall Loss 0.028065    Objective Loss 0.028065                                        LR 0.000500    Time 0.074823    
Epoch: [135][  207/  207]    Overall Loss 0.030136    Objective Loss 0.030136    Top1 96.828992    Top5 100.000000    LR 0.000500    Time 0.074569    
--- validate (epoch=135)-----------
5136 samples (512 per mini-batch)
Epoch: [135][   10/   11]    Loss 0.787553    Top1 83.007812    Top5 99.628906    
Epoch: [135][   11/   11]    Loss 0.843202    Top1 83.002336    Top5 99.630062    
==> Top1: 83.002    Top5: 99.630    Loss: 0.843

==> Confusion:
[[268   1   1   0   2   0   3  25]
 [  2 247  18   3   0   3   0  27]
 [  8  25 220   2   0   7   0  38]
 [  0   1   0 742  46   6   9  33]
 [  2   0   0  45 763   1  20  48]
 [  4   4   9  10  23 709  12 123]
 [  2   0   0   6  30   9 746  44]
 [ 24   4  10  39  58  66  20 568]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [136][   10/  207]    Overall Loss 0.081727    Objective Loss 0.081727                                        LR 0.000500    Time 0.131719    
Epoch: [136][   20/  207]    Overall Loss 0.073869    Objective Loss 0.073869                                        LR 0.000500    Time 0.102943    
Epoch: [136][   30/  207]    Overall Loss 0.082598    Objective Loss 0.082598                                        LR 0.000500    Time 0.092768    
Epoch: [136][   40/  207]    Overall Loss 0.088565    Objective Loss 0.088565                                        LR 0.000500    Time 0.087285    
Epoch: [136][   50/  207]    Overall Loss 0.088299    Objective Loss 0.088299                                        LR 0.000500    Time 0.084203    
Epoch: [136][   60/  207]    Overall Loss 0.089072    Objective Loss 0.089072                                        LR 0.000500    Time 0.082031    
Epoch: [136][   70/  207]    Overall Loss 0.091681    Objective Loss 0.091681                                        LR 0.000500    Time 0.080492    
Epoch: [136][   80/  207]    Overall Loss 0.091986    Objective Loss 0.091986                                        LR 0.000500    Time 0.079446    
Epoch: [136][   90/  207]    Overall Loss 0.094709    Objective Loss 0.094709                                        LR 0.000500    Time 0.078428    
Epoch: [136][  100/  207]    Overall Loss 0.096954    Objective Loss 0.096954                                        LR 0.000500    Time 0.077778    
Epoch: [136][  110/  207]    Overall Loss 0.096570    Objective Loss 0.096570                                        LR 0.000500    Time 0.077338    
Epoch: [136][  120/  207]    Overall Loss 0.095044    Objective Loss 0.095044                                        LR 0.000500    Time 0.076883    
Epoch: [136][  130/  207]    Overall Loss 0.093317    Objective Loss 0.093317                                        LR 0.000500    Time 0.076511    
Epoch: [136][  140/  207]    Overall Loss 0.091103    Objective Loss 0.091103                                        LR 0.000500    Time 0.076095    
Epoch: [136][  150/  207]    Overall Loss 0.088438    Objective Loss 0.088438                                        LR 0.000500    Time 0.075753    
Epoch: [136][  160/  207]    Overall Loss 0.088178    Objective Loss 0.088178                                        LR 0.000500    Time 0.075390    
Epoch: [136][  170/  207]    Overall Loss 0.085989    Objective Loss 0.085989                                        LR 0.000500    Time 0.075154    
Epoch: [136][  180/  207]    Overall Loss 0.084816    Objective Loss 0.084816                                        LR 0.000500    Time 0.074927    
Epoch: [136][  190/  207]    Overall Loss 0.084420    Objective Loss 0.084420                                        LR 0.000500    Time 0.074853    
Epoch: [136][  200/  207]    Overall Loss 0.086286    Objective Loss 0.086286                                        LR 0.000500    Time 0.074734    
Epoch: [136][  207/  207]    Overall Loss 0.086452    Objective Loss 0.086452    Top1 96.489241    Top5 100.000000    LR 0.000500    Time 0.074487    
--- validate (epoch=136)-----------
5136 samples (512 per mini-batch)
Epoch: [136][   10/   11]    Loss 0.563050    Top1 82.675781    Top5 99.726562    
Epoch: [136][   11/   11]    Loss 0.551165    Top1 82.690810    Top5 99.727414    
==> Top1: 82.691    Top5: 99.727    Loss: 0.551

==> Confusion:
[[266   5   1   3   0  13   1  11]
 [  2 270  22   1   0   3   0   2]
 [  2  28 262   0   0   4   0   4]
 [  0   4   1 719  50  29  15  19]
 [  2   0   0  45 727  37  39  29]
 [  2   8   5   4   5 857   4   9]
 [  1   0   0   2  15  41 755  23]
 [ 27  23  15  32  45 231  25 391]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [137][   10/  207]    Overall Loss 0.084066    Objective Loss 0.084066                                        LR 0.000500    Time 0.130307    
Epoch: [137][   20/  207]    Overall Loss 0.073595    Objective Loss 0.073595                                        LR 0.000500    Time 0.101192    
Epoch: [137][   30/  207]    Overall Loss 0.067571    Objective Loss 0.067571                                        LR 0.000500    Time 0.090859    
Epoch: [137][   40/  207]    Overall Loss 0.063712    Objective Loss 0.063712                                        LR 0.000500    Time 0.087247    
Epoch: [137][   50/  207]    Overall Loss 0.058947    Objective Loss 0.058947                                        LR 0.000500    Time 0.087203    
Epoch: [137][   60/  207]    Overall Loss 0.055756    Objective Loss 0.055756                                        LR 0.000500    Time 0.084499    
Epoch: [137][   70/  207]    Overall Loss 0.053618    Objective Loss 0.053618                                        LR 0.000500    Time 0.082606    
Epoch: [137][   80/  207]    Overall Loss 0.052125    Objective Loss 0.052125                                        LR 0.000500    Time 0.081204    
Epoch: [137][   90/  207]    Overall Loss 0.049940    Objective Loss 0.049940                                        LR 0.000500    Time 0.080071    
Epoch: [137][  100/  207]    Overall Loss 0.048207    Objective Loss 0.048207                                        LR 0.000500    Time 0.079316    
Epoch: [137][  110/  207]    Overall Loss 0.046353    Objective Loss 0.046353                                        LR 0.000500    Time 0.078814    
Epoch: [137][  120/  207]    Overall Loss 0.044524    Objective Loss 0.044524                                        LR 0.000500    Time 0.078187    
Epoch: [137][  130/  207]    Overall Loss 0.043231    Objective Loss 0.043231                                        LR 0.000500    Time 0.077682    
Epoch: [137][  140/  207]    Overall Loss 0.042089    Objective Loss 0.042089                                        LR 0.000500    Time 0.077246    
Epoch: [137][  150/  207]    Overall Loss 0.040859    Objective Loss 0.040859                                        LR 0.000500    Time 0.077177    
Epoch: [137][  160/  207]    Overall Loss 0.039743    Objective Loss 0.039743                                        LR 0.000500    Time 0.076723    
Epoch: [137][  170/  207]    Overall Loss 0.038808    Objective Loss 0.038808                                        LR 0.000500    Time 0.076565    
Epoch: [137][  180/  207]    Overall Loss 0.037721    Objective Loss 0.037721                                        LR 0.000500    Time 0.076345    
Epoch: [137][  190/  207]    Overall Loss 0.036899    Objective Loss 0.036899                                        LR 0.000500    Time 0.076049    
Epoch: [137][  200/  207]    Overall Loss 0.036224    Objective Loss 0.036224                                        LR 0.000500    Time 0.075774    
Epoch: [137][  207/  207]    Overall Loss 0.035779    Objective Loss 0.035779    Top1 97.961495    Top5 100.000000    LR 0.000500    Time 0.075505    
--- validate (epoch=137)-----------
5136 samples (512 per mini-batch)
Epoch: [137][   10/   11]    Loss 0.527233    Top1 84.765625    Top5 99.726562    
Epoch: [137][   11/   11]    Loss 0.483945    Top1 84.774143    Top5 99.727414    
==> Top1: 84.774    Top5: 99.727    Loss: 0.484

==> Confusion:
[[269   1   5   2   2   1   1  19]
 [  1 253  31   0   0   3   2  10]
 [  1  11 280   1   0   3   0   4]
 [  0   0   1 748  55  11  11  11]
 [  2   0   0  45 782   2  22  26]
 [  4   7   9  13  24 786  20  31]
 [  1   1   0   7  30   7 765  26]
 [ 22  12  15  45  82 114  28 471]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [138][   10/  207]    Overall Loss 0.022021    Objective Loss 0.022021                                        LR 0.000500    Time 0.127460    
Epoch: [138][   20/  207]    Overall Loss 0.020163    Objective Loss 0.020163                                        LR 0.000500    Time 0.098999    
Epoch: [138][   30/  207]    Overall Loss 0.020864    Objective Loss 0.020864                                        LR 0.000500    Time 0.089328    
Epoch: [138][   40/  207]    Overall Loss 0.019855    Objective Loss 0.019855                                        LR 0.000500    Time 0.084829    
Epoch: [138][   50/  207]    Overall Loss 0.019843    Objective Loss 0.019843                                        LR 0.000500    Time 0.082383    
Epoch: [138][   60/  207]    Overall Loss 0.019660    Objective Loss 0.019660                                        LR 0.000500    Time 0.080484    
Epoch: [138][   70/  207]    Overall Loss 0.019828    Objective Loss 0.019828                                        LR 0.000500    Time 0.079083    
Epoch: [138][   80/  207]    Overall Loss 0.019598    Objective Loss 0.019598                                        LR 0.000500    Time 0.078022    
Epoch: [138][   90/  207]    Overall Loss 0.019615    Objective Loss 0.019615                                        LR 0.000500    Time 0.077302    
Epoch: [138][  100/  207]    Overall Loss 0.019644    Objective Loss 0.019644                                        LR 0.000500    Time 0.076731    
Epoch: [138][  110/  207]    Overall Loss 0.019617    Objective Loss 0.019617                                        LR 0.000500    Time 0.076205    
Epoch: [138][  120/  207]    Overall Loss 0.019375    Objective Loss 0.019375                                        LR 0.000500    Time 0.075956    
Epoch: [138][  130/  207]    Overall Loss 0.019449    Objective Loss 0.019449                                        LR 0.000500    Time 0.075686    
Epoch: [138][  140/  207]    Overall Loss 0.019536    Objective Loss 0.019536                                        LR 0.000500    Time 0.075455    
Epoch: [138][  150/  207]    Overall Loss 0.019466    Objective Loss 0.019466                                        LR 0.000500    Time 0.075151    
Epoch: [138][  160/  207]    Overall Loss 0.019365    Objective Loss 0.019365                                        LR 0.000500    Time 0.074848    
Epoch: [138][  170/  207]    Overall Loss 0.019108    Objective Loss 0.019108                                        LR 0.000500    Time 0.074673    
Epoch: [138][  180/  207]    Overall Loss 0.019116    Objective Loss 0.019116                                        LR 0.000500    Time 0.074543    
Epoch: [138][  190/  207]    Overall Loss 0.019045    Objective Loss 0.019045                                        LR 0.000500    Time 0.074538    
Epoch: [138][  200/  207]    Overall Loss 0.019003    Objective Loss 0.019003                                        LR 0.000500    Time 0.074424    
Epoch: [138][  207/  207]    Overall Loss 0.019004    Objective Loss 0.019004    Top1 98.301246    Top5 100.000000    LR 0.000500    Time 0.074189    
--- validate (epoch=138)-----------
5136 samples (512 per mini-batch)
Epoch: [138][   10/   11]    Loss 0.547171    Top1 84.882812    Top5 99.765625    
Epoch: [138][   11/   11]    Loss 0.498297    Top1 84.929907    Top5 99.766355    
==> Top1: 84.930    Top5: 99.766    Loss: 0.498

==> Confusion:
[[268   2   5   3   1   1   1  19]
 [  2 258  25   1   0   3   1  10]
 [  1  13 278   1   0   2   0   5]
 [  0   1   0 760  46  11  10   9]
 [  1   0   0  55 782   4  18  19]
 [  3   7   8  18  26 788  15  29]
 [  1   0   0   8  37  12 756  23]
 [ 18   8  14  59  82 113  23 472]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [139][   10/  207]    Overall Loss 0.016471    Objective Loss 0.016471                                        LR 0.000500    Time 0.131295    
Epoch: [139][   20/  207]    Overall Loss 0.015834    Objective Loss 0.015834                                        LR 0.000500    Time 0.102617    
Epoch: [139][   30/  207]    Overall Loss 0.015733    Objective Loss 0.015733                                        LR 0.000500    Time 0.092711    
Epoch: [139][   40/  207]    Overall Loss 0.016161    Objective Loss 0.016161                                        LR 0.000500    Time 0.088003    
Epoch: [139][   50/  207]    Overall Loss 0.016016    Objective Loss 0.016016                                        LR 0.000500    Time 0.084371    
Epoch: [139][   60/  207]    Overall Loss 0.015966    Objective Loss 0.015966                                        LR 0.000500    Time 0.082132    
Epoch: [139][   70/  207]    Overall Loss 0.015644    Objective Loss 0.015644                                        LR 0.000500    Time 0.080411    
Epoch: [139][   80/  207]    Overall Loss 0.015794    Objective Loss 0.015794                                        LR 0.000500    Time 0.079216    
Epoch: [139][   90/  207]    Overall Loss 0.015841    Objective Loss 0.015841                                        LR 0.000500    Time 0.078442    
Epoch: [139][  100/  207]    Overall Loss 0.016043    Objective Loss 0.016043                                        LR 0.000500    Time 0.077563    
Epoch: [139][  110/  207]    Overall Loss 0.016126    Objective Loss 0.016126                                        LR 0.000500    Time 0.077131    
Epoch: [139][  120/  207]    Overall Loss 0.016207    Objective Loss 0.016207                                        LR 0.000500    Time 0.076556    
Epoch: [139][  130/  207]    Overall Loss 0.016340    Objective Loss 0.016340                                        LR 0.000500    Time 0.076114    
Epoch: [139][  140/  207]    Overall Loss 0.016408    Objective Loss 0.016408                                        LR 0.000500    Time 0.075676    
Epoch: [139][  150/  207]    Overall Loss 0.016356    Objective Loss 0.016356                                        LR 0.000500    Time 0.075385    
Epoch: [139][  160/  207]    Overall Loss 0.016463    Objective Loss 0.016463                                        LR 0.000500    Time 0.075166    
Epoch: [139][  170/  207]    Overall Loss 0.016481    Objective Loss 0.016481                                        LR 0.000500    Time 0.075032    
Epoch: [139][  180/  207]    Overall Loss 0.016478    Objective Loss 0.016478                                        LR 0.000500    Time 0.074911    
Epoch: [139][  190/  207]    Overall Loss 0.016341    Objective Loss 0.016341                                        LR 0.000500    Time 0.074735    
Epoch: [139][  200/  207]    Overall Loss 0.016443    Objective Loss 0.016443                                        LR 0.000500    Time 0.074696    
Epoch: [139][  207/  207]    Overall Loss 0.016499    Objective Loss 0.016499    Top1 98.187995    Top5 100.000000    LR 0.000500    Time 0.074477    
--- validate (epoch=139)-----------
5136 samples (512 per mini-batch)
Epoch: [139][   10/   11]    Loss 0.535262    Top1 85.312500    Top5 99.765625    
Epoch: [139][   11/   11]    Loss 0.500687    Top1 85.299844    Top5 99.766355    
==> Top1: 85.300    Top5: 99.766    Loss: 0.501

==> Confusion:
[[274   2   2   2   1   1   1  17]
 [  2 258  23   0   0   3   1  13]
 [  2  17 273   0   0   3   0   5]
 [  0   1   1 725  64  13  11  22]
 [  2   0   0  24 788   4  25  36]
 [  3   5   8   8  17 789  19  45]
 [  1   0   0   7  27   8 769  25]
 [ 23  12  16  30  72 103  28 505]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [140][   10/  207]    Overall Loss 0.014797    Objective Loss 0.014797                                        LR 0.000250    Time 0.131785    
Epoch: [140][   20/  207]    Overall Loss 0.013675    Objective Loss 0.013675                                        LR 0.000250    Time 0.101013    
Epoch: [140][   30/  207]    Overall Loss 0.013779    Objective Loss 0.013779                                        LR 0.000250    Time 0.091288    
Epoch: [140][   40/  207]    Overall Loss 0.014471    Objective Loss 0.014471                                        LR 0.000250    Time 0.086073    
Epoch: [140][   50/  207]    Overall Loss 0.014319    Objective Loss 0.014319                                        LR 0.000250    Time 0.083359    
Epoch: [140][   60/  207]    Overall Loss 0.013931    Objective Loss 0.013931                                        LR 0.000250    Time 0.083465    
Epoch: [140][   70/  207]    Overall Loss 0.014186    Objective Loss 0.014186                                        LR 0.000250    Time 0.081948    
Epoch: [140][   80/  207]    Overall Loss 0.014112    Objective Loss 0.014112                                        LR 0.000250    Time 0.080582    
Epoch: [140][   90/  207]    Overall Loss 0.014105    Objective Loss 0.014105                                        LR 0.000250    Time 0.079506    
Epoch: [140][  100/  207]    Overall Loss 0.014129    Objective Loss 0.014129                                        LR 0.000250    Time 0.078503    
Epoch: [140][  110/  207]    Overall Loss 0.014041    Objective Loss 0.014041                                        LR 0.000250    Time 0.077902    
Epoch: [140][  120/  207]    Overall Loss 0.013958    Objective Loss 0.013958                                        LR 0.000250    Time 0.077496    
Epoch: [140][  130/  207]    Overall Loss 0.013893    Objective Loss 0.013893                                        LR 0.000250    Time 0.077045    
Epoch: [140][  140/  207]    Overall Loss 0.013763    Objective Loss 0.013763                                        LR 0.000250    Time 0.076546    
Epoch: [140][  150/  207]    Overall Loss 0.013677    Objective Loss 0.013677                                        LR 0.000250    Time 0.076286    
Epoch: [140][  160/  207]    Overall Loss 0.013857    Objective Loss 0.013857                                        LR 0.000250    Time 0.075929    
Epoch: [140][  170/  207]    Overall Loss 0.013905    Objective Loss 0.013905                                        LR 0.000250    Time 0.075684    
Epoch: [140][  180/  207]    Overall Loss 0.013912    Objective Loss 0.013912                                        LR 0.000250    Time 0.075418    
Epoch: [140][  190/  207]    Overall Loss 0.013866    Objective Loss 0.013866                                        LR 0.000250    Time 0.075157    
Epoch: [140][  200/  207]    Overall Loss 0.013955    Objective Loss 0.013955                                        LR 0.000250    Time 0.074934    
Epoch: [140][  207/  207]    Overall Loss 0.013967    Objective Loss 0.013967    Top1 98.301246    Top5 100.000000    LR 0.000250    Time 0.074696    
--- validate (epoch=140)-----------
5136 samples (512 per mini-batch)
Epoch: [140][   10/   11]    Loss 0.574839    Top1 85.664062    Top5 99.765625    
Epoch: [140][   11/   11]    Loss 0.528383    Top1 85.689252    Top5 99.766355    
==> Top1: 85.689    Top5: 99.766    Loss: 0.528

==> Confusion:
[[267   2   2   3   2   0   2  22]
 [  1 258  22   1   0   4   1  13]
 [  1  17 270   1   0   2   0   9]
 [  0   0   1 753  49   8   8  18]
 [  2   0   0  43 773   6  20  35]
 [  3   7   8   9  18 784  14  51]
 [  1   0   0   5  28   8 766  29]
 [ 18   9  13  45  61  90  23 530]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [141][   10/  207]    Overall Loss 0.014151    Objective Loss 0.014151                                        LR 0.000250    Time 0.113423    
Epoch: [141][   20/  207]    Overall Loss 0.013820    Objective Loss 0.013820                                        LR 0.000250    Time 0.092153    
Epoch: [141][   30/  207]    Overall Loss 0.013528    Objective Loss 0.013528                                        LR 0.000250    Time 0.084947    
Epoch: [141][   40/  207]    Overall Loss 0.013177    Objective Loss 0.013177                                        LR 0.000250    Time 0.081401    
Epoch: [141][   50/  207]    Overall Loss 0.013069    Objective Loss 0.013069                                        LR 0.000250    Time 0.079342    
Epoch: [141][   60/  207]    Overall Loss 0.012998    Objective Loss 0.012998                                        LR 0.000250    Time 0.077889    
Epoch: [141][   70/  207]    Overall Loss 0.012867    Objective Loss 0.012867                                        LR 0.000250    Time 0.077133    
Epoch: [141][   80/  207]    Overall Loss 0.013130    Objective Loss 0.013130                                        LR 0.000250    Time 0.076401    
Epoch: [141][   90/  207]    Overall Loss 0.013108    Objective Loss 0.013108                                        LR 0.000250    Time 0.075944    
Epoch: [141][  100/  207]    Overall Loss 0.013142    Objective Loss 0.013142                                        LR 0.000250    Time 0.075579    
Epoch: [141][  110/  207]    Overall Loss 0.013205    Objective Loss 0.013205                                        LR 0.000250    Time 0.075195    
Epoch: [141][  120/  207]    Overall Loss 0.013382    Objective Loss 0.013382                                        LR 0.000250    Time 0.075018    
Epoch: [141][  130/  207]    Overall Loss 0.013314    Objective Loss 0.013314                                        LR 0.000250    Time 0.074863    
Epoch: [141][  140/  207]    Overall Loss 0.013425    Objective Loss 0.013425                                        LR 0.000250    Time 0.074617    
Epoch: [141][  150/  207]    Overall Loss 0.013485    Objective Loss 0.013485                                        LR 0.000250    Time 0.074476    
Epoch: [141][  160/  207]    Overall Loss 0.013610    Objective Loss 0.013610                                        LR 0.000250    Time 0.074278    
Epoch: [141][  170/  207]    Overall Loss 0.013549    Objective Loss 0.013549                                        LR 0.000250    Time 0.074111    
Epoch: [141][  180/  207]    Overall Loss 0.013468    Objective Loss 0.013468                                        LR 0.000250    Time 0.073983    
Epoch: [141][  190/  207]    Overall Loss 0.013454    Objective Loss 0.013454                                        LR 0.000250    Time 0.073903    
Epoch: [141][  200/  207]    Overall Loss 0.013290    Objective Loss 0.013290                                        LR 0.000250    Time 0.073778    
Epoch: [141][  207/  207]    Overall Loss 0.013222    Objective Loss 0.013222    Top1 99.093998    Top5 100.000000    LR 0.000250    Time 0.073568    
--- validate (epoch=141)-----------
5136 samples (512 per mini-batch)
Epoch: [141][   10/   11]    Loss 0.597505    Top1 85.625000    Top5 99.765625    
Epoch: [141][   11/   11]    Loss 0.568906    Top1 85.650312    Top5 99.766355    
==> Top1: 85.650    Top5: 99.766    Loss: 0.569

==> Confusion:
[[268   2   2   3   2   0   2  21]
 [  1 254  24   1   0   3   1  16]
 [  1  15 268   1   0   3   0  12]
 [  0   0   1 749  49   8  11  19]
 [  2   0   0  38 772   6  21  40]
 [  2   4   8  10  20 774  18  58]
 [  1   0   0   6  25   6 768  31]
 [ 17   7  13  36  62  83  25 546]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [142][   10/  207]    Overall Loss 0.014776    Objective Loss 0.014776                                        LR 0.000250    Time 0.132298    
Epoch: [142][   20/  207]    Overall Loss 0.013041    Objective Loss 0.013041                                        LR 0.000250    Time 0.101511    
Epoch: [142][   30/  207]    Overall Loss 0.012706    Objective Loss 0.012706                                        LR 0.000250    Time 0.091576    
Epoch: [142][   40/  207]    Overall Loss 0.012388    Objective Loss 0.012388                                        LR 0.000250    Time 0.086706    
Epoch: [142][   50/  207]    Overall Loss 0.012476    Objective Loss 0.012476                                        LR 0.000250    Time 0.083847    
Epoch: [142][   60/  207]    Overall Loss 0.012502    Objective Loss 0.012502                                        LR 0.000250    Time 0.082124    
Epoch: [142][   70/  207]    Overall Loss 0.012324    Objective Loss 0.012324                                        LR 0.000250    Time 0.080386    
Epoch: [142][   80/  207]    Overall Loss 0.012464    Objective Loss 0.012464                                        LR 0.000250    Time 0.079282    
Epoch: [142][   90/  207]    Overall Loss 0.012524    Objective Loss 0.012524                                        LR 0.000250    Time 0.078499    
Epoch: [142][  100/  207]    Overall Loss 0.012449    Objective Loss 0.012449                                        LR 0.000250    Time 0.077787    
Epoch: [142][  110/  207]    Overall Loss 0.012356    Objective Loss 0.012356                                        LR 0.000250    Time 0.077241    
Epoch: [142][  120/  207]    Overall Loss 0.012350    Objective Loss 0.012350                                        LR 0.000250    Time 0.076826    
Epoch: [142][  130/  207]    Overall Loss 0.012417    Objective Loss 0.012417                                        LR 0.000250    Time 0.076353    
Epoch: [142][  140/  207]    Overall Loss 0.012426    Objective Loss 0.012426                                        LR 0.000250    Time 0.076030    
Epoch: [142][  150/  207]    Overall Loss 0.012663    Objective Loss 0.012663                                        LR 0.000250    Time 0.075862    
Epoch: [142][  160/  207]    Overall Loss 0.012687    Objective Loss 0.012687                                        LR 0.000250    Time 0.075637    
Epoch: [142][  170/  207]    Overall Loss 0.012676    Objective Loss 0.012676                                        LR 0.000250    Time 0.075380    
Epoch: [142][  180/  207]    Overall Loss 0.012764    Objective Loss 0.012764                                        LR 0.000250    Time 0.075192    
Epoch: [142][  190/  207]    Overall Loss 0.012866    Objective Loss 0.012866                                        LR 0.000250    Time 0.075083    
Epoch: [142][  200/  207]    Overall Loss 0.012910    Objective Loss 0.012910                                        LR 0.000250    Time 0.074854    
Epoch: [142][  207/  207]    Overall Loss 0.012887    Objective Loss 0.012887    Top1 98.754247    Top5 99.886750    LR 0.000250    Time 0.074620    
--- validate (epoch=142)-----------
5136 samples (512 per mini-batch)
Epoch: [142][   10/   11]    Loss 0.584081    Top1 85.527344    Top5 99.785156    
Epoch: [142][   11/   11]    Loss 0.535788    Top1 85.572430    Top5 99.785826    
==> Top1: 85.572    Top5: 99.786    Loss: 0.536

==> Confusion:
[[270   2   3   2   2   0   2  19]
 [  1 256  24   0   0   4   2  13]
 [  1  17 266   0   0   3   0  13]
 [  0   0   1 742  51  10  11  22]
 [  2   0   0  36 770   8  24  39]
 [  4   4   7   8  19 784  17  51]
 [  1   0   0   5  27   7 770  27]
 [ 17   7  12  33  66  90  27 537]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [143][   10/  207]    Overall Loss 0.010173    Objective Loss 0.010173                                        LR 0.000250    Time 0.132184    
Epoch: [143][   20/  207]    Overall Loss 0.012953    Objective Loss 0.012953                                        LR 0.000250    Time 0.102511    
Epoch: [143][   30/  207]    Overall Loss 0.013416    Objective Loss 0.013416                                        LR 0.000250    Time 0.092813    
Epoch: [143][   40/  207]    Overall Loss 0.013061    Objective Loss 0.013061                                        LR 0.000250    Time 0.088848    
Epoch: [143][   50/  207]    Overall Loss 0.012842    Objective Loss 0.012842                                        LR 0.000250    Time 0.087187    
Epoch: [143][   60/  207]    Overall Loss 0.012619    Objective Loss 0.012619                                        LR 0.000250    Time 0.085032    
Epoch: [143][   70/  207]    Overall Loss 0.012615    Objective Loss 0.012615                                        LR 0.000250    Time 0.083749    
Epoch: [143][   80/  207]    Overall Loss 0.012654    Objective Loss 0.012654                                        LR 0.000250    Time 0.083436    
Epoch: [143][   90/  207]    Overall Loss 0.012514    Objective Loss 0.012514                                        LR 0.000250    Time 0.082426    
Epoch: [143][  100/  207]    Overall Loss 0.012521    Objective Loss 0.012521                                        LR 0.000250    Time 0.081864    
Epoch: [143][  110/  207]    Overall Loss 0.012381    Objective Loss 0.012381                                        LR 0.000250    Time 0.081470    
Epoch: [143][  120/  207]    Overall Loss 0.012419    Objective Loss 0.012419                                        LR 0.000250    Time 0.080974    
Epoch: [143][  130/  207]    Overall Loss 0.012334    Objective Loss 0.012334                                        LR 0.000250    Time 0.080604    
Epoch: [143][  140/  207]    Overall Loss 0.012477    Objective Loss 0.012477                                        LR 0.000250    Time 0.080253    
Epoch: [143][  150/  207]    Overall Loss 0.012467    Objective Loss 0.012467                                        LR 0.000250    Time 0.079964    
Epoch: [143][  160/  207]    Overall Loss 0.012461    Objective Loss 0.012461                                        LR 0.000250    Time 0.079547    
Epoch: [143][  170/  207]    Overall Loss 0.012448    Objective Loss 0.012448                                        LR 0.000250    Time 0.079183    
Epoch: [143][  180/  207]    Overall Loss 0.012616    Objective Loss 0.012616                                        LR 0.000250    Time 0.078780    
Epoch: [143][  190/  207]    Overall Loss 0.012677    Objective Loss 0.012677                                        LR 0.000250    Time 0.078387    
Epoch: [143][  200/  207]    Overall Loss 0.012722    Objective Loss 0.012722                                        LR 0.000250    Time 0.077976    
Epoch: [143][  207/  207]    Overall Loss 0.012687    Objective Loss 0.012687    Top1 99.093998    Top5 100.000000    LR 0.000250    Time 0.077609    
--- validate (epoch=143)-----------
5136 samples (512 per mini-batch)
Epoch: [143][   10/   11]    Loss 0.571018    Top1 85.585938    Top5 99.785156    
Epoch: [143][   11/   11]    Loss 0.596135    Top1 85.591900    Top5 99.785826    
==> Top1: 85.592    Top5: 99.786    Loss: 0.596

==> Confusion:
[[270   2   3   3   2   0   2  18]
 [  1 261  22   1   0   1   1  13]
 [  1  18 272   1   0   2   0   6]
 [  0   2   1 743  54   9   9  19]
 [  2   0   0  35 778   6  18  40]
 [  4   7  10  10  20 773  15  55]
 [  1   0   0   5  32   7 764  28]
 [ 19  12  15  34  69  84  21 535]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [144][   10/  207]    Overall Loss 0.012035    Objective Loss 0.012035                                        LR 0.000250    Time 0.139657    
Epoch: [144][   20/  207]    Overall Loss 0.012199    Objective Loss 0.012199                                        LR 0.000250    Time 0.105595    
Epoch: [144][   30/  207]    Overall Loss 0.011964    Objective Loss 0.011964                                        LR 0.000250    Time 0.096089    
Epoch: [144][   40/  207]    Overall Loss 0.011931    Objective Loss 0.011931                                        LR 0.000250    Time 0.090331    
Epoch: [144][   50/  207]    Overall Loss 0.011792    Objective Loss 0.011792                                        LR 0.000250    Time 0.087080    
Epoch: [144][   60/  207]    Overall Loss 0.011904    Objective Loss 0.011904                                        LR 0.000250    Time 0.084367    
Epoch: [144][   70/  207]    Overall Loss 0.011863    Objective Loss 0.011863                                        LR 0.000250    Time 0.082735    
Epoch: [144][   80/  207]    Overall Loss 0.011804    Objective Loss 0.011804                                        LR 0.000250    Time 0.081249    
Epoch: [144][   90/  207]    Overall Loss 0.011809    Objective Loss 0.011809                                        LR 0.000250    Time 0.080431    
Epoch: [144][  100/  207]    Overall Loss 0.011795    Objective Loss 0.011795                                        LR 0.000250    Time 0.079577    
Epoch: [144][  110/  207]    Overall Loss 0.011952    Objective Loss 0.011952                                        LR 0.000250    Time 0.079476    
Epoch: [144][  120/  207]    Overall Loss 0.012069    Objective Loss 0.012069                                        LR 0.000250    Time 0.079405    
Epoch: [144][  130/  207]    Overall Loss 0.012224    Objective Loss 0.012224                                        LR 0.000250    Time 0.079471    
Epoch: [144][  140/  207]    Overall Loss 0.012189    Objective Loss 0.012189                                        LR 0.000250    Time 0.079026    
Epoch: [144][  150/  207]    Overall Loss 0.012170    Objective Loss 0.012170                                        LR 0.000250    Time 0.078490    
Epoch: [144][  160/  207]    Overall Loss 0.012096    Objective Loss 0.012096                                        LR 0.000250    Time 0.078673    
Epoch: [144][  170/  207]    Overall Loss 0.012178    Objective Loss 0.012178                                        LR 0.000250    Time 0.079496    
Epoch: [144][  180/  207]    Overall Loss 0.012199    Objective Loss 0.012199                                        LR 0.000250    Time 0.080158    
Epoch: [144][  190/  207]    Overall Loss 0.012327    Objective Loss 0.012327                                        LR 0.000250    Time 0.080770    
Epoch: [144][  200/  207]    Overall Loss 0.012479    Objective Loss 0.012479                                        LR 0.000250    Time 0.081258    
Epoch: [144][  207/  207]    Overall Loss 0.012506    Objective Loss 0.012506    Top1 98.301246    Top5 100.000000    LR 0.000250    Time 0.080780    
--- validate (epoch=144)-----------
5136 samples (512 per mini-batch)
Epoch: [144][   10/   11]    Loss 0.580011    Top1 85.722656    Top5 99.726562    
Epoch: [144][   11/   11]    Loss 0.544923    Top1 85.747664    Top5 99.727414    
==> Top1: 85.748    Top5: 99.727    Loss: 0.545

==> Confusion:
[[269   2   3   1   2   0   2  21]
 [  1 257  26   0   0   1   2  13]
 [  1  17 275   0   0   3   0   4]
 [  0   1   1 743  51   9   7  25]
 [  2   0   0  37 776   5  24  35]
 [  3   6   9   8  18 776  13  61]
 [  1   0   0   5  31   7 761  32]
 [ 16  13  17  33  64  75  24 547]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [145][   10/  207]    Overall Loss 0.012199    Objective Loss 0.012199                                        LR 0.000250    Time 0.132506    
Epoch: [145][   20/  207]    Overall Loss 0.011554    Objective Loss 0.011554                                        LR 0.000250    Time 0.102504    
Epoch: [145][   30/  207]    Overall Loss 0.012316    Objective Loss 0.012316                                        LR 0.000250    Time 0.092132    
Epoch: [145][   40/  207]    Overall Loss 0.012011    Objective Loss 0.012011                                        LR 0.000250    Time 0.087226    
Epoch: [145][   50/  207]    Overall Loss 0.011902    Objective Loss 0.011902                                        LR 0.000250    Time 0.088024    
Epoch: [145][   60/  207]    Overall Loss 0.011651    Objective Loss 0.011651                                        LR 0.000250    Time 0.088617    
Epoch: [145][   70/  207]    Overall Loss 0.011635    Objective Loss 0.011635                                        LR 0.000250    Time 0.089127    
Epoch: [145][   80/  207]    Overall Loss 0.011846    Objective Loss 0.011846                                        LR 0.000250    Time 0.089439    
Epoch: [145][   90/  207]    Overall Loss 0.011926    Objective Loss 0.011926                                        LR 0.000250    Time 0.089630    
Epoch: [145][  100/  207]    Overall Loss 0.012143    Objective Loss 0.012143                                        LR 0.000250    Time 0.089876    
Epoch: [145][  110/  207]    Overall Loss 0.011940    Objective Loss 0.011940                                        LR 0.000250    Time 0.090053    
Epoch: [145][  120/  207]    Overall Loss 0.012024    Objective Loss 0.012024                                        LR 0.000250    Time 0.090156    
Epoch: [145][  130/  207]    Overall Loss 0.012111    Objective Loss 0.012111                                        LR 0.000250    Time 0.090305    
Epoch: [145][  140/  207]    Overall Loss 0.012162    Objective Loss 0.012162                                        LR 0.000250    Time 0.090389    
Epoch: [145][  150/  207]    Overall Loss 0.012258    Objective Loss 0.012258                                        LR 0.000250    Time 0.090481    
Epoch: [145][  160/  207]    Overall Loss 0.012289    Objective Loss 0.012289                                        LR 0.000250    Time 0.090512    
Epoch: [145][  170/  207]    Overall Loss 0.012374    Objective Loss 0.012374                                        LR 0.000250    Time 0.090580    
Epoch: [145][  180/  207]    Overall Loss 0.012416    Objective Loss 0.012416                                        LR 0.000250    Time 0.090652    
Epoch: [145][  190/  207]    Overall Loss 0.012507    Objective Loss 0.012507                                        LR 0.000250    Time 0.090660    
Epoch: [145][  200/  207]    Overall Loss 0.012549    Objective Loss 0.012549                                        LR 0.000250    Time 0.090724    
Epoch: [145][  207/  207]    Overall Loss 0.012571    Objective Loss 0.012571    Top1 99.660249    Top5 100.000000    LR 0.000250    Time 0.090618    
--- validate (epoch=145)-----------
5136 samples (512 per mini-batch)
Epoch: [145][   10/   11]    Loss 0.623293    Top1 85.644531    Top5 99.726562    
Epoch: [145][   11/   11]    Loss 0.630394    Top1 85.552960    Top5 99.727414    
==> Top1: 85.553    Top5: 99.727    Loss: 0.630

==> Confusion:
[[266   2   3   2   2   0   2  23]
 [  1 261  22   0   0   1   1  14]
 [  1  18 268   0   0   2   0  11]
 [  0   2   1 736  51   8   9  30]
 [  2   0   0  35 779   3  20  40]
 [  4   7   9   6  19 761  13  75]
 [  1   0   0   4  28   6 764  34]
 [ 14  10  15  35  61  74  21 559]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [146][   10/  207]    Overall Loss 0.012991    Objective Loss 0.012991                                        LR 0.000250    Time 0.150004    
Epoch: [146][   20/  207]    Overall Loss 0.013321    Objective Loss 0.013321                                        LR 0.000250    Time 0.110424    
Epoch: [146][   30/  207]    Overall Loss 0.013163    Objective Loss 0.013163                                        LR 0.000250    Time 0.097396    
Epoch: [146][   40/  207]    Overall Loss 0.012985    Objective Loss 0.012985                                        LR 0.000250    Time 0.092453    
Epoch: [146][   50/  207]    Overall Loss 0.012832    Objective Loss 0.012832                                        LR 0.000250    Time 0.092139    
Epoch: [146][   60/  207]    Overall Loss 0.012740    Objective Loss 0.012740                                        LR 0.000250    Time 0.089269    
Epoch: [146][   70/  207]    Overall Loss 0.012585    Objective Loss 0.012585                                        LR 0.000250    Time 0.086542    
Epoch: [146][   80/  207]    Overall Loss 0.012360    Objective Loss 0.012360                                        LR 0.000250    Time 0.084448    
Epoch: [146][   90/  207]    Overall Loss 0.012377    Objective Loss 0.012377                                        LR 0.000250    Time 0.083104    
Epoch: [146][  100/  207]    Overall Loss 0.012381    Objective Loss 0.012381                                        LR 0.000250    Time 0.081812    
Epoch: [146][  110/  207]    Overall Loss 0.012443    Objective Loss 0.012443                                        LR 0.000250    Time 0.080763    
Epoch: [146][  120/  207]    Overall Loss 0.012379    Objective Loss 0.012379                                        LR 0.000250    Time 0.080008    
Epoch: [146][  130/  207]    Overall Loss 0.012424    Objective Loss 0.012424                                        LR 0.000250    Time 0.079341    
Epoch: [146][  140/  207]    Overall Loss 0.012427    Objective Loss 0.012427                                        LR 0.000250    Time 0.078809    
Epoch: [146][  150/  207]    Overall Loss 0.012588    Objective Loss 0.012588                                        LR 0.000250    Time 0.078290    
Epoch: [146][  160/  207]    Overall Loss 0.012646    Objective Loss 0.012646                                        LR 0.000250    Time 0.078226    
Epoch: [146][  170/  207]    Overall Loss 0.012619    Objective Loss 0.012619                                        LR 0.000250    Time 0.077885    
Epoch: [146][  180/  207]    Overall Loss 0.012624    Objective Loss 0.012624                                        LR 0.000250    Time 0.077545    
Epoch: [146][  190/  207]    Overall Loss 0.012629    Objective Loss 0.012629                                        LR 0.000250    Time 0.077797    
Epoch: [146][  200/  207]    Overall Loss 0.012599    Objective Loss 0.012599                                        LR 0.000250    Time 0.078025    
Epoch: [146][  207/  207]    Overall Loss 0.012671    Objective Loss 0.012671    Top1 98.187995    Top5 100.000000    LR 0.000250    Time 0.077726    
--- validate (epoch=146)-----------
5136 samples (512 per mini-batch)
Epoch: [146][   10/   11]    Loss 0.529761    Top1 85.742188    Top5 99.726562    
Epoch: [146][   11/   11]    Loss 0.629913    Top1 85.708723    Top5 99.727414    
==> Top1: 85.709    Top5: 99.727    Loss: 0.630

==> Confusion:
[[270   2   3   3   2   0   2  18]
 [  1 264  21   0   0   3   1  10]
 [  1  17 275   0   0   3   0   4]
 [  0   2   1 750  50  11   7  16]
 [  2   0   0  43 772   6  23  33]
 [  5   7  10   6  18 796  12  40]
 [  1   0   0   7  27   9 769  24]
 [ 17  13  18  39  71 105  20 506]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [147][   10/  207]    Overall Loss 0.010479    Objective Loss 0.010479                                        LR 0.000250    Time 0.157670    
Epoch: [147][   20/  207]    Overall Loss 0.011541    Objective Loss 0.011541                                        LR 0.000250    Time 0.124163    
Epoch: [147][   30/  207]    Overall Loss 0.011590    Objective Loss 0.011590                                        LR 0.000250    Time 0.112568    
Epoch: [147][   40/  207]    Overall Loss 0.011934    Objective Loss 0.011934                                        LR 0.000250    Time 0.107220    
Epoch: [147][   50/  207]    Overall Loss 0.011978    Objective Loss 0.011978                                        LR 0.000250    Time 0.101337    
Epoch: [147][   60/  207]    Overall Loss 0.011884    Objective Loss 0.011884                                        LR 0.000250    Time 0.096210    
Epoch: [147][   70/  207]    Overall Loss 0.011876    Objective Loss 0.011876                                        LR 0.000250    Time 0.092793    
Epoch: [147][   80/  207]    Overall Loss 0.011796    Objective Loss 0.011796                                        LR 0.000250    Time 0.090357    
Epoch: [147][   90/  207]    Overall Loss 0.011881    Objective Loss 0.011881                                        LR 0.000250    Time 0.088856    
Epoch: [147][  100/  207]    Overall Loss 0.011848    Objective Loss 0.011848                                        LR 0.000250    Time 0.087364    
Epoch: [147][  110/  207]    Overall Loss 0.012056    Objective Loss 0.012056                                        LR 0.000250    Time 0.086299    
Epoch: [147][  120/  207]    Overall Loss 0.012052    Objective Loss 0.012052                                        LR 0.000250    Time 0.085669    
Epoch: [147][  130/  207]    Overall Loss 0.012006    Objective Loss 0.012006                                        LR 0.000250    Time 0.084643    
Epoch: [147][  140/  207]    Overall Loss 0.011897    Objective Loss 0.011897                                        LR 0.000250    Time 0.083831    
Epoch: [147][  150/  207]    Overall Loss 0.012108    Objective Loss 0.012108                                        LR 0.000250    Time 0.083165    
Epoch: [147][  160/  207]    Overall Loss 0.012093    Objective Loss 0.012093                                        LR 0.000250    Time 0.082505    
Epoch: [147][  170/  207]    Overall Loss 0.012163    Objective Loss 0.012163                                        LR 0.000250    Time 0.081883    
Epoch: [147][  180/  207]    Overall Loss 0.012180    Objective Loss 0.012180                                        LR 0.000250    Time 0.081281    
Epoch: [147][  190/  207]    Overall Loss 0.012295    Objective Loss 0.012295                                        LR 0.000250    Time 0.080773    
Epoch: [147][  200/  207]    Overall Loss 0.012482    Objective Loss 0.012482                                        LR 0.000250    Time 0.080377    
Epoch: [147][  207/  207]    Overall Loss 0.012501    Objective Loss 0.012501    Top1 98.867497    Top5 100.000000    LR 0.000250    Time 0.079948    
--- validate (epoch=147)-----------
5136 samples (512 per mini-batch)
Epoch: [147][   10/   11]    Loss 0.592709    Top1 85.761719    Top5 99.726562    
Epoch: [147][   11/   11]    Loss 0.581209    Top1 85.708723    Top5 99.727414    
==> Top1: 85.709    Top5: 99.727    Loss: 0.581

==> Confusion:
[[271   2   3   3   0   0   2  19]
 [  1 257  25   0   0   3   2  12]
 [  1  16 269   1   0   3   0  10]
 [  0   1   1 744  38  10  11  32]
 [  2   0   0  48 755   6  20  48]
 [  3   5   7   6  17 774  18  64]
 [  1   0   0   5  20   9 767  35]
 [ 16   7  13  35  56  77  20 565]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [148][   10/  207]    Overall Loss 0.013266    Objective Loss 0.013266                                        LR 0.000250    Time 0.127938    
Epoch: [148][   20/  207]    Overall Loss 0.011922    Objective Loss 0.011922                                        LR 0.000250    Time 0.101501    
Epoch: [148][   30/  207]    Overall Loss 0.012255    Objective Loss 0.012255                                        LR 0.000250    Time 0.093190    
Epoch: [148][   40/  207]    Overall Loss 0.012556    Objective Loss 0.012556                                        LR 0.000250    Time 0.088074    
Epoch: [148][   50/  207]    Overall Loss 0.012641    Objective Loss 0.012641                                        LR 0.000250    Time 0.084877    
Epoch: [148][   60/  207]    Overall Loss 0.012628    Objective Loss 0.012628                                        LR 0.000250    Time 0.082629    
Epoch: [148][   70/  207]    Overall Loss 0.012403    Objective Loss 0.012403                                        LR 0.000250    Time 0.081440    
Epoch: [148][   80/  207]    Overall Loss 0.012161    Objective Loss 0.012161                                        LR 0.000250    Time 0.080695    
Epoch: [148][   90/  207]    Overall Loss 0.012238    Objective Loss 0.012238                                        LR 0.000250    Time 0.080448    
Epoch: [148][  100/  207]    Overall Loss 0.012337    Objective Loss 0.012337                                        LR 0.000250    Time 0.080548    
Epoch: [148][  110/  207]    Overall Loss 0.012253    Objective Loss 0.012253                                        LR 0.000250    Time 0.080424    
Epoch: [148][  120/  207]    Overall Loss 0.012124    Objective Loss 0.012124                                        LR 0.000250    Time 0.080538    
Epoch: [148][  130/  207]    Overall Loss 0.012167    Objective Loss 0.012167                                        LR 0.000250    Time 0.082643    
Epoch: [148][  140/  207]    Overall Loss 0.012253    Objective Loss 0.012253                                        LR 0.000250    Time 0.084202    
Epoch: [148][  150/  207]    Overall Loss 0.012362    Objective Loss 0.012362                                        LR 0.000250    Time 0.084198    
Epoch: [148][  160/  207]    Overall Loss 0.012414    Objective Loss 0.012414                                        LR 0.000250    Time 0.083935    
Epoch: [148][  170/  207]    Overall Loss 0.012426    Objective Loss 0.012426                                        LR 0.000250    Time 0.083714    
Epoch: [148][  180/  207]    Overall Loss 0.012408    Objective Loss 0.012408                                        LR 0.000250    Time 0.083261    
Epoch: [148][  190/  207]    Overall Loss 0.012492    Objective Loss 0.012492                                        LR 0.000250    Time 0.082755    
Epoch: [148][  200/  207]    Overall Loss 0.012506    Objective Loss 0.012506                                        LR 0.000250    Time 0.082187    
Epoch: [148][  207/  207]    Overall Loss 0.012534    Objective Loss 0.012534    Top1 98.867497    Top5 100.000000    LR 0.000250    Time 0.081837    
--- validate (epoch=148)-----------
5136 samples (512 per mini-batch)
Epoch: [148][   10/   11]    Loss 0.546983    Top1 85.507812    Top5 99.765625    
Epoch: [148][   11/   11]    Loss 0.552191    Top1 85.533489    Top5 99.766355    
==> Top1: 85.533    Top5: 99.766    Loss: 0.552

==> Confusion:
[[275   2   3   3   1   0   2  14]
 [  2 258  24   0   0   3   1  12]
 [  2  15 275   0   0   3   0   5]
 [  0   1   1 749  49  10  12  15]
 [  2   0   0  40 772   4  29  32]
 [  4   9   7   8  18 793  16  39]
 [  2   0   0   8  31  10 763  23]
 [ 20  11  15  39  65  99  32 508]]

==> Best [Top1: 85.767   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 123]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [149][   10/  207]    Overall Loss 0.012760    Objective Loss 0.012760                                        LR 0.000250    Time 0.167870    
Epoch: [149][   20/  207]    Overall Loss 0.012705    Objective Loss 0.012705                                        LR 0.000250    Time 0.124648    
Epoch: [149][   30/  207]    Overall Loss 0.012707    Objective Loss 0.012707                                        LR 0.000250    Time 0.109526    
Epoch: [149][   40/  207]    Overall Loss 0.012910    Objective Loss 0.012910                                        LR 0.000250    Time 0.104164    
Epoch: [149][   50/  207]    Overall Loss 0.012560    Objective Loss 0.012560                                        LR 0.000250    Time 0.098712    
Epoch: [149][   60/  207]    Overall Loss 0.012468    Objective Loss 0.012468                                        LR 0.000250    Time 0.096663    
Epoch: [149][   70/  207]    Overall Loss 0.012432    Objective Loss 0.012432                                        LR 0.000250    Time 0.095068    
Epoch: [149][   80/  207]    Overall Loss 0.012358    Objective Loss 0.012358                                        LR 0.000250    Time 0.092875    
Epoch: [149][   90/  207]    Overall Loss 0.012663    Objective Loss 0.012663                                        LR 0.000250    Time 0.090480    
Epoch: [149][  100/  207]    Overall Loss 0.012553    Objective Loss 0.012553                                        LR 0.000250    Time 0.088906    
Epoch: [149][  110/  207]    Overall Loss 0.012528    Objective Loss 0.012528                                        LR 0.000250    Time 0.088073    
Epoch: [149][  120/  207]    Overall Loss 0.012633    Objective Loss 0.012633                                        LR 0.000250    Time 0.087418    
Epoch: [149][  130/  207]    Overall Loss 0.012625    Objective Loss 0.012625                                        LR 0.000250    Time 0.086219    
Epoch: [149][  140/  207]    Overall Loss 0.012535    Objective Loss 0.012535                                        LR 0.000250    Time 0.085265    
Epoch: [149][  150/  207]    Overall Loss 0.012476    Objective Loss 0.012476                                        LR 0.000250    Time 0.084317    
Epoch: [149][  160/  207]    Overall Loss 0.012383    Objective Loss 0.012383                                        LR 0.000250    Time 0.083679    
Epoch: [149][  170/  207]    Overall Loss 0.012461    Objective Loss 0.012461                                        LR 0.000250    Time 0.083571    
Epoch: [149][  180/  207]    Overall Loss 0.012444    Objective Loss 0.012444                                        LR 0.000250    Time 0.083195    
Epoch: [149][  190/  207]    Overall Loss 0.012468    Objective Loss 0.012468                                        LR 0.000250    Time 0.083192    
Epoch: [149][  200/  207]    Overall Loss 0.012493    Objective Loss 0.012493                                        LR 0.000250    Time 0.082903    
Epoch: [149][  207/  207]    Overall Loss 0.012496    Objective Loss 0.012496    Top1 99.093998    Top5 100.000000    LR 0.000250    Time 0.082711    
--- validate (epoch=149)-----------
5136 samples (512 per mini-batch)
Epoch: [149][   10/   11]    Loss 0.620768    Top1 85.957031    Top5 99.785156    
Epoch: [149][   11/   11]    Loss 0.582782    Top1 85.981308    Top5 99.785826    
==> Top1: 85.981    Top5: 99.786    Loss: 0.583

==> Confusion:
[[269   2   3   2   0   0   2  22]
 [  1 258  22   0   0   4   1  14]
 [  1  12 272   0   0   3   0  12]
 [  0   0   0 742  49   9   8  29]
 [  2   0   0  34 774   8  18  43]
 [  3   4   8   6  14 799  11  49]
 [  1   0   0   4  27  12 758  35]
 [ 15   7  11  28  58 106  20 544]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [150][   10/  207]    Overall Loss 0.012662    Objective Loss 0.012662                                        LR 0.000250    Time 0.144959    
Epoch: [150][   20/  207]    Overall Loss 0.013903    Objective Loss 0.013903                                        LR 0.000250    Time 0.111653    
Epoch: [150][   30/  207]    Overall Loss 0.013074    Objective Loss 0.013074                                        LR 0.000250    Time 0.102766    
Epoch: [150][   40/  207]    Overall Loss 0.012887    Objective Loss 0.012887                                        LR 0.000250    Time 0.097449    
Epoch: [150][   50/  207]    Overall Loss 0.013020    Objective Loss 0.013020                                        LR 0.000250    Time 0.094005    
Epoch: [150][   60/  207]    Overall Loss 0.012965    Objective Loss 0.012965                                        LR 0.000250    Time 0.091619    
Epoch: [150][   70/  207]    Overall Loss 0.012729    Objective Loss 0.012729                                        LR 0.000250    Time 0.090015    
Epoch: [150][   80/  207]    Overall Loss 0.012734    Objective Loss 0.012734                                        LR 0.000250    Time 0.088535    
Epoch: [150][   90/  207]    Overall Loss 0.012646    Objective Loss 0.012646                                        LR 0.000250    Time 0.087724    
Epoch: [150][  100/  207]    Overall Loss 0.012660    Objective Loss 0.012660                                        LR 0.000250    Time 0.087118    
Epoch: [150][  110/  207]    Overall Loss 0.012509    Objective Loss 0.012509                                        LR 0.000250    Time 0.086412    
Epoch: [150][  120/  207]    Overall Loss 0.012579    Objective Loss 0.012579                                        LR 0.000250    Time 0.085691    
Epoch: [150][  130/  207]    Overall Loss 0.012476    Objective Loss 0.012476                                        LR 0.000250    Time 0.085329    
Epoch: [150][  140/  207]    Overall Loss 0.012461    Objective Loss 0.012461                                        LR 0.000250    Time 0.084875    
Epoch: [150][  150/  207]    Overall Loss 0.012611    Objective Loss 0.012611                                        LR 0.000250    Time 0.084787    
Epoch: [150][  160/  207]    Overall Loss 0.012924    Objective Loss 0.012924                                        LR 0.000250    Time 0.085142    
Epoch: [150][  170/  207]    Overall Loss 0.013139    Objective Loss 0.013139                                        LR 0.000250    Time 0.084974    
Epoch: [150][  180/  207]    Overall Loss 0.013076    Objective Loss 0.013076                                        LR 0.000250    Time 0.084496    
Epoch: [150][  190/  207]    Overall Loss 0.013080    Objective Loss 0.013080                                        LR 0.000250    Time 0.083851    
Epoch: [150][  200/  207]    Overall Loss 0.013408    Objective Loss 0.013408                                        LR 0.000250    Time 0.083366    
Epoch: [150][  207/  207]    Overall Loss 0.013607    Objective Loss 0.013607    Top1 98.527746    Top5 99.886750    LR 0.000250    Time 0.082956    
--- validate (epoch=150)-----------
5136 samples (512 per mini-batch)
Epoch: [150][   10/   11]    Loss 0.698097    Top1 84.843750    Top5 99.648438    
Epoch: [150][   11/   11]    Loss 0.664878    Top1 84.852025    Top5 99.649533    
==> Top1: 84.852    Top5: 99.650    Loss: 0.665

==> Confusion:
[[271   2   2   3   1   0   2  19]
 [  2 259  20   0   0   1   1  17]
 [  2  18 254   1   0   2   0  23]
 [  0   1   1 738  62   7   6  22]
 [  1   0   0  31 800   1  15  31]
 [  5   5   8   8  32 733  24  79]
 [  1   0   0   4  42   4 750  36]
 [ 15   6  12  38  82  62  21 553]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [151][   10/  207]    Overall Loss 0.016233    Objective Loss 0.016233                                        LR 0.000250    Time 0.140520    
Epoch: [151][   20/  207]    Overall Loss 0.018762    Objective Loss 0.018762                                        LR 0.000250    Time 0.107183    
Epoch: [151][   30/  207]    Overall Loss 0.017036    Objective Loss 0.017036                                        LR 0.000250    Time 0.098622    
Epoch: [151][   40/  207]    Overall Loss 0.016300    Objective Loss 0.016300                                        LR 0.000250    Time 0.092613    
Epoch: [151][   50/  207]    Overall Loss 0.016309    Objective Loss 0.016309                                        LR 0.000250    Time 0.090133    
Epoch: [151][   60/  207]    Overall Loss 0.015809    Objective Loss 0.015809                                        LR 0.000250    Time 0.088554    
Epoch: [151][   70/  207]    Overall Loss 0.015417    Objective Loss 0.015417                                        LR 0.000250    Time 0.087340    
Epoch: [151][   80/  207]    Overall Loss 0.015259    Objective Loss 0.015259                                        LR 0.000250    Time 0.085713    
Epoch: [151][   90/  207]    Overall Loss 0.014886    Objective Loss 0.014886                                        LR 0.000250    Time 0.085056    
Epoch: [151][  100/  207]    Overall Loss 0.014766    Objective Loss 0.014766                                        LR 0.000250    Time 0.084549    
Epoch: [151][  110/  207]    Overall Loss 0.014725    Objective Loss 0.014725                                        LR 0.000250    Time 0.083633    
Epoch: [151][  120/  207]    Overall Loss 0.014511    Objective Loss 0.014511                                        LR 0.000250    Time 0.082759    
Epoch: [151][  130/  207]    Overall Loss 0.014337    Objective Loss 0.014337                                        LR 0.000250    Time 0.081894    
Epoch: [151][  140/  207]    Overall Loss 0.014235    Objective Loss 0.014235                                        LR 0.000250    Time 0.081193    
Epoch: [151][  150/  207]    Overall Loss 0.014345    Objective Loss 0.014345                                        LR 0.000250    Time 0.080446    
Epoch: [151][  160/  207]    Overall Loss 0.014272    Objective Loss 0.014272                                        LR 0.000250    Time 0.079967    
Epoch: [151][  170/  207]    Overall Loss 0.014320    Objective Loss 0.014320                                        LR 0.000250    Time 0.079398    
Epoch: [151][  180/  207]    Overall Loss 0.014435    Objective Loss 0.014435                                        LR 0.000250    Time 0.079047    
Epoch: [151][  190/  207]    Overall Loss 0.014633    Objective Loss 0.014633                                        LR 0.000250    Time 0.078697    
Epoch: [151][  200/  207]    Overall Loss 0.015066    Objective Loss 0.015066                                        LR 0.000250    Time 0.078299    
Epoch: [151][  207/  207]    Overall Loss 0.015123    Objective Loss 0.015123    Top1 98.414496    Top5 100.000000    LR 0.000250    Time 0.077950    
--- validate (epoch=151)-----------
5136 samples (512 per mini-batch)
Epoch: [151][   10/   11]    Loss 0.613276    Top1 84.863281    Top5 99.785156    
Epoch: [151][   11/   11]    Loss 0.582372    Top1 84.871495    Top5 99.785826    
==> Top1: 84.871    Top5: 99.786    Loss: 0.582

==> Confusion:
[[264   1  10   3   2   1   1  18]
 [  1 244  42   0   0   4   1   8]
 [  0  11 283   0   0   3   0   3]
 [  0   2   1 762  46  10   5  11]
 [  2   0   0  57 768   4  18  30]
 [  3   5  12   9  21 781  15  48]
 [  1   0   0   7  40   9 760  20]
 [ 18  11  23  55  74  95  16 497]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [152][   10/  207]    Overall Loss 0.017110    Objective Loss 0.017110                                        LR 0.000250    Time 0.136112    
Epoch: [152][   20/  207]    Overall Loss 0.016233    Objective Loss 0.016233                                        LR 0.000250    Time 0.104645    
Epoch: [152][   30/  207]    Overall Loss 0.017026    Objective Loss 0.017026                                        LR 0.000250    Time 0.093538    
Epoch: [152][   40/  207]    Overall Loss 0.017328    Objective Loss 0.017328                                        LR 0.000250    Time 0.088988    
Epoch: [152][   50/  207]    Overall Loss 0.017376    Objective Loss 0.017376                                        LR 0.000250    Time 0.085755    
Epoch: [152][   60/  207]    Overall Loss 0.017282    Objective Loss 0.017282                                        LR 0.000250    Time 0.083368    
Epoch: [152][   70/  207]    Overall Loss 0.017336    Objective Loss 0.017336                                        LR 0.000250    Time 0.082002    
Epoch: [152][   80/  207]    Overall Loss 0.017160    Objective Loss 0.017160                                        LR 0.000250    Time 0.080876    
Epoch: [152][   90/  207]    Overall Loss 0.016947    Objective Loss 0.016947                                        LR 0.000250    Time 0.080665    
Epoch: [152][  100/  207]    Overall Loss 0.016822    Objective Loss 0.016822                                        LR 0.000250    Time 0.080007    
Epoch: [152][  110/  207]    Overall Loss 0.017024    Objective Loss 0.017024                                        LR 0.000250    Time 0.079768    
Epoch: [152][  120/  207]    Overall Loss 0.017007    Objective Loss 0.017007                                        LR 0.000250    Time 0.079781    
Epoch: [152][  130/  207]    Overall Loss 0.016889    Objective Loss 0.016889                                        LR 0.000250    Time 0.079824    
Epoch: [152][  140/  207]    Overall Loss 0.017150    Objective Loss 0.017150                                        LR 0.000250    Time 0.079796    
Epoch: [152][  150/  207]    Overall Loss 0.017072    Objective Loss 0.017072                                        LR 0.000250    Time 0.079803    
Epoch: [152][  160/  207]    Overall Loss 0.016909    Objective Loss 0.016909                                        LR 0.000250    Time 0.079826    
Epoch: [152][  170/  207]    Overall Loss 0.016693    Objective Loss 0.016693                                        LR 0.000250    Time 0.079459    
Epoch: [152][  180/  207]    Overall Loss 0.016744    Objective Loss 0.016744                                        LR 0.000250    Time 0.078999    
Epoch: [152][  190/  207]    Overall Loss 0.017013    Objective Loss 0.017013                                        LR 0.000250    Time 0.078591    
Epoch: [152][  200/  207]    Overall Loss 0.017017    Objective Loss 0.017017                                        LR 0.000250    Time 0.078423    
Epoch: [152][  207/  207]    Overall Loss 0.017044    Objective Loss 0.017044    Top1 98.980747    Top5 100.000000    LR 0.000250    Time 0.078199    
--- validate (epoch=152)-----------
5136 samples (512 per mini-batch)
Epoch: [152][   10/   11]    Loss 0.669063    Top1 85.410156    Top5 99.746094    
Epoch: [152][   11/   11]    Loss 0.610781    Top1 85.455607    Top5 99.746885    
==> Top1: 85.456    Top5: 99.747    Loss: 0.611

==> Confusion:
[[263   1   5   3   2   0   1  25]
 [  1 248  29   0   0   2   2  18]
 [  1  15 270   1   0   2   0  11]
 [  0   0   0 751  49   7   5  25]
 [  1   0   0  49 781   1  11  36]
 [  5   6   7  13  20 764  10  69]
 [  1   0   0   7  35   5 748  41]
 [  9   8  16  47  62  68  15 564]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [153][   10/  207]    Overall Loss 0.019005    Objective Loss 0.019005                                        LR 0.000250    Time 0.135788    
Epoch: [153][   20/  207]    Overall Loss 0.019259    Objective Loss 0.019259                                        LR 0.000250    Time 0.106723    
Epoch: [153][   30/  207]    Overall Loss 0.019216    Objective Loss 0.019216                                        LR 0.000250    Time 0.094673    
Epoch: [153][   40/  207]    Overall Loss 0.020770    Objective Loss 0.020770                                        LR 0.000250    Time 0.089767    
Epoch: [153][   50/  207]    Overall Loss 0.021720    Objective Loss 0.021720                                        LR 0.000250    Time 0.087114    
Epoch: [153][   60/  207]    Overall Loss 0.022221    Objective Loss 0.022221                                        LR 0.000250    Time 0.085742    
Epoch: [153][   70/  207]    Overall Loss 0.022153    Objective Loss 0.022153                                        LR 0.000250    Time 0.084133    
Epoch: [153][   80/  207]    Overall Loss 0.023343    Objective Loss 0.023343                                        LR 0.000250    Time 0.082606    
Epoch: [153][   90/  207]    Overall Loss 0.024627    Objective Loss 0.024627                                        LR 0.000250    Time 0.081435    
Epoch: [153][  100/  207]    Overall Loss 0.024918    Objective Loss 0.024918                                        LR 0.000250    Time 0.080516    
Epoch: [153][  110/  207]    Overall Loss 0.025212    Objective Loss 0.025212                                        LR 0.000250    Time 0.079971    
Epoch: [153][  120/  207]    Overall Loss 0.025248    Objective Loss 0.025248                                        LR 0.000250    Time 0.079485    
Epoch: [153][  130/  207]    Overall Loss 0.025465    Objective Loss 0.025465                                        LR 0.000250    Time 0.079182    
Epoch: [153][  140/  207]    Overall Loss 0.026134    Objective Loss 0.026134                                        LR 0.000250    Time 0.079184    
Epoch: [153][  150/  207]    Overall Loss 0.026205    Objective Loss 0.026205                                        LR 0.000250    Time 0.078982    
Epoch: [153][  160/  207]    Overall Loss 0.026060    Objective Loss 0.026060                                        LR 0.000250    Time 0.078809    
Epoch: [153][  170/  207]    Overall Loss 0.026305    Objective Loss 0.026305                                        LR 0.000250    Time 0.078508    
Epoch: [153][  180/  207]    Overall Loss 0.026334    Objective Loss 0.026334                                        LR 0.000250    Time 0.078449    
Epoch: [153][  190/  207]    Overall Loss 0.026375    Objective Loss 0.026375                                        LR 0.000250    Time 0.078449    
Epoch: [153][  200/  207]    Overall Loss 0.026458    Objective Loss 0.026458                                        LR 0.000250    Time 0.078635    
Epoch: [153][  207/  207]    Overall Loss 0.026467    Objective Loss 0.026467    Top1 96.602492    Top5 99.886750    LR 0.000250    Time 0.078975    
--- validate (epoch=153)-----------
5136 samples (512 per mini-batch)
Epoch: [153][   10/   11]    Loss 0.533311    Top1 84.531250    Top5 99.707031    
Epoch: [153][   11/   11]    Loss 0.530813    Top1 84.462617    Top5 99.707944    
==> Top1: 84.463    Top5: 99.708    Loss: 0.531

==> Confusion:
[[272   3   7   3   2   1   2  10]
 [  3 252  32   2   0   4   0   7]
 [  2  17 274   0   0   4   0   3]
 [  0   0   1 754  47  12   9  14]
 [  2   0   0  54 766  10  22  25]
 [  5   9   8   9  18 810  18  17]
 [  2   1   0   3  32  13 770  16]
 [ 26  15  21  48  69 144  26 440]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [154][   10/  207]    Overall Loss 0.022305    Objective Loss 0.022305                                        LR 0.000250    Time 0.163981    
Epoch: [154][   20/  207]    Overall Loss 0.024263    Objective Loss 0.024263                                        LR 0.000250    Time 0.118308    
Epoch: [154][   30/  207]    Overall Loss 0.023108    Objective Loss 0.023108                                        LR 0.000250    Time 0.102802    
Epoch: [154][   40/  207]    Overall Loss 0.022361    Objective Loss 0.022361                                        LR 0.000250    Time 0.096426    
Epoch: [154][   50/  207]    Overall Loss 0.022261    Objective Loss 0.022261                                        LR 0.000250    Time 0.091164    
Epoch: [154][   60/  207]    Overall Loss 0.021222    Objective Loss 0.021222                                        LR 0.000250    Time 0.087889    
Epoch: [154][   70/  207]    Overall Loss 0.020615    Objective Loss 0.020615                                        LR 0.000250    Time 0.085521    
Epoch: [154][   80/  207]    Overall Loss 0.020188    Objective Loss 0.020188                                        LR 0.000250    Time 0.083727    
Epoch: [154][   90/  207]    Overall Loss 0.019881    Objective Loss 0.019881                                        LR 0.000250    Time 0.083085    
Epoch: [154][  100/  207]    Overall Loss 0.019412    Objective Loss 0.019412                                        LR 0.000250    Time 0.082261    
Epoch: [154][  110/  207]    Overall Loss 0.019138    Objective Loss 0.019138                                        LR 0.000250    Time 0.081277    
Epoch: [154][  120/  207]    Overall Loss 0.018594    Objective Loss 0.018594                                        LR 0.000250    Time 0.080440    
Epoch: [154][  130/  207]    Overall Loss 0.018327    Objective Loss 0.018327                                        LR 0.000250    Time 0.079911    
Epoch: [154][  140/  207]    Overall Loss 0.018068    Objective Loss 0.018068                                        LR 0.000250    Time 0.079466    
Epoch: [154][  150/  207]    Overall Loss 0.017807    Objective Loss 0.017807                                        LR 0.000250    Time 0.079669    
Epoch: [154][  160/  207]    Overall Loss 0.017576    Objective Loss 0.017576                                        LR 0.000250    Time 0.079430    
Epoch: [154][  170/  207]    Overall Loss 0.017346    Objective Loss 0.017346                                        LR 0.000250    Time 0.078946    
Epoch: [154][  180/  207]    Overall Loss 0.017375    Objective Loss 0.017375                                        LR 0.000250    Time 0.078547    
Epoch: [154][  190/  207]    Overall Loss 0.017215    Objective Loss 0.017215                                        LR 0.000250    Time 0.078105    
Epoch: [154][  200/  207]    Overall Loss 0.016976    Objective Loss 0.016976                                        LR 0.000250    Time 0.077844    
Epoch: [154][  207/  207]    Overall Loss 0.016932    Objective Loss 0.016932    Top1 97.734994    Top5 100.000000    LR 0.000250    Time 0.077523    
--- validate (epoch=154)-----------
5136 samples (512 per mini-batch)
Epoch: [154][   10/   11]    Loss 0.522954    Top1 85.722656    Top5 99.687500    
Epoch: [154][   11/   11]    Loss 0.488078    Top1 85.728193    Top5 99.688474    
==> Top1: 85.728    Top5: 99.688    Loss: 0.488

==> Confusion:
[[276   3   3   4   0   0   2  12]
 [  1 264  24   0   0   4   1   6]
 [  1  18 272   1   0   3   0   5]
 [  0   0   1 750  54  10   9  13]
 [  2   0   0  39 777   6  24  31]
 [  5   6   8   7  17 803  16  32]
 [  1   0   0   6  26   9 777  18]
 [ 21  14  14  42  67 115  32 484]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [155][   10/  207]    Overall Loss 0.013631    Objective Loss 0.013631                                        LR 0.000250    Time 0.139171    
Epoch: [155][   20/  207]    Overall Loss 0.013201    Objective Loss 0.013201                                        LR 0.000250    Time 0.104498    
Epoch: [155][   30/  207]    Overall Loss 0.012408    Objective Loss 0.012408                                        LR 0.000250    Time 0.093262    
Epoch: [155][   40/  207]    Overall Loss 0.012369    Objective Loss 0.012369                                        LR 0.000250    Time 0.091086    
Epoch: [155][   50/  207]    Overall Loss 0.012427    Objective Loss 0.012427                                        LR 0.000250    Time 0.088359    
Epoch: [155][   60/  207]    Overall Loss 0.012367    Objective Loss 0.012367                                        LR 0.000250    Time 0.085771    
Epoch: [155][   70/  207]    Overall Loss 0.012165    Objective Loss 0.012165                                        LR 0.000250    Time 0.083794    
Epoch: [155][   80/  207]    Overall Loss 0.012106    Objective Loss 0.012106                                        LR 0.000250    Time 0.082283    
Epoch: [155][   90/  207]    Overall Loss 0.012085    Objective Loss 0.012085                                        LR 0.000250    Time 0.081330    
Epoch: [155][  100/  207]    Overall Loss 0.012123    Objective Loss 0.012123                                        LR 0.000250    Time 0.080310    
Epoch: [155][  110/  207]    Overall Loss 0.012213    Objective Loss 0.012213                                        LR 0.000250    Time 0.079969    
Epoch: [155][  120/  207]    Overall Loss 0.012065    Objective Loss 0.012065                                        LR 0.000250    Time 0.079644    
Epoch: [155][  130/  207]    Overall Loss 0.012067    Objective Loss 0.012067                                        LR 0.000250    Time 0.079635    
Epoch: [155][  140/  207]    Overall Loss 0.012128    Objective Loss 0.012128                                        LR 0.000250    Time 0.079504    
Epoch: [155][  150/  207]    Overall Loss 0.012167    Objective Loss 0.012167                                        LR 0.000250    Time 0.079322    
Epoch: [155][  160/  207]    Overall Loss 0.012102    Objective Loss 0.012102                                        LR 0.000250    Time 0.079032    
Epoch: [155][  170/  207]    Overall Loss 0.012198    Objective Loss 0.012198                                        LR 0.000250    Time 0.078709    
Epoch: [155][  180/  207]    Overall Loss 0.012277    Objective Loss 0.012277                                        LR 0.000250    Time 0.078423    
Epoch: [155][  190/  207]    Overall Loss 0.012334    Objective Loss 0.012334                                        LR 0.000250    Time 0.078090    
Epoch: [155][  200/  207]    Overall Loss 0.012418    Objective Loss 0.012418                                        LR 0.000250    Time 0.078153    
Epoch: [155][  207/  207]    Overall Loss 0.012332    Objective Loss 0.012332    Top1 98.754247    Top5 100.000000    LR 0.000250    Time 0.078136    
--- validate (epoch=155)-----------
5136 samples (512 per mini-batch)
Epoch: [155][   10/   11]    Loss 0.571188    Top1 85.683594    Top5 99.746094    
Epoch: [155][   11/   11]    Loss 0.542298    Top1 85.669782    Top5 99.746885    
==> Top1: 85.670    Top5: 99.747    Loss: 0.542

==> Confusion:
[[271   2   3   3   0   0   2  19]
 [  1 260  22   0   0   4   1  12]
 [  2  17 267   0   0   4   0  10]
 [  0   0   1 744  53   9  11  19]
 [  2   0   0  36 777   6  22  36]
 [  4   6   7   5  19 795  13  45]
 [  1   0   0   4  27  11 767  27]
 [ 17   9  15  35  67 100  27 519]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [156][   10/  207]    Overall Loss 0.011006    Objective Loss 0.011006                                        LR 0.000250    Time 0.126440    
Epoch: [156][   20/  207]    Overall Loss 0.012018    Objective Loss 0.012018                                        LR 0.000250    Time 0.101417    
Epoch: [156][   30/  207]    Overall Loss 0.012093    Objective Loss 0.012093                                        LR 0.000250    Time 0.092914    
Epoch: [156][   40/  207]    Overall Loss 0.011460    Objective Loss 0.011460                                        LR 0.000250    Time 0.096967    
Epoch: [156][   50/  207]    Overall Loss 0.011856    Objective Loss 0.011856                                        LR 0.000250    Time 0.092771    
Epoch: [156][   60/  207]    Overall Loss 0.011814    Objective Loss 0.011814                                        LR 0.000250    Time 0.091232    
Epoch: [156][   70/  207]    Overall Loss 0.011623    Objective Loss 0.011623                                        LR 0.000250    Time 0.089276    
Epoch: [156][   80/  207]    Overall Loss 0.011592    Objective Loss 0.011592                                        LR 0.000250    Time 0.088006    
Epoch: [156][   90/  207]    Overall Loss 0.011733    Objective Loss 0.011733                                        LR 0.000250    Time 0.086375    
Epoch: [156][  100/  207]    Overall Loss 0.011710    Objective Loss 0.011710                                        LR 0.000250    Time 0.085433    
Epoch: [156][  110/  207]    Overall Loss 0.011718    Objective Loss 0.011718                                        LR 0.000250    Time 0.084823    
Epoch: [156][  120/  207]    Overall Loss 0.011643    Objective Loss 0.011643                                        LR 0.000250    Time 0.084261    
Epoch: [156][  130/  207]    Overall Loss 0.011615    Objective Loss 0.011615                                        LR 0.000250    Time 0.083475    
Epoch: [156][  140/  207]    Overall Loss 0.011687    Objective Loss 0.011687                                        LR 0.000250    Time 0.082652    
Epoch: [156][  150/  207]    Overall Loss 0.011643    Objective Loss 0.011643                                        LR 0.000250    Time 0.082206    
Epoch: [156][  160/  207]    Overall Loss 0.011700    Objective Loss 0.011700                                        LR 0.000250    Time 0.081569    
Epoch: [156][  170/  207]    Overall Loss 0.011779    Objective Loss 0.011779                                        LR 0.000250    Time 0.080997    
Epoch: [156][  180/  207]    Overall Loss 0.011755    Objective Loss 0.011755                                        LR 0.000250    Time 0.080585    
Epoch: [156][  190/  207]    Overall Loss 0.011743    Objective Loss 0.011743                                        LR 0.000250    Time 0.080132    
Epoch: [156][  200/  207]    Overall Loss 0.011885    Objective Loss 0.011885                                        LR 0.000250    Time 0.079756    
Epoch: [156][  207/  207]    Overall Loss 0.011814    Objective Loss 0.011814    Top1 99.320498    Top5 100.000000    LR 0.000250    Time 0.079349    
--- validate (epoch=156)-----------
5136 samples (512 per mini-batch)
Epoch: [156][   10/   11]    Loss 0.586815    Top1 85.566406    Top5 99.726562    
Epoch: [156][   11/   11]    Loss 0.533971    Top1 85.611371    Top5 99.727414    
==> Top1: 85.611    Top5: 99.727    Loss: 0.534

==> Confusion:
[[272   2   3   3   2   0   2  16]
 [  2 255  25   0   0   3   1  14]
 [  1  15 270   0   0   6   0   8]
 [  0   1   0 739  57   9  12  19]
 [  1   0   0  34 783   5  23  33]
 [  5   6   7   6  22 792  14  42]
 [  1   0   0   5  28   9 773  21]
 [ 19   8  14  36  77  97  25 513]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [157][   10/  207]    Overall Loss 0.011088    Objective Loss 0.011088                                        LR 0.000250    Time 0.131416    
Epoch: [157][   20/  207]    Overall Loss 0.011233    Objective Loss 0.011233                                        LR 0.000250    Time 0.102662    
Epoch: [157][   30/  207]    Overall Loss 0.011280    Objective Loss 0.011280                                        LR 0.000250    Time 0.092339    
Epoch: [157][   40/  207]    Overall Loss 0.011425    Objective Loss 0.011425                                        LR 0.000250    Time 0.088497    
Epoch: [157][   50/  207]    Overall Loss 0.011597    Objective Loss 0.011597                                        LR 0.000250    Time 0.086223    
Epoch: [157][   60/  207]    Overall Loss 0.011733    Objective Loss 0.011733                                        LR 0.000250    Time 0.084347    
Epoch: [157][   70/  207]    Overall Loss 0.011735    Objective Loss 0.011735                                        LR 0.000250    Time 0.083361    
Epoch: [157][   80/  207]    Overall Loss 0.011672    Objective Loss 0.011672                                        LR 0.000250    Time 0.081883    
Epoch: [157][   90/  207]    Overall Loss 0.011657    Objective Loss 0.011657                                        LR 0.000250    Time 0.080633    
Epoch: [157][  100/  207]    Overall Loss 0.011554    Objective Loss 0.011554                                        LR 0.000250    Time 0.079814    
Epoch: [157][  110/  207]    Overall Loss 0.011515    Objective Loss 0.011515                                        LR 0.000250    Time 0.078956    
Epoch: [157][  120/  207]    Overall Loss 0.011662    Objective Loss 0.011662                                        LR 0.000250    Time 0.078992    
Epoch: [157][  130/  207]    Overall Loss 0.011614    Objective Loss 0.011614                                        LR 0.000250    Time 0.079123    
Epoch: [157][  140/  207]    Overall Loss 0.011614    Objective Loss 0.011614                                        LR 0.000250    Time 0.078615    
Epoch: [157][  150/  207]    Overall Loss 0.011658    Objective Loss 0.011658                                        LR 0.000250    Time 0.078274    
Epoch: [157][  160/  207]    Overall Loss 0.011539    Objective Loss 0.011539                                        LR 0.000250    Time 0.077774    
Epoch: [157][  170/  207]    Overall Loss 0.011582    Objective Loss 0.011582                                        LR 0.000250    Time 0.077483    
Epoch: [157][  180/  207]    Overall Loss 0.011699    Objective Loss 0.011699                                        LR 0.000250    Time 0.077265    
Epoch: [157][  190/  207]    Overall Loss 0.011661    Objective Loss 0.011661                                        LR 0.000250    Time 0.077043    
Epoch: [157][  200/  207]    Overall Loss 0.011663    Objective Loss 0.011663                                        LR 0.000250    Time 0.076762    
Epoch: [157][  207/  207]    Overall Loss 0.011644    Objective Loss 0.011644    Top1 99.433749    Top5 100.000000    LR 0.000250    Time 0.076463    
--- validate (epoch=157)-----------
5136 samples (512 per mini-batch)
Epoch: [157][   10/   11]    Loss 0.628184    Top1 85.546875    Top5 99.648438    
Epoch: [157][   11/   11]    Loss 0.597282    Top1 85.552960    Top5 99.649533    
==> Top1: 85.553    Top5: 99.650    Loss: 0.597

==> Confusion:
[[267   2   3   3   2   0   2  21]
 [  2 260  22   0   0   1   1  14]
 [  1  17 270   1   0   1   0  10]
 [  0   0   1 745  44   9   8  30]
 [  1   0   0  42 766   6  19  45]
 [  3   8  11   9  19 771  11  62]
 [  1   0   0   5  27   9 756  39]
 [ 13   9  15  39  65  70  19 559]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [158][   10/  207]    Overall Loss 0.010559    Objective Loss 0.010559                                        LR 0.000250    Time 0.132076    
Epoch: [158][   20/  207]    Overall Loss 0.009811    Objective Loss 0.009811                                        LR 0.000250    Time 0.103273    
Epoch: [158][   30/  207]    Overall Loss 0.009767    Objective Loss 0.009767                                        LR 0.000250    Time 0.093459    
Epoch: [158][   40/  207]    Overall Loss 0.010286    Objective Loss 0.010286                                        LR 0.000250    Time 0.087517    
Epoch: [158][   50/  207]    Overall Loss 0.010850    Objective Loss 0.010850                                        LR 0.000250    Time 0.084478    
Epoch: [158][   60/  207]    Overall Loss 0.011145    Objective Loss 0.011145                                        LR 0.000250    Time 0.082608    
Epoch: [158][   70/  207]    Overall Loss 0.011314    Objective Loss 0.011314                                        LR 0.000250    Time 0.080967    
Epoch: [158][   80/  207]    Overall Loss 0.011206    Objective Loss 0.011206                                        LR 0.000250    Time 0.079776    
Epoch: [158][   90/  207]    Overall Loss 0.011311    Objective Loss 0.011311                                        LR 0.000250    Time 0.079077    
Epoch: [158][  100/  207]    Overall Loss 0.011463    Objective Loss 0.011463                                        LR 0.000250    Time 0.078357    
Epoch: [158][  110/  207]    Overall Loss 0.011426    Objective Loss 0.011426                                        LR 0.000250    Time 0.078569    
Epoch: [158][  120/  207]    Overall Loss 0.011510    Objective Loss 0.011510                                        LR 0.000250    Time 0.078706    
Epoch: [158][  130/  207]    Overall Loss 0.011560    Objective Loss 0.011560                                        LR 0.000250    Time 0.078379    
Epoch: [158][  140/  207]    Overall Loss 0.011523    Objective Loss 0.011523                                        LR 0.000250    Time 0.077830    
Epoch: [158][  150/  207]    Overall Loss 0.011707    Objective Loss 0.011707                                        LR 0.000250    Time 0.077459    
Epoch: [158][  160/  207]    Overall Loss 0.011736    Objective Loss 0.011736                                        LR 0.000250    Time 0.077056    
Epoch: [158][  170/  207]    Overall Loss 0.011810    Objective Loss 0.011810                                        LR 0.000250    Time 0.076634    
Epoch: [158][  180/  207]    Overall Loss 0.011891    Objective Loss 0.011891                                        LR 0.000250    Time 0.076358    
Epoch: [158][  190/  207]    Overall Loss 0.011959    Objective Loss 0.011959                                        LR 0.000250    Time 0.076128    
Epoch: [158][  200/  207]    Overall Loss 0.011948    Objective Loss 0.011948                                        LR 0.000250    Time 0.075917    
Epoch: [158][  207/  207]    Overall Loss 0.011968    Objective Loss 0.011968    Top1 98.074745    Top5 100.000000    LR 0.000250    Time 0.075643    
--- validate (epoch=158)-----------
5136 samples (512 per mini-batch)
Epoch: [158][   10/   11]    Loss 0.568043    Top1 85.488281    Top5 99.765625    
Epoch: [158][   11/   11]    Loss 0.557176    Top1 85.494548    Top5 99.766355    
==> Top1: 85.495    Top5: 99.766    Loss: 0.557

==> Confusion:
[[270   2   6   4   1   0   1  16]
 [  1 267  22   0   0   1   1   8]
 [  1  18 273   0   0   4   0   4]
 [  0   2   1 754  42   7   9  22]
 [  2   0   0  49 756   7  22  43]
 [  4   9  13   9  18 783  12  46]
 [  1   0   0   6  25  12 762  31]
 [ 18  15  15  47  55  94  19 526]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [159][   10/  207]    Overall Loss 0.011496    Objective Loss 0.011496                                        LR 0.000250    Time 0.128452    
Epoch: [159][   20/  207]    Overall Loss 0.011767    Objective Loss 0.011767                                        LR 0.000250    Time 0.102811    
Epoch: [159][   30/  207]    Overall Loss 0.011891    Objective Loss 0.011891                                        LR 0.000250    Time 0.094987    
Epoch: [159][   40/  207]    Overall Loss 0.011747    Objective Loss 0.011747                                        LR 0.000250    Time 0.090900    
Epoch: [159][   50/  207]    Overall Loss 0.011527    Objective Loss 0.011527                                        LR 0.000250    Time 0.087807    
Epoch: [159][   60/  207]    Overall Loss 0.011891    Objective Loss 0.011891                                        LR 0.000250    Time 0.086318    
Epoch: [159][   70/  207]    Overall Loss 0.011725    Objective Loss 0.011725                                        LR 0.000250    Time 0.084360    
Epoch: [159][   80/  207]    Overall Loss 0.011861    Objective Loss 0.011861                                        LR 0.000250    Time 0.082473    
Epoch: [159][   90/  207]    Overall Loss 0.011935    Objective Loss 0.011935                                        LR 0.000250    Time 0.081293    
Epoch: [159][  100/  207]    Overall Loss 0.011930    Objective Loss 0.011930                                        LR 0.000250    Time 0.080135    
Epoch: [159][  110/  207]    Overall Loss 0.011909    Objective Loss 0.011909                                        LR 0.000250    Time 0.079378    
Epoch: [159][  120/  207]    Overall Loss 0.011859    Objective Loss 0.011859                                        LR 0.000250    Time 0.078717    
Epoch: [159][  130/  207]    Overall Loss 0.011796    Objective Loss 0.011796                                        LR 0.000250    Time 0.078240    
Epoch: [159][  140/  207]    Overall Loss 0.011922    Objective Loss 0.011922                                        LR 0.000250    Time 0.077909    
Epoch: [159][  150/  207]    Overall Loss 0.012013    Objective Loss 0.012013                                        LR 0.000250    Time 0.077551    
Epoch: [159][  160/  207]    Overall Loss 0.011918    Objective Loss 0.011918                                        LR 0.000250    Time 0.077226    
Epoch: [159][  170/  207]    Overall Loss 0.012015    Objective Loss 0.012015                                        LR 0.000250    Time 0.076835    
Epoch: [159][  180/  207]    Overall Loss 0.012258    Objective Loss 0.012258                                        LR 0.000250    Time 0.076762    
Epoch: [159][  190/  207]    Overall Loss 0.012332    Objective Loss 0.012332                                        LR 0.000250    Time 0.076667    
Epoch: [159][  200/  207]    Overall Loss 0.012359    Objective Loss 0.012359                                        LR 0.000250    Time 0.076394    
Epoch: [159][  207/  207]    Overall Loss 0.012417    Objective Loss 0.012417    Top1 98.414496    Top5 100.000000    LR 0.000250    Time 0.076118    
--- validate (epoch=159)-----------
5136 samples (512 per mini-batch)
Epoch: [159][   10/   11]    Loss 0.549455    Top1 85.429688    Top5 99.687500    
Epoch: [159][   11/   11]    Loss 0.678967    Top1 85.416667    Top5 99.688474    
==> Top1: 85.417    Top5: 99.688    Loss: 0.679

==> Confusion:
[[274   2   3   4   2   0   2  13]
 [  1 263  20   0   0   2   1  13]
 [  1  19 270   0   0   5   0   5]
 [  0   1   1 756  47   9  11  12]
 [  1   0   0  44 778   4  22  30]
 [  6   6  11  10  20 778  20  43]
 [  1   0   0   7  30   7 770  22]
 [ 17  11  15  57  73  86  32 498]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [160][   10/  207]    Overall Loss 0.010928    Objective Loss 0.010928                                        LR 0.000250    Time 0.129416    
Epoch: [160][   20/  207]    Overall Loss 0.010345    Objective Loss 0.010345                                        LR 0.000250    Time 0.100697    
Epoch: [160][   30/  207]    Overall Loss 0.010694    Objective Loss 0.010694                                        LR 0.000250    Time 0.090907    
Epoch: [160][   40/  207]    Overall Loss 0.011360    Objective Loss 0.011360                                        LR 0.000250    Time 0.086171    
Epoch: [160][   50/  207]    Overall Loss 0.011572    Objective Loss 0.011572                                        LR 0.000250    Time 0.083339    
Epoch: [160][   60/  207]    Overall Loss 0.011367    Objective Loss 0.011367                                        LR 0.000250    Time 0.081620    
Epoch: [160][   70/  207]    Overall Loss 0.011464    Objective Loss 0.011464                                        LR 0.000250    Time 0.080023    
Epoch: [160][   80/  207]    Overall Loss 0.011604    Objective Loss 0.011604                                        LR 0.000250    Time 0.079215    
Epoch: [160][   90/  207]    Overall Loss 0.011590    Objective Loss 0.011590                                        LR 0.000250    Time 0.078472    
Epoch: [160][  100/  207]    Overall Loss 0.011793    Objective Loss 0.011793                                        LR 0.000250    Time 0.077682    
Epoch: [160][  110/  207]    Overall Loss 0.012371    Objective Loss 0.012371                                        LR 0.000250    Time 0.077301    
Epoch: [160][  120/  207]    Overall Loss 0.013955    Objective Loss 0.013955                                        LR 0.000250    Time 0.076710    
Epoch: [160][  130/  207]    Overall Loss 0.016119    Objective Loss 0.016119                                        LR 0.000250    Time 0.076279    
Epoch: [160][  140/  207]    Overall Loss 0.019223    Objective Loss 0.019223                                        LR 0.000250    Time 0.075877    
Epoch: [160][  150/  207]    Overall Loss 0.023469    Objective Loss 0.023469                                        LR 0.000250    Time 0.075501    
Epoch: [160][  160/  207]    Overall Loss 0.027103    Objective Loss 0.027103                                        LR 0.000250    Time 0.075308    
Epoch: [160][  170/  207]    Overall Loss 0.029860    Objective Loss 0.029860                                        LR 0.000250    Time 0.075087    
Epoch: [160][  180/  207]    Overall Loss 0.030984    Objective Loss 0.030984                                        LR 0.000250    Time 0.074948    
Epoch: [160][  190/  207]    Overall Loss 0.031598    Objective Loss 0.031598                                        LR 0.000250    Time 0.074856    
Epoch: [160][  200/  207]    Overall Loss 0.032509    Objective Loss 0.032509                                        LR 0.000250    Time 0.074668    
Epoch: [160][  207/  207]    Overall Loss 0.032794    Objective Loss 0.032794    Top1 97.055493    Top5 100.000000    LR 0.000250    Time 0.074429    
--- validate (epoch=160)-----------
5136 samples (512 per mini-batch)
Epoch: [160][   10/   11]    Loss 0.581379    Top1 84.785156    Top5 99.746094    
Epoch: [160][   11/   11]    Loss 0.548275    Top1 84.813084    Top5 99.746885    
==> Top1: 84.813    Top5: 99.747    Loss: 0.548

==> Confusion:
[[269   1   5   3   0   4   0  18]
 [  1 267  19   1   0   1   1  10]
 [  4  26 262   1   0   3   0   4]
 [  0   3   0 758  41  10   9  16]
 [  2   0   0  58 754   8  28  29]
 [  4   8   7   7  18 796  15  39]
 [  1   0   0   8  23  11 766  28]
 [ 18  18  14  47  55 123  30 484]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [161][   10/  207]    Overall Loss 0.033165    Objective Loss 0.033165                                        LR 0.000250    Time 0.130544    
Epoch: [161][   20/  207]    Overall Loss 0.033051    Objective Loss 0.033051                                        LR 0.000250    Time 0.101054    
Epoch: [161][   30/  207]    Overall Loss 0.030156    Objective Loss 0.030156                                        LR 0.000250    Time 0.092139    
Epoch: [161][   40/  207]    Overall Loss 0.028928    Objective Loss 0.028928                                        LR 0.000250    Time 0.087463    
Epoch: [161][   50/  207]    Overall Loss 0.028650    Objective Loss 0.028650                                        LR 0.000250    Time 0.084399    
Epoch: [161][   60/  207]    Overall Loss 0.028436    Objective Loss 0.028436                                        LR 0.000250    Time 0.082189    
Epoch: [161][   70/  207]    Overall Loss 0.027860    Objective Loss 0.027860                                        LR 0.000250    Time 0.080817    
Epoch: [161][   80/  207]    Overall Loss 0.027300    Objective Loss 0.027300                                        LR 0.000250    Time 0.079548    
Epoch: [161][   90/  207]    Overall Loss 0.026974    Objective Loss 0.026974                                        LR 0.000250    Time 0.078650    
Epoch: [161][  100/  207]    Overall Loss 0.026602    Objective Loss 0.026602                                        LR 0.000250    Time 0.077770    
Epoch: [161][  110/  207]    Overall Loss 0.027280    Objective Loss 0.027280                                        LR 0.000250    Time 0.077174    
Epoch: [161][  120/  207]    Overall Loss 0.027639    Objective Loss 0.027639                                        LR 0.000250    Time 0.076930    
Epoch: [161][  130/  207]    Overall Loss 0.027458    Objective Loss 0.027458                                        LR 0.000250    Time 0.076568    
Epoch: [161][  140/  207]    Overall Loss 0.026798    Objective Loss 0.026798                                        LR 0.000250    Time 0.076296    
Epoch: [161][  150/  207]    Overall Loss 0.026294    Objective Loss 0.026294                                        LR 0.000250    Time 0.075993    
Epoch: [161][  160/  207]    Overall Loss 0.025993    Objective Loss 0.025993                                        LR 0.000250    Time 0.075845    
Epoch: [161][  170/  207]    Overall Loss 0.025934    Objective Loss 0.025934                                        LR 0.000250    Time 0.075534    
Epoch: [161][  180/  207]    Overall Loss 0.025584    Objective Loss 0.025584                                        LR 0.000250    Time 0.075322    
Epoch: [161][  190/  207]    Overall Loss 0.025178    Objective Loss 0.025178                                        LR 0.000250    Time 0.075130    
Epoch: [161][  200/  207]    Overall Loss 0.025083    Objective Loss 0.025083                                        LR 0.000250    Time 0.074876    
Epoch: [161][  207/  207]    Overall Loss 0.025277    Objective Loss 0.025277    Top1 98.640997    Top5 100.000000    LR 0.000250    Time 0.074635    
--- validate (epoch=161)-----------
5136 samples (512 per mini-batch)
Epoch: [161][   10/   11]    Loss 0.524030    Top1 84.472656    Top5 99.746094    
Epoch: [161][   11/   11]    Loss 0.660874    Top1 84.462617    Top5 99.727414    
==> Top1: 84.463    Top5: 99.727    Loss: 0.661

==> Confusion:
[[271   4   6   3   1   3   2  10]
 [  1 250  34   3   0   6   0   6]
 [  2  13 279   0   0   4   0   2]
 [  0   2   1 749  50  10   9  16]
 [  2   0   0  44 766   8  27  32]
 [  3   6   7   8  13 818  13  26]
 [  2   0   0   7  23  18 766  21]
 [ 25  18  18  45  68 153  23 439]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [162][   10/  207]    Overall Loss 0.020761    Objective Loss 0.020761                                        LR 0.000250    Time 0.130981    
Epoch: [162][   20/  207]    Overall Loss 0.017667    Objective Loss 0.017667                                        LR 0.000250    Time 0.100666    
Epoch: [162][   30/  207]    Overall Loss 0.017380    Objective Loss 0.017380                                        LR 0.000250    Time 0.090723    
Epoch: [162][   40/  207]    Overall Loss 0.016475    Objective Loss 0.016475                                        LR 0.000250    Time 0.086217    
Epoch: [162][   50/  207]    Overall Loss 0.015896    Objective Loss 0.015896                                        LR 0.000250    Time 0.083849    
Epoch: [162][   60/  207]    Overall Loss 0.015466    Objective Loss 0.015466                                        LR 0.000250    Time 0.082014    
Epoch: [162][   70/  207]    Overall Loss 0.015177    Objective Loss 0.015177                                        LR 0.000250    Time 0.080462    
Epoch: [162][   80/  207]    Overall Loss 0.014995    Objective Loss 0.014995                                        LR 0.000250    Time 0.079731    
Epoch: [162][   90/  207]    Overall Loss 0.015011    Objective Loss 0.015011                                        LR 0.000250    Time 0.079279    
Epoch: [162][  100/  207]    Overall Loss 0.014763    Objective Loss 0.014763                                        LR 0.000250    Time 0.078841    
Epoch: [162][  110/  207]    Overall Loss 0.014805    Objective Loss 0.014805                                        LR 0.000250    Time 0.078605    
Epoch: [162][  120/  207]    Overall Loss 0.014830    Objective Loss 0.014830                                        LR 0.000250    Time 0.078064    
Epoch: [162][  130/  207]    Overall Loss 0.014720    Objective Loss 0.014720                                        LR 0.000250    Time 0.077519    
Epoch: [162][  140/  207]    Overall Loss 0.014538    Objective Loss 0.014538                                        LR 0.000250    Time 0.077041    
Epoch: [162][  150/  207]    Overall Loss 0.014494    Objective Loss 0.014494                                        LR 0.000250    Time 0.076794    
Epoch: [162][  160/  207]    Overall Loss 0.014333    Objective Loss 0.014333                                        LR 0.000250    Time 0.076812    
Epoch: [162][  170/  207]    Overall Loss 0.014135    Objective Loss 0.014135                                        LR 0.000250    Time 0.076462    
Epoch: [162][  180/  207]    Overall Loss 0.014147    Objective Loss 0.014147                                        LR 0.000250    Time 0.076193    
Epoch: [162][  190/  207]    Overall Loss 0.014018    Objective Loss 0.014018                                        LR 0.000250    Time 0.075939    
Epoch: [162][  200/  207]    Overall Loss 0.013913    Objective Loss 0.013913                                        LR 0.000250    Time 0.075748    
Epoch: [162][  207/  207]    Overall Loss 0.013899    Objective Loss 0.013899    Top1 99.320498    Top5 100.000000    LR 0.000250    Time 0.075460    
--- validate (epoch=162)-----------
5136 samples (512 per mini-batch)
Epoch: [162][   10/   11]    Loss 0.571724    Top1 85.429688    Top5 99.687500    
Epoch: [162][   11/   11]    Loss 0.524929    Top1 85.475078    Top5 99.688474    
==> Top1: 85.475    Top5: 99.688    Loss: 0.525

==> Confusion:
[[273   1   4   3   1   0   2  16]
 [  1 260  24   1   0   3   1  10]
 [  2  16 269   0   0   5   0   8]
 [  0   1   1 751  49   9  11  15]
 [  2   0   0  45 767   4  26  35]
 [  4   5   8   9  21 779  13  55]
 [  1   0   0   8  23  11 770  24]
 [ 16  12  11  41  70  93  25 521]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [163][   10/  207]    Overall Loss 0.013070    Objective Loss 0.013070                                        LR 0.000250    Time 0.110764    
Epoch: [163][   20/  207]    Overall Loss 0.012585    Objective Loss 0.012585                                        LR 0.000250    Time 0.091127    
Epoch: [163][   30/  207]    Overall Loss 0.012198    Objective Loss 0.012198                                        LR 0.000250    Time 0.083911    
Epoch: [163][   40/  207]    Overall Loss 0.011900    Objective Loss 0.011900                                        LR 0.000250    Time 0.080734    
Epoch: [163][   50/  207]    Overall Loss 0.011872    Objective Loss 0.011872                                        LR 0.000250    Time 0.078759    
Epoch: [163][   60/  207]    Overall Loss 0.011438    Objective Loss 0.011438                                        LR 0.000250    Time 0.077730    
Epoch: [163][   70/  207]    Overall Loss 0.011658    Objective Loss 0.011658                                        LR 0.000250    Time 0.076691    
Epoch: [163][   80/  207]    Overall Loss 0.011611    Objective Loss 0.011611                                        LR 0.000250    Time 0.076452    
Epoch: [163][   90/  207]    Overall Loss 0.011567    Objective Loss 0.011567                                        LR 0.000250    Time 0.075977    
Epoch: [163][  100/  207]    Overall Loss 0.011574    Objective Loss 0.011574                                        LR 0.000250    Time 0.075505    
Epoch: [163][  110/  207]    Overall Loss 0.011555    Objective Loss 0.011555                                        LR 0.000250    Time 0.075065    
Epoch: [163][  120/  207]    Overall Loss 0.011587    Objective Loss 0.011587                                        LR 0.000250    Time 0.074720    
Epoch: [163][  130/  207]    Overall Loss 0.011644    Objective Loss 0.011644                                        LR 0.000250    Time 0.074522    
Epoch: [163][  140/  207]    Overall Loss 0.011690    Objective Loss 0.011690                                        LR 0.000250    Time 0.074330    
Epoch: [163][  150/  207]    Overall Loss 0.011811    Objective Loss 0.011811                                        LR 0.000250    Time 0.074079    
Epoch: [163][  160/  207]    Overall Loss 0.011778    Objective Loss 0.011778                                        LR 0.000250    Time 0.074055    
Epoch: [163][  170/  207]    Overall Loss 0.011866    Objective Loss 0.011866                                        LR 0.000250    Time 0.073897    
Epoch: [163][  180/  207]    Overall Loss 0.011851    Objective Loss 0.011851                                        LR 0.000250    Time 0.073721    
Epoch: [163][  190/  207]    Overall Loss 0.011915    Objective Loss 0.011915                                        LR 0.000250    Time 0.073690    
Epoch: [163][  200/  207]    Overall Loss 0.011870    Objective Loss 0.011870                                        LR 0.000250    Time 0.073708    
Epoch: [163][  207/  207]    Overall Loss 0.011873    Objective Loss 0.011873    Top1 98.980747    Top5 100.000000    LR 0.000250    Time 0.073490    
--- validate (epoch=163)-----------
5136 samples (512 per mini-batch)
Epoch: [163][   10/   11]    Loss 0.590401    Top1 85.605469    Top5 99.687500    
Epoch: [163][   11/   11]    Loss 0.538003    Top1 85.650312    Top5 99.688474    
==> Top1: 85.650    Top5: 99.688    Loss: 0.538

==> Confusion:
[[274   0   5   3   1   0   2  15]
 [  1 255  27   0   0   2   1  14]
 [  1  16 269   1   0   3   0  10]
 [  0   1   1 750  46   7   8  24]
 [  2   0   0  43 765   6  20  43]
 [  3   7  10   6  15 797   8  48]
 [  1   0   0   5  30  14 752  35]
 [ 18  11  12  43  58  91  19 537]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [164][   10/  207]    Overall Loss 0.011601    Objective Loss 0.011601                                        LR 0.000250    Time 0.127765    
Epoch: [164][   20/  207]    Overall Loss 0.011005    Objective Loss 0.011005                                        LR 0.000250    Time 0.100044    
Epoch: [164][   30/  207]    Overall Loss 0.010298    Objective Loss 0.010298                                        LR 0.000250    Time 0.090419    
Epoch: [164][   40/  207]    Overall Loss 0.010399    Objective Loss 0.010399                                        LR 0.000250    Time 0.085595    
Epoch: [164][   50/  207]    Overall Loss 0.010705    Objective Loss 0.010705                                        LR 0.000250    Time 0.082926    
Epoch: [164][   60/  207]    Overall Loss 0.010891    Objective Loss 0.010891                                        LR 0.000250    Time 0.080877    
Epoch: [164][   70/  207]    Overall Loss 0.011167    Objective Loss 0.011167                                        LR 0.000250    Time 0.079464    
Epoch: [164][   80/  207]    Overall Loss 0.011159    Objective Loss 0.011159                                        LR 0.000250    Time 0.078415    
Epoch: [164][   90/  207]    Overall Loss 0.011238    Objective Loss 0.011238                                        LR 0.000250    Time 0.077756    
Epoch: [164][  100/  207]    Overall Loss 0.011253    Objective Loss 0.011253                                        LR 0.000250    Time 0.077300    
Epoch: [164][  110/  207]    Overall Loss 0.011189    Objective Loss 0.011189                                        LR 0.000250    Time 0.076728    
Epoch: [164][  120/  207]    Overall Loss 0.011153    Objective Loss 0.011153                                        LR 0.000250    Time 0.076159    
Epoch: [164][  130/  207]    Overall Loss 0.011356    Objective Loss 0.011356                                        LR 0.000250    Time 0.075744    
Epoch: [164][  140/  207]    Overall Loss 0.011339    Objective Loss 0.011339                                        LR 0.000250    Time 0.075424    
Epoch: [164][  150/  207]    Overall Loss 0.011235    Objective Loss 0.011235                                        LR 0.000250    Time 0.075228    
Epoch: [164][  160/  207]    Overall Loss 0.011310    Objective Loss 0.011310                                        LR 0.000250    Time 0.074984    
Epoch: [164][  170/  207]    Overall Loss 0.011240    Objective Loss 0.011240                                        LR 0.000250    Time 0.074868    
Epoch: [164][  180/  207]    Overall Loss 0.011188    Objective Loss 0.011188                                        LR 0.000250    Time 0.074793    
Epoch: [164][  190/  207]    Overall Loss 0.011267    Objective Loss 0.011267                                        LR 0.000250    Time 0.074629    
Epoch: [164][  200/  207]    Overall Loss 0.011251    Objective Loss 0.011251                                        LR 0.000250    Time 0.074557    
Epoch: [164][  207/  207]    Overall Loss 0.011270    Objective Loss 0.011270    Top1 98.640997    Top5 99.886750    LR 0.000250    Time 0.074319    
--- validate (epoch=164)-----------
5136 samples (512 per mini-batch)
Epoch: [164][   10/   11]    Loss 0.594580    Top1 85.371094    Top5 99.589844    
Epoch: [164][   11/   11]    Loss 0.684143    Top1 85.358255    Top5 99.591121    
==> Top1: 85.358    Top5: 99.591    Loss: 0.684

==> Confusion:
[[274   1   6   3   1   0   1  14]
 [  2 264  21   0   0   2   1  10]
 [  2  18 267   0   0   4   0   9]
 [  0   2   1 736  54   6   9  29]
 [  2   0   0  37 774   3  18  45]
 [  3   9  11   8  19 762  15  67]
 [  1   0   0   4  31   7 756  38]
 [ 19  16  11  33  65  78  16 551]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [165][   10/  207]    Overall Loss 0.010144    Objective Loss 0.010144                                        LR 0.000250    Time 0.131880    
Epoch: [165][   20/  207]    Overall Loss 0.010051    Objective Loss 0.010051                                        LR 0.000250    Time 0.101265    
Epoch: [165][   30/  207]    Overall Loss 0.010315    Objective Loss 0.010315                                        LR 0.000250    Time 0.092110    
Epoch: [165][   40/  207]    Overall Loss 0.010711    Objective Loss 0.010711                                        LR 0.000250    Time 0.087112    
Epoch: [165][   50/  207]    Overall Loss 0.010640    Objective Loss 0.010640                                        LR 0.000250    Time 0.083777    
Epoch: [165][   60/  207]    Overall Loss 0.010431    Objective Loss 0.010431                                        LR 0.000250    Time 0.081635    
Epoch: [165][   70/  207]    Overall Loss 0.010311    Objective Loss 0.010311                                        LR 0.000250    Time 0.080158    
Epoch: [165][   80/  207]    Overall Loss 0.010421    Objective Loss 0.010421                                        LR 0.000250    Time 0.079144    
Epoch: [165][   90/  207]    Overall Loss 0.010573    Objective Loss 0.010573                                        LR 0.000250    Time 0.078326    
Epoch: [165][  100/  207]    Overall Loss 0.010506    Objective Loss 0.010506                                        LR 0.000250    Time 0.077560    
Epoch: [165][  110/  207]    Overall Loss 0.010652    Objective Loss 0.010652                                        LR 0.000250    Time 0.076933    
Epoch: [165][  120/  207]    Overall Loss 0.010762    Objective Loss 0.010762                                        LR 0.000250    Time 0.076686    
Epoch: [165][  130/  207]    Overall Loss 0.010877    Objective Loss 0.010877                                        LR 0.000250    Time 0.076346    
Epoch: [165][  140/  207]    Overall Loss 0.011006    Objective Loss 0.011006                                        LR 0.000250    Time 0.076083    
Epoch: [165][  150/  207]    Overall Loss 0.011031    Objective Loss 0.011031                                        LR 0.000250    Time 0.075949    
Epoch: [165][  160/  207]    Overall Loss 0.010971    Objective Loss 0.010971                                        LR 0.000250    Time 0.075594    
Epoch: [165][  170/  207]    Overall Loss 0.010955    Objective Loss 0.010955                                        LR 0.000250    Time 0.075434    
Epoch: [165][  180/  207]    Overall Loss 0.010924    Objective Loss 0.010924                                        LR 0.000250    Time 0.075253    
Epoch: [165][  190/  207]    Overall Loss 0.010925    Objective Loss 0.010925                                        LR 0.000250    Time 0.075078    
Epoch: [165][  200/  207]    Overall Loss 0.010858    Objective Loss 0.010858                                        LR 0.000250    Time 0.074886    
Epoch: [165][  207/  207]    Overall Loss 0.010891    Objective Loss 0.010891    Top1 98.980747    Top5 100.000000    LR 0.000250    Time 0.074644    
--- validate (epoch=165)-----------
5136 samples (512 per mini-batch)
Epoch: [165][   10/   11]    Loss 0.600432    Top1 85.488281    Top5 99.726562    
Epoch: [165][   11/   11]    Loss 0.547941    Top1 85.514019    Top5 99.727414    
==> Top1: 85.514    Top5: 99.727    Loss: 0.548

==> Confusion:
[[273   1   5   3   1   0   2  15]
 [  1 257  25   0   0   3   1  13]
 [  1  18 268   1   0   4   0   8]
 [  0   2   1 761  43   5  11  14]
 [  2   0   0  51 764   5  23  34]
 [  7   4   9  10  18 777  15  54]
 [  2   0   0   7  27   8 765  28]
 [ 15  12  12  48  65  85  25 527]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [166][   10/  207]    Overall Loss 0.011572    Objective Loss 0.011572                                        LR 0.000250    Time 0.128826    
Epoch: [166][   20/  207]    Overall Loss 0.011191    Objective Loss 0.011191                                        LR 0.000250    Time 0.099472    
Epoch: [166][   30/  207]    Overall Loss 0.011060    Objective Loss 0.011060                                        LR 0.000250    Time 0.093576    
Epoch: [166][   40/  207]    Overall Loss 0.010783    Objective Loss 0.010783                                        LR 0.000250    Time 0.087896    
Epoch: [166][   50/  207]    Overall Loss 0.010382    Objective Loss 0.010382                                        LR 0.000250    Time 0.084669    
Epoch: [166][   60/  207]    Overall Loss 0.010319    Objective Loss 0.010319                                        LR 0.000250    Time 0.082800    
Epoch: [166][   70/  207]    Overall Loss 0.010222    Objective Loss 0.010222                                        LR 0.000250    Time 0.081287    
Epoch: [166][   80/  207]    Overall Loss 0.010222    Objective Loss 0.010222                                        LR 0.000250    Time 0.080061    
Epoch: [166][   90/  207]    Overall Loss 0.010219    Objective Loss 0.010219                                        LR 0.000250    Time 0.079114    
Epoch: [166][  100/  207]    Overall Loss 0.010345    Objective Loss 0.010345                                        LR 0.000250    Time 0.078415    
Epoch: [166][  110/  207]    Overall Loss 0.010281    Objective Loss 0.010281                                        LR 0.000250    Time 0.077792    
Epoch: [166][  120/  207]    Overall Loss 0.010348    Objective Loss 0.010348                                        LR 0.000250    Time 0.077493    
Epoch: [166][  130/  207]    Overall Loss 0.010462    Objective Loss 0.010462                                        LR 0.000250    Time 0.076955    
Epoch: [166][  140/  207]    Overall Loss 0.010419    Objective Loss 0.010419                                        LR 0.000250    Time 0.076592    
Epoch: [166][  150/  207]    Overall Loss 0.010436    Objective Loss 0.010436                                        LR 0.000250    Time 0.076173    
Epoch: [166][  160/  207]    Overall Loss 0.010421    Objective Loss 0.010421                                        LR 0.000250    Time 0.076088    
Epoch: [166][  170/  207]    Overall Loss 0.010559    Objective Loss 0.010559                                        LR 0.000250    Time 0.075781    
Epoch: [166][  180/  207]    Overall Loss 0.010615    Objective Loss 0.010615                                        LR 0.000250    Time 0.075602    
Epoch: [166][  190/  207]    Overall Loss 0.010685    Objective Loss 0.010685                                        LR 0.000250    Time 0.075350    
Epoch: [166][  200/  207]    Overall Loss 0.010865    Objective Loss 0.010865                                        LR 0.000250    Time 0.075190    
Epoch: [166][  207/  207]    Overall Loss 0.010965    Objective Loss 0.010965    Top1 98.414496    Top5 100.000000    LR 0.000250    Time 0.074940    
--- validate (epoch=166)-----------
5136 samples (512 per mini-batch)
Epoch: [166][   10/   11]    Loss 0.584166    Top1 85.585938    Top5 99.726562    
Epoch: [166][   11/   11]    Loss 0.564064    Top1 85.552960    Top5 99.727414    
==> Top1: 85.553    Top5: 99.727    Loss: 0.564

==> Confusion:
[[275   1   3   4   1   0   2  14]
 [  1 261  20   2   0   3   1  12]
 [  2  18 264   0   0   6   0  10]
 [  0   0   1 754  50   9   8  15]
 [  1   0   0  39 771   6  19  43]
 [  4   7   5   9  21 789  12  47]
 [  2   1   0   7  26   9 761  31]
 [ 16  13  12  37  66 103  23 519]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [167][   10/  207]    Overall Loss 0.009980    Objective Loss 0.009980                                        LR 0.000250    Time 0.112596    
Epoch: [167][   20/  207]    Overall Loss 0.010027    Objective Loss 0.010027                                        LR 0.000250    Time 0.091516    
Epoch: [167][   30/  207]    Overall Loss 0.010308    Objective Loss 0.010308                                        LR 0.000250    Time 0.084426    
Epoch: [167][   40/  207]    Overall Loss 0.010751    Objective Loss 0.010751                                        LR 0.000250    Time 0.081021    
Epoch: [167][   50/  207]    Overall Loss 0.010811    Objective Loss 0.010811                                        LR 0.000250    Time 0.078860    
Epoch: [167][   60/  207]    Overall Loss 0.010647    Objective Loss 0.010647                                        LR 0.000250    Time 0.077385    
Epoch: [167][   70/  207]    Overall Loss 0.010854    Objective Loss 0.010854                                        LR 0.000250    Time 0.076312    
Epoch: [167][   80/  207]    Overall Loss 0.010662    Objective Loss 0.010662                                        LR 0.000250    Time 0.075874    
Epoch: [167][   90/  207]    Overall Loss 0.010826    Objective Loss 0.010826                                        LR 0.000250    Time 0.075128    
Epoch: [167][  100/  207]    Overall Loss 0.010923    Objective Loss 0.010923                                        LR 0.000250    Time 0.074862    
Epoch: [167][  110/  207]    Overall Loss 0.010978    Objective Loss 0.010978                                        LR 0.000250    Time 0.074483    
Epoch: [167][  120/  207]    Overall Loss 0.011106    Objective Loss 0.011106                                        LR 0.000250    Time 0.074146    
Epoch: [167][  130/  207]    Overall Loss 0.011296    Objective Loss 0.011296                                        LR 0.000250    Time 0.074019    
Epoch: [167][  140/  207]    Overall Loss 0.011171    Objective Loss 0.011171                                        LR 0.000250    Time 0.073792    
Epoch: [167][  150/  207]    Overall Loss 0.011184    Objective Loss 0.011184                                        LR 0.000250    Time 0.073603    
Epoch: [167][  160/  207]    Overall Loss 0.011189    Objective Loss 0.011189                                        LR 0.000250    Time 0.073527    
Epoch: [167][  170/  207]    Overall Loss 0.011266    Objective Loss 0.011266                                        LR 0.000250    Time 0.073358    
Epoch: [167][  180/  207]    Overall Loss 0.011188    Objective Loss 0.011188                                        LR 0.000250    Time 0.073224    
Epoch: [167][  190/  207]    Overall Loss 0.011161    Objective Loss 0.011161                                        LR 0.000250    Time 0.073140    
Epoch: [167][  200/  207]    Overall Loss 0.011164    Objective Loss 0.011164                                        LR 0.000250    Time 0.073124    
Epoch: [167][  207/  207]    Overall Loss 0.011260    Objective Loss 0.011260    Top1 98.527746    Top5 99.886750    LR 0.000250    Time 0.072927    
--- validate (epoch=167)-----------
5136 samples (512 per mini-batch)
Epoch: [167][   10/   11]    Loss 0.568429    Top1 85.585938    Top5 99.746094    
Epoch: [167][   11/   11]    Loss 0.583439    Top1 85.611371    Top5 99.746885    
==> Top1: 85.611    Top5: 99.747    Loss: 0.583

==> Confusion:
[[276   2   3   3   1   0   2  13]
 [  1 263  21   0   0   4   1  10]
 [  3  21 262   0   0   5   0   9]
 [  0   0   0 746  54   8  11  18]
 [  1   0   0  33 783   6  22  34]
 [  9   5   8   8  18 793  12  41]
 [  1   0   0   5  32  11 764  24]
 [ 25  12  14  42  71  89  26 510]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [168][   10/  207]    Overall Loss 0.010583    Objective Loss 0.010583                                        LR 0.000250    Time 0.132463    
Epoch: [168][   20/  207]    Overall Loss 0.011757    Objective Loss 0.011757                                        LR 0.000250    Time 0.101576    
Epoch: [168][   30/  207]    Overall Loss 0.011034    Objective Loss 0.011034                                        LR 0.000250    Time 0.091506    
Epoch: [168][   40/  207]    Overall Loss 0.010813    Objective Loss 0.010813                                        LR 0.000250    Time 0.086467    
Epoch: [168][   50/  207]    Overall Loss 0.010356    Objective Loss 0.010356                                        LR 0.000250    Time 0.083455    
Epoch: [168][   60/  207]    Overall Loss 0.010357    Objective Loss 0.010357                                        LR 0.000250    Time 0.081283    
Epoch: [168][   70/  207]    Overall Loss 0.010413    Objective Loss 0.010413                                        LR 0.000250    Time 0.079777    
Epoch: [168][   80/  207]    Overall Loss 0.010690    Objective Loss 0.010690                                        LR 0.000250    Time 0.078806    
Epoch: [168][   90/  207]    Overall Loss 0.010693    Objective Loss 0.010693                                        LR 0.000250    Time 0.077831    
Epoch: [168][  100/  207]    Overall Loss 0.011007    Objective Loss 0.011007                                        LR 0.000250    Time 0.077357    
Epoch: [168][  110/  207]    Overall Loss 0.011441    Objective Loss 0.011441                                        LR 0.000250    Time 0.076767    
Epoch: [168][  120/  207]    Overall Loss 0.011466    Objective Loss 0.011466                                        LR 0.000250    Time 0.076289    
Epoch: [168][  130/  207]    Overall Loss 0.011492    Objective Loss 0.011492                                        LR 0.000250    Time 0.075893    
Epoch: [168][  140/  207]    Overall Loss 0.011524    Objective Loss 0.011524                                        LR 0.000250    Time 0.075549    
Epoch: [168][  150/  207]    Overall Loss 0.011571    Objective Loss 0.011571                                        LR 0.000250    Time 0.075267    
Epoch: [168][  160/  207]    Overall Loss 0.011690    Objective Loss 0.011690                                        LR 0.000250    Time 0.074991    
Epoch: [168][  170/  207]    Overall Loss 0.011654    Objective Loss 0.011654                                        LR 0.000250    Time 0.074742    
Epoch: [168][  180/  207]    Overall Loss 0.011757    Objective Loss 0.011757                                        LR 0.000250    Time 0.074573    
Epoch: [168][  190/  207]    Overall Loss 0.011852    Objective Loss 0.011852                                        LR 0.000250    Time 0.074452    
Epoch: [168][  200/  207]    Overall Loss 0.011810    Objective Loss 0.011810                                        LR 0.000250    Time 0.074280    
Epoch: [168][  207/  207]    Overall Loss 0.011963    Objective Loss 0.011963    Top1 99.207248    Top5 100.000000    LR 0.000250    Time 0.074064    
--- validate (epoch=168)-----------
5136 samples (512 per mini-batch)
Epoch: [168][   10/   11]    Loss 0.533313    Top1 84.707031    Top5 99.648438    
Epoch: [168][   11/   11]    Loss 0.600207    Top1 84.715732    Top5 99.649533    
==> Top1: 84.716    Top5: 99.650    Loss: 0.600

==> Confusion:
[[275   3   5   4   1   2   2   8]
 [  1 261  28   0   0   4   2   4]
 [  2  18 271   1   0   5   0   3]
 [  0   3   1 758  38  11  15  11]
 [  1   0   1  47 761   9  33  27]
 [  3   6   6  10  12 822  19  16]
 [  1   1   0   5  20  12 784  14]
 [ 19  20  19  52  62 154  44 419]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [169][   10/  207]    Overall Loss 0.019804    Objective Loss 0.019804                                        LR 0.000250    Time 0.128248    
Epoch: [169][   20/  207]    Overall Loss 0.018148    Objective Loss 0.018148                                        LR 0.000250    Time 0.099965    
Epoch: [169][   30/  207]    Overall Loss 0.016985    Objective Loss 0.016985                                        LR 0.000250    Time 0.090588    
Epoch: [169][   40/  207]    Overall Loss 0.016674    Objective Loss 0.016674                                        LR 0.000250    Time 0.085745    
Epoch: [169][   50/  207]    Overall Loss 0.016346    Objective Loss 0.016346                                        LR 0.000250    Time 0.082966    
Epoch: [169][   60/  207]    Overall Loss 0.015878    Objective Loss 0.015878                                        LR 0.000250    Time 0.081056    
Epoch: [169][   70/  207]    Overall Loss 0.015265    Objective Loss 0.015265                                        LR 0.000250    Time 0.079584    
Epoch: [169][   80/  207]    Overall Loss 0.014945    Objective Loss 0.014945                                        LR 0.000250    Time 0.078399    
Epoch: [169][   90/  207]    Overall Loss 0.015333    Objective Loss 0.015333                                        LR 0.000250    Time 0.077788    
Epoch: [169][  100/  207]    Overall Loss 0.016296    Objective Loss 0.016296                                        LR 0.000250    Time 0.077133    
Epoch: [169][  110/  207]    Overall Loss 0.017422    Objective Loss 0.017422                                        LR 0.000250    Time 0.076527    
Epoch: [169][  120/  207]    Overall Loss 0.018126    Objective Loss 0.018126                                        LR 0.000250    Time 0.076072    
Epoch: [169][  130/  207]    Overall Loss 0.019040    Objective Loss 0.019040                                        LR 0.000250    Time 0.075658    
Epoch: [169][  140/  207]    Overall Loss 0.020202    Objective Loss 0.020202                                        LR 0.000250    Time 0.075366    
Epoch: [169][  150/  207]    Overall Loss 0.022084    Objective Loss 0.022084                                        LR 0.000250    Time 0.075130    
Epoch: [169][  160/  207]    Overall Loss 0.024190    Objective Loss 0.024190                                        LR 0.000250    Time 0.074851    
Epoch: [169][  170/  207]    Overall Loss 0.025563    Objective Loss 0.025563                                        LR 0.000250    Time 0.074650    
Epoch: [169][  180/  207]    Overall Loss 0.026645    Objective Loss 0.026645                                        LR 0.000250    Time 0.074523    
Epoch: [169][  190/  207]    Overall Loss 0.027413    Objective Loss 0.027413                                        LR 0.000250    Time 0.074427    
Epoch: [169][  200/  207]    Overall Loss 0.028701    Objective Loss 0.028701                                        LR 0.000250    Time 0.074337    
Epoch: [169][  207/  207]    Overall Loss 0.029324    Objective Loss 0.029324    Top1 97.395243    Top5 100.000000    LR 0.000250    Time 0.074092    
--- validate (epoch=169)-----------
5136 samples (512 per mini-batch)
Epoch: [169][   10/   11]    Loss 0.658080    Top1 83.671875    Top5 99.609375    
Epoch: [169][   11/   11]    Loss 0.603158    Top1 83.683801    Top5 99.610592    
==> Top1: 83.684    Top5: 99.611    Loss: 0.603

==> Confusion:
[[285   3   1   3   1   0   0   7]
 [  8 238  47   0   0   2   0   5]
 [  6  14 274   1   0   1   0   4]
 [  0   2   2 752  45  12   9  15]
 [  3   0   0  53 764  10  20  29]
 [ 23   5  19  12  12 766   9  48]
 [  5   1   0   8  32  15 749  27]
 [ 39  14  22  54  68  96  26 470]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [170][   10/  207]    Overall Loss 0.042827    Objective Loss 0.042827                                        LR 0.000250    Time 0.128701    
Epoch: [170][   20/  207]    Overall Loss 0.050975    Objective Loss 0.050975                                        LR 0.000250    Time 0.100629    
Epoch: [170][   30/  207]    Overall Loss 0.049174    Objective Loss 0.049174                                        LR 0.000250    Time 0.090976    
Epoch: [170][   40/  207]    Overall Loss 0.048982    Objective Loss 0.048982                                        LR 0.000250    Time 0.085852    
Epoch: [170][   50/  207]    Overall Loss 0.049255    Objective Loss 0.049255                                        LR 0.000250    Time 0.083045    
Epoch: [170][   60/  207]    Overall Loss 0.046403    Objective Loss 0.046403                                        LR 0.000250    Time 0.081264    
Epoch: [170][   70/  207]    Overall Loss 0.044399    Objective Loss 0.044399                                        LR 0.000250    Time 0.080175    
Epoch: [170][   80/  207]    Overall Loss 0.043525    Objective Loss 0.043525                                        LR 0.000250    Time 0.078969    
Epoch: [170][   90/  207]    Overall Loss 0.041586    Objective Loss 0.041586                                        LR 0.000250    Time 0.078040    
Epoch: [170][  100/  207]    Overall Loss 0.039754    Objective Loss 0.039754                                        LR 0.000250    Time 0.077261    
Epoch: [170][  110/  207]    Overall Loss 0.038180    Objective Loss 0.038180                                        LR 0.000250    Time 0.076829    
Epoch: [170][  120/  207]    Overall Loss 0.036805    Objective Loss 0.036805                                        LR 0.000250    Time 0.076526    
Epoch: [170][  130/  207]    Overall Loss 0.035735    Objective Loss 0.035735                                        LR 0.000250    Time 0.076065    
Epoch: [170][  140/  207]    Overall Loss 0.034609    Objective Loss 0.034609                                        LR 0.000250    Time 0.075699    
Epoch: [170][  150/  207]    Overall Loss 0.033910    Objective Loss 0.033910                                        LR 0.000250    Time 0.075452    
Epoch: [170][  160/  207]    Overall Loss 0.033069    Objective Loss 0.033069                                        LR 0.000250    Time 0.075126    
Epoch: [170][  170/  207]    Overall Loss 0.032306    Objective Loss 0.032306                                        LR 0.000250    Time 0.074791    
Epoch: [170][  180/  207]    Overall Loss 0.031599    Objective Loss 0.031599                                        LR 0.000250    Time 0.074616    
Epoch: [170][  190/  207]    Overall Loss 0.030806    Objective Loss 0.030806                                        LR 0.000250    Time 0.074477    
Epoch: [170][  200/  207]    Overall Loss 0.030051    Objective Loss 0.030051                                        LR 0.000250    Time 0.074233    
Epoch: [170][  207/  207]    Overall Loss 0.029612    Objective Loss 0.029612    Top1 98.414496    Top5 100.000000    LR 0.000250    Time 0.074013    
--- validate (epoch=170)-----------
5136 samples (512 per mini-batch)
Epoch: [170][   10/   11]    Loss 0.599749    Top1 85.390625    Top5 99.687500    
Epoch: [170][   11/   11]    Loss 0.586262    Top1 85.358255    Top5 99.688474    
==> Top1: 85.358    Top5: 99.688    Loss: 0.586

==> Confusion:
[[270   2   4   3   2   0   1  18]
 [  1 265  20   1   0   1   1  11]
 [  1  17 269   1   0   2   0  10]
 [  0   1   1 735  55   8  13  24]
 [  2   0   0  32 782   4  25  34]
 [  8   8  10   4  20 774  24  46]
 [  1   1   0   3  23  10 774  25]
 [ 15  14  13  41  70  91  30 515]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [171][   10/  207]    Overall Loss 0.014455    Objective Loss 0.014455                                        LR 0.000250    Time 0.132970    
Epoch: [171][   20/  207]    Overall Loss 0.014013    Objective Loss 0.014013                                        LR 0.000250    Time 0.102397    
Epoch: [171][   30/  207]    Overall Loss 0.013648    Objective Loss 0.013648                                        LR 0.000250    Time 0.093220    
Epoch: [171][   40/  207]    Overall Loss 0.013451    Objective Loss 0.013451                                        LR 0.000250    Time 0.088382    
Epoch: [171][   50/  207]    Overall Loss 0.013562    Objective Loss 0.013562                                        LR 0.000250    Time 0.085633    
Epoch: [171][   60/  207]    Overall Loss 0.013501    Objective Loss 0.013501                                        LR 0.000250    Time 0.083927    
Epoch: [171][   70/  207]    Overall Loss 0.013550    Objective Loss 0.013550                                        LR 0.000250    Time 0.082578    
Epoch: [171][   80/  207]    Overall Loss 0.013585    Objective Loss 0.013585                                        LR 0.000250    Time 0.081653    
Epoch: [171][   90/  207]    Overall Loss 0.013473    Objective Loss 0.013473                                        LR 0.000250    Time 0.081630    
Epoch: [171][  100/  207]    Overall Loss 0.013279    Objective Loss 0.013279                                        LR 0.000250    Time 0.083475    
Epoch: [171][  110/  207]    Overall Loss 0.013046    Objective Loss 0.013046                                        LR 0.000250    Time 0.082848    
Epoch: [171][  120/  207]    Overall Loss 0.013165    Objective Loss 0.013165                                        LR 0.000250    Time 0.082090    
Epoch: [171][  130/  207]    Overall Loss 0.013219    Objective Loss 0.013219                                        LR 0.000250    Time 0.081633    
Epoch: [171][  140/  207]    Overall Loss 0.013255    Objective Loss 0.013255                                        LR 0.000250    Time 0.081873    
Epoch: [171][  150/  207]    Overall Loss 0.013188    Objective Loss 0.013188                                        LR 0.000250    Time 0.081588    
Epoch: [171][  160/  207]    Overall Loss 0.013176    Objective Loss 0.013176                                        LR 0.000250    Time 0.081373    
Epoch: [171][  170/  207]    Overall Loss 0.013200    Objective Loss 0.013200                                        LR 0.000250    Time 0.080839    
Epoch: [171][  180/  207]    Overall Loss 0.013200    Objective Loss 0.013200                                        LR 0.000250    Time 0.080402    
Epoch: [171][  190/  207]    Overall Loss 0.013114    Objective Loss 0.013114                                        LR 0.000250    Time 0.079930    
Epoch: [171][  200/  207]    Overall Loss 0.013109    Objective Loss 0.013109                                        LR 0.000250    Time 0.079733    
Epoch: [171][  207/  207]    Overall Loss 0.013042    Objective Loss 0.013042    Top1 98.527746    Top5 99.886750    LR 0.000250    Time 0.079444    
--- validate (epoch=171)-----------
5136 samples (512 per mini-batch)
Epoch: [171][   10/   11]    Loss 0.588862    Top1 85.703125    Top5 99.765625    
Epoch: [171][   11/   11]    Loss 0.581181    Top1 85.708723    Top5 99.766355    
==> Top1: 85.709    Top5: 99.766    Loss: 0.581

==> Confusion:
[[276   1   3   3   2   2   2  11]
 [  1 254  28   1   0   3   1  12]
 [  1  15 273   1   0   2   0   8]
 [  0   2   1 746  48   8   7  25]
 [  2   0   0  36 777   6  22  36]
 [  7   6   7   8  17 790  15  44]
 [  1   0   0   5  26  10 766  29]
 [ 19  10  13  37  64  99  27 520]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [172][   10/  207]    Overall Loss 0.011683    Objective Loss 0.011683                                        LR 0.000250    Time 0.143584    
Epoch: [172][   20/  207]    Overall Loss 0.011020    Objective Loss 0.011020                                        LR 0.000250    Time 0.110840    
Epoch: [172][   30/  207]    Overall Loss 0.010786    Objective Loss 0.010786                                        LR 0.000250    Time 0.099451    
Epoch: [172][   40/  207]    Overall Loss 0.010974    Objective Loss 0.010974                                        LR 0.000250    Time 0.094654    
Epoch: [172][   50/  207]    Overall Loss 0.011141    Objective Loss 0.011141                                        LR 0.000250    Time 0.092850    
Epoch: [172][   60/  207]    Overall Loss 0.011386    Objective Loss 0.011386                                        LR 0.000250    Time 0.090971    
Epoch: [172][   70/  207]    Overall Loss 0.011770    Objective Loss 0.011770                                        LR 0.000250    Time 0.089331    
Epoch: [172][   80/  207]    Overall Loss 0.011757    Objective Loss 0.011757                                        LR 0.000250    Time 0.087666    
Epoch: [172][   90/  207]    Overall Loss 0.011782    Objective Loss 0.011782                                        LR 0.000250    Time 0.086798    
Epoch: [172][  100/  207]    Overall Loss 0.011775    Objective Loss 0.011775                                        LR 0.000250    Time 0.086395    
Epoch: [172][  110/  207]    Overall Loss 0.011820    Objective Loss 0.011820                                        LR 0.000250    Time 0.086021    
Epoch: [172][  120/  207]    Overall Loss 0.011903    Objective Loss 0.011903                                        LR 0.000250    Time 0.085504    
Epoch: [172][  130/  207]    Overall Loss 0.011878    Objective Loss 0.011878                                        LR 0.000250    Time 0.084574    
Epoch: [172][  140/  207]    Overall Loss 0.011807    Objective Loss 0.011807                                        LR 0.000250    Time 0.083645    
Epoch: [172][  150/  207]    Overall Loss 0.011901    Objective Loss 0.011901                                        LR 0.000250    Time 0.082873    
Epoch: [172][  160/  207]    Overall Loss 0.011864    Objective Loss 0.011864                                        LR 0.000250    Time 0.082157    
Epoch: [172][  170/  207]    Overall Loss 0.011800    Objective Loss 0.011800                                        LR 0.000250    Time 0.081554    
Epoch: [172][  180/  207]    Overall Loss 0.011747    Objective Loss 0.011747                                        LR 0.000250    Time 0.081058    
Epoch: [172][  190/  207]    Overall Loss 0.011676    Objective Loss 0.011676                                        LR 0.000250    Time 0.080571    
Epoch: [172][  200/  207]    Overall Loss 0.011604    Objective Loss 0.011604                                        LR 0.000250    Time 0.080199    
Epoch: [172][  207/  207]    Overall Loss 0.011618    Objective Loss 0.011618    Top1 98.980747    Top5 100.000000    LR 0.000250    Time 0.079778    
--- validate (epoch=172)-----------
5136 samples (512 per mini-batch)
Epoch: [172][   10/   11]    Loss 0.644053    Top1 85.488281    Top5 99.648438    
Epoch: [172][   11/   11]    Loss 0.667665    Top1 85.475078    Top5 99.630062    
==> Top1: 85.475    Top5: 99.630    Loss: 0.668

==> Confusion:
[[272   1   3   3   2   0   2  17]
 [  2 256  24   0   0   3   1  14]
 [  1  17 267   1   0   2   0  12]
 [  0   2   1 738  47   8   6  35]
 [  2   0   0  38 768   5  22  44]
 [  5   6   9   8  16 764  10  76]
 [  1   1   0   6  24  10 756  39]
 [ 14   9  12  35  60  70  20 569]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [173][   10/  207]    Overall Loss 0.010674    Objective Loss 0.010674                                        LR 0.000250    Time 0.144625    
Epoch: [173][   20/  207]    Overall Loss 0.010030    Objective Loss 0.010030                                        LR 0.000250    Time 0.111754    
Epoch: [173][   30/  207]    Overall Loss 0.010725    Objective Loss 0.010725                                        LR 0.000250    Time 0.099776    
Epoch: [173][   40/  207]    Overall Loss 0.010802    Objective Loss 0.010802                                        LR 0.000250    Time 0.094173    
Epoch: [173][   50/  207]    Overall Loss 0.010633    Objective Loss 0.010633                                        LR 0.000250    Time 0.090744    
Epoch: [173][   60/  207]    Overall Loss 0.010681    Objective Loss 0.010681                                        LR 0.000250    Time 0.088611    
Epoch: [173][   70/  207]    Overall Loss 0.010724    Objective Loss 0.010724                                        LR 0.000250    Time 0.087273    
Epoch: [173][   80/  207]    Overall Loss 0.010398    Objective Loss 0.010398                                        LR 0.000250    Time 0.086574    
Epoch: [173][   90/  207]    Overall Loss 0.010376    Objective Loss 0.010376                                        LR 0.000250    Time 0.084947    
Epoch: [173][  100/  207]    Overall Loss 0.010426    Objective Loss 0.010426                                        LR 0.000250    Time 0.084362    
Epoch: [173][  110/  207]    Overall Loss 0.010492    Objective Loss 0.010492                                        LR 0.000250    Time 0.084489    
Epoch: [173][  120/  207]    Overall Loss 0.010676    Objective Loss 0.010676                                        LR 0.000250    Time 0.084272    
Epoch: [173][  130/  207]    Overall Loss 0.010736    Objective Loss 0.010736                                        LR 0.000250    Time 0.083333    
Epoch: [173][  140/  207]    Overall Loss 0.010617    Objective Loss 0.010617                                        LR 0.000250    Time 0.082571    
Epoch: [173][  150/  207]    Overall Loss 0.010562    Objective Loss 0.010562                                        LR 0.000250    Time 0.082487    
Epoch: [173][  160/  207]    Overall Loss 0.010606    Objective Loss 0.010606                                        LR 0.000250    Time 0.082063    
Epoch: [173][  170/  207]    Overall Loss 0.010675    Objective Loss 0.010675                                        LR 0.000250    Time 0.081470    
Epoch: [173][  180/  207]    Overall Loss 0.010610    Objective Loss 0.010610                                        LR 0.000250    Time 0.081272    
Epoch: [173][  190/  207]    Overall Loss 0.010595    Objective Loss 0.010595                                        LR 0.000250    Time 0.081149    
Epoch: [173][  200/  207]    Overall Loss 0.010646    Objective Loss 0.010646                                        LR 0.000250    Time 0.081024    
Epoch: [173][  207/  207]    Overall Loss 0.010695    Objective Loss 0.010695    Top1 99.207248    Top5 100.000000    LR 0.000250    Time 0.080785    
--- validate (epoch=173)-----------
5136 samples (512 per mini-batch)
Epoch: [173][   10/   11]    Loss 0.626782    Top1 85.898438    Top5 99.667969    
Epoch: [173][   11/   11]    Loss 0.614831    Top1 85.922897    Top5 99.669003    
==> Top1: 85.923    Top5: 99.669    Loss: 0.615

==> Confusion:
[[270   0   4   3   2   0   2  19]
 [  2 256  25   0   0   3   1  13]
 [  1  18 266   0   0   3   0  12]
 [  0   1   0 748  42   7   9  30]
 [  1   0   0  39 774   6  21  38]
 [  5   6   9   5  16 774  12  67]
 [  1   0   0   5  28  12 760  31]
 [ 15   9  14  32  60  70  24 565]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [174][   10/  207]    Overall Loss 0.009785    Objective Loss 0.009785                                        LR 0.000250    Time 0.135119    
Epoch: [174][   20/  207]    Overall Loss 0.009547    Objective Loss 0.009547                                        LR 0.000250    Time 0.103380    
Epoch: [174][   30/  207]    Overall Loss 0.010207    Objective Loss 0.010207                                        LR 0.000250    Time 0.092958    
Epoch: [174][   40/  207]    Overall Loss 0.010155    Objective Loss 0.010155                                        LR 0.000250    Time 0.088852    
Epoch: [174][   50/  207]    Overall Loss 0.009928    Objective Loss 0.009928                                        LR 0.000250    Time 0.088871    
Epoch: [174][   60/  207]    Overall Loss 0.009984    Objective Loss 0.009984                                        LR 0.000250    Time 0.086914    
Epoch: [174][   70/  207]    Overall Loss 0.009954    Objective Loss 0.009954                                        LR 0.000250    Time 0.084664    
Epoch: [174][   80/  207]    Overall Loss 0.009974    Objective Loss 0.009974                                        LR 0.000250    Time 0.083084    
Epoch: [174][   90/  207]    Overall Loss 0.009802    Objective Loss 0.009802                                        LR 0.000250    Time 0.081834    
Epoch: [174][  100/  207]    Overall Loss 0.009706    Objective Loss 0.009706                                        LR 0.000250    Time 0.080808    
Epoch: [174][  110/  207]    Overall Loss 0.009823    Objective Loss 0.009823                                        LR 0.000250    Time 0.079990    
Epoch: [174][  120/  207]    Overall Loss 0.009897    Objective Loss 0.009897                                        LR 0.000250    Time 0.079189    
Epoch: [174][  130/  207]    Overall Loss 0.009979    Objective Loss 0.009979                                        LR 0.000250    Time 0.078519    
Epoch: [174][  140/  207]    Overall Loss 0.009964    Objective Loss 0.009964                                        LR 0.000250    Time 0.078120    
Epoch: [174][  150/  207]    Overall Loss 0.009975    Objective Loss 0.009975                                        LR 0.000250    Time 0.077651    
Epoch: [174][  160/  207]    Overall Loss 0.009984    Objective Loss 0.009984                                        LR 0.000250    Time 0.077488    
Epoch: [174][  170/  207]    Overall Loss 0.009956    Objective Loss 0.009956                                        LR 0.000250    Time 0.077213    
Epoch: [174][  180/  207]    Overall Loss 0.010194    Objective Loss 0.010194                                        LR 0.000250    Time 0.076882    
Epoch: [174][  190/  207]    Overall Loss 0.010183    Objective Loss 0.010183                                        LR 0.000250    Time 0.076777    
Epoch: [174][  200/  207]    Overall Loss 0.010222    Objective Loss 0.010222                                        LR 0.000250    Time 0.076970    
Epoch: [174][  207/  207]    Overall Loss 0.010196    Objective Loss 0.010196    Top1 98.754247    Top5 100.000000    LR 0.000250    Time 0.076769    
--- validate (epoch=174)-----------
5136 samples (512 per mini-batch)
Epoch: [174][   10/   11]    Loss 0.594415    Top1 85.371094    Top5 99.707031    
Epoch: [174][   11/   11]    Loss 0.568509    Top1 85.377726    Top5 99.707944    
==> Top1: 85.378    Top5: 99.708    Loss: 0.569

==> Confusion:
[[273   2   4   4   2   0   1  14]
 [  2 263  20   0   0   4   1  10]
 [  2  19 265   1   0   3   0  10]
 [  0   2   0 745  49   9   7  25]
 [  2   0   0  43 776   6  17  35]
 [  4   7   9   5  19 792  11  47]
 [  2   0   0   4  32  15 756  28]
 [ 18  11  14  42  72  95  22 515]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [175][   10/  207]    Overall Loss 0.008170    Objective Loss 0.008170                                        LR 0.000250    Time 0.134776    
Epoch: [175][   20/  207]    Overall Loss 0.009548    Objective Loss 0.009548                                        LR 0.000250    Time 0.106507    
Epoch: [175][   30/  207]    Overall Loss 0.010104    Objective Loss 0.010104                                        LR 0.000250    Time 0.095897    
Epoch: [175][   40/  207]    Overall Loss 0.009674    Objective Loss 0.009674                                        LR 0.000250    Time 0.091882    
Epoch: [175][   50/  207]    Overall Loss 0.009639    Objective Loss 0.009639                                        LR 0.000250    Time 0.088005    
Epoch: [175][   60/  207]    Overall Loss 0.009814    Objective Loss 0.009814                                        LR 0.000250    Time 0.085880    
Epoch: [175][   70/  207]    Overall Loss 0.009811    Objective Loss 0.009811                                        LR 0.000250    Time 0.083710    
Epoch: [175][   80/  207]    Overall Loss 0.009664    Objective Loss 0.009664                                        LR 0.000250    Time 0.082463    
Epoch: [175][   90/  207]    Overall Loss 0.009846    Objective Loss 0.009846                                        LR 0.000250    Time 0.081475    
Epoch: [175][  100/  207]    Overall Loss 0.009669    Objective Loss 0.009669                                        LR 0.000250    Time 0.080906    
Epoch: [175][  110/  207]    Overall Loss 0.009712    Objective Loss 0.009712                                        LR 0.000250    Time 0.080153    
Epoch: [175][  120/  207]    Overall Loss 0.010182    Objective Loss 0.010182                                        LR 0.000250    Time 0.079619    
Epoch: [175][  130/  207]    Overall Loss 0.010255    Objective Loss 0.010255                                        LR 0.000250    Time 0.079236    
Epoch: [175][  140/  207]    Overall Loss 0.010505    Objective Loss 0.010505                                        LR 0.000250    Time 0.078692    
Epoch: [175][  150/  207]    Overall Loss 0.010591    Objective Loss 0.010591                                        LR 0.000250    Time 0.078467    
Epoch: [175][  160/  207]    Overall Loss 0.010725    Objective Loss 0.010725                                        LR 0.000250    Time 0.078087    
Epoch: [175][  170/  207]    Overall Loss 0.010726    Objective Loss 0.010726                                        LR 0.000250    Time 0.077668    
Epoch: [175][  180/  207]    Overall Loss 0.010814    Objective Loss 0.010814                                        LR 0.000250    Time 0.077293    
Epoch: [175][  190/  207]    Overall Loss 0.010807    Objective Loss 0.010807                                        LR 0.000250    Time 0.076981    
Epoch: [175][  200/  207]    Overall Loss 0.010904    Objective Loss 0.010904                                        LR 0.000250    Time 0.076708    
Epoch: [175][  207/  207]    Overall Loss 0.010896    Objective Loss 0.010896    Top1 99.207248    Top5 100.000000    LR 0.000250    Time 0.076435    
--- validate (epoch=175)-----------
5136 samples (512 per mini-batch)
Epoch: [175][   10/   11]    Loss 0.645460    Top1 85.527344    Top5 99.609375    
Epoch: [175][   11/   11]    Loss 0.687914    Top1 85.533489    Top5 99.591121    
==> Top1: 85.533    Top5: 99.591    Loss: 0.688

==> Confusion:
[[271   2   3   4   2   0   2  16]
 [  2 256  24   0   0   4   0  14]
 [  1  17 269   1   0   3   0   9]
 [  0   2   1 744  47   7   7  29]
 [  1   0   0  39 785   4  17  33]
 [  4   4   7   9  22 770  12  66]
 [  1   0   0   6  38   9 751  32]
 [ 14  12  12  39  75  73  17 547]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [176][   10/  207]    Overall Loss 0.008614    Objective Loss 0.008614                                        LR 0.000250    Time 0.130951    
Epoch: [176][   20/  207]    Overall Loss 0.009648    Objective Loss 0.009648                                        LR 0.000250    Time 0.101044    
Epoch: [176][   30/  207]    Overall Loss 0.009903    Objective Loss 0.009903                                        LR 0.000250    Time 0.091205    
Epoch: [176][   40/  207]    Overall Loss 0.010132    Objective Loss 0.010132                                        LR 0.000250    Time 0.086160    
Epoch: [176][   50/  207]    Overall Loss 0.010098    Objective Loss 0.010098                                        LR 0.000250    Time 0.082978    
Epoch: [176][   60/  207]    Overall Loss 0.010097    Objective Loss 0.010097                                        LR 0.000250    Time 0.081075    
Epoch: [176][   70/  207]    Overall Loss 0.010234    Objective Loss 0.010234                                        LR 0.000250    Time 0.079678    
Epoch: [176][   80/  207]    Overall Loss 0.010618    Objective Loss 0.010618                                        LR 0.000250    Time 0.078667    
Epoch: [176][   90/  207]    Overall Loss 0.010810    Objective Loss 0.010810                                        LR 0.000250    Time 0.078052    
Epoch: [176][  100/  207]    Overall Loss 0.010754    Objective Loss 0.010754                                        LR 0.000250    Time 0.077297    
Epoch: [176][  110/  207]    Overall Loss 0.010657    Objective Loss 0.010657                                        LR 0.000250    Time 0.076889    
Epoch: [176][  120/  207]    Overall Loss 0.010673    Objective Loss 0.010673                                        LR 0.000250    Time 0.076554    
Epoch: [176][  130/  207]    Overall Loss 0.010713    Objective Loss 0.010713                                        LR 0.000250    Time 0.076143    
Epoch: [176][  140/  207]    Overall Loss 0.010754    Objective Loss 0.010754                                        LR 0.000250    Time 0.075897    
Epoch: [176][  150/  207]    Overall Loss 0.010802    Objective Loss 0.010802                                        LR 0.000250    Time 0.075541    
Epoch: [176][  160/  207]    Overall Loss 0.010923    Objective Loss 0.010923                                        LR 0.000250    Time 0.075321    
Epoch: [176][  170/  207]    Overall Loss 0.011070    Objective Loss 0.011070                                        LR 0.000250    Time 0.075047    
Epoch: [176][  180/  207]    Overall Loss 0.011180    Objective Loss 0.011180                                        LR 0.000250    Time 0.074963    
Epoch: [176][  190/  207]    Overall Loss 0.011254    Objective Loss 0.011254                                        LR 0.000250    Time 0.074775    
Epoch: [176][  200/  207]    Overall Loss 0.011290    Objective Loss 0.011290                                        LR 0.000250    Time 0.074578    
Epoch: [176][  207/  207]    Overall Loss 0.011349    Objective Loss 0.011349    Top1 98.867497    Top5 100.000000    LR 0.000250    Time 0.074325    
--- validate (epoch=176)-----------
5136 samples (512 per mini-batch)
Epoch: [176][   10/   11]    Loss 0.624551    Top1 85.625000    Top5 99.707031    
Epoch: [176][   11/   11]    Loss 0.690086    Top1 85.630841    Top5 99.707944    
==> Top1: 85.631    Top5: 99.708    Loss: 0.690

==> Confusion:
[[272   2   3   4   2   0   2  15]
 [  1 261  22   0   0   5   0  11]
 [  3  20 262   1   0   4   0  10]
 [  0   0   0 762  41   8   6  20]
 [  2   0   0  47 772   6  13  39]
 [  4   6   6   9  19 788   7  55]
 [  1   0   0   5  30  13 752  36]
 [ 15  11  11  49  63  93  18 529]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [177][   10/  207]    Overall Loss 0.012220    Objective Loss 0.012220                                        LR 0.000250    Time 0.135204    
Epoch: [177][   20/  207]    Overall Loss 0.011058    Objective Loss 0.011058                                        LR 0.000250    Time 0.105074    
Epoch: [177][   30/  207]    Overall Loss 0.010316    Objective Loss 0.010316                                        LR 0.000250    Time 0.095009    
Epoch: [177][   40/  207]    Overall Loss 0.010423    Objective Loss 0.010423                                        LR 0.000250    Time 0.089228    
Epoch: [177][   50/  207]    Overall Loss 0.010418    Objective Loss 0.010418                                        LR 0.000250    Time 0.086637    
Epoch: [177][   60/  207]    Overall Loss 0.010395    Objective Loss 0.010395                                        LR 0.000250    Time 0.084472    
Epoch: [177][   70/  207]    Overall Loss 0.010443    Objective Loss 0.010443                                        LR 0.000250    Time 0.083296    
Epoch: [177][   80/  207]    Overall Loss 0.010387    Objective Loss 0.010387                                        LR 0.000250    Time 0.081786    
Epoch: [177][   90/  207]    Overall Loss 0.010350    Objective Loss 0.010350                                        LR 0.000250    Time 0.080604    
Epoch: [177][  100/  207]    Overall Loss 0.010436    Objective Loss 0.010436                                        LR 0.000250    Time 0.079814    
Epoch: [177][  110/  207]    Overall Loss 0.010648    Objective Loss 0.010648                                        LR 0.000250    Time 0.079097    
Epoch: [177][  120/  207]    Overall Loss 0.010765    Objective Loss 0.010765                                        LR 0.000250    Time 0.078785    
Epoch: [177][  130/  207]    Overall Loss 0.010940    Objective Loss 0.010940                                        LR 0.000250    Time 0.078538    
Epoch: [177][  140/  207]    Overall Loss 0.011052    Objective Loss 0.011052                                        LR 0.000250    Time 0.078091    
Epoch: [177][  150/  207]    Overall Loss 0.011224    Objective Loss 0.011224                                        LR 0.000250    Time 0.077682    
Epoch: [177][  160/  207]    Overall Loss 0.011439    Objective Loss 0.011439                                        LR 0.000250    Time 0.077352    
Epoch: [177][  170/  207]    Overall Loss 0.011625    Objective Loss 0.011625                                        LR 0.000250    Time 0.077461    
Epoch: [177][  180/  207]    Overall Loss 0.011638    Objective Loss 0.011638                                        LR 0.000250    Time 0.077678    
Epoch: [177][  190/  207]    Overall Loss 0.011978    Objective Loss 0.011978                                        LR 0.000250    Time 0.077376    
Epoch: [177][  200/  207]    Overall Loss 0.012300    Objective Loss 0.012300                                        LR 0.000250    Time 0.077576    
Epoch: [177][  207/  207]    Overall Loss 0.012544    Objective Loss 0.012544    Top1 98.527746    Top5 100.000000    LR 0.000250    Time 0.077287    
--- validate (epoch=177)-----------
5136 samples (512 per mini-batch)
Epoch: [177][   10/   11]    Loss 0.585489    Top1 85.390625    Top5 99.765625    
Epoch: [177][   11/   11]    Loss 0.539488    Top1 85.416667    Top5 99.766355    
==> Top1: 85.417    Top5: 99.766    Loss: 0.539

==> Confusion:
[[273   2   3   2   1   2   2  15]
 [  2 256  26   0   0   4   1  11]
 [  2  19 266   0   0   5   0   8]
 [  0   1   0 736  62  11   7  20]
 [  2   0   0  32 786   8  21  30]
 [  2   5   8   4  16 812  11  36]
 [  1   0   0   6  36  11 763  20]
 [ 15  11  13  38  69 119  29 495]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [178][   10/  207]    Overall Loss 0.018480    Objective Loss 0.018480                                        LR 0.000250    Time 0.130647    
Epoch: [178][   20/  207]    Overall Loss 0.020298    Objective Loss 0.020298                                        LR 0.000250    Time 0.100154    
Epoch: [178][   30/  207]    Overall Loss 0.022869    Objective Loss 0.022869                                        LR 0.000250    Time 0.091060    
Epoch: [178][   40/  207]    Overall Loss 0.025574    Objective Loss 0.025574                                        LR 0.000250    Time 0.086440    
Epoch: [178][   50/  207]    Overall Loss 0.026314    Objective Loss 0.026314                                        LR 0.000250    Time 0.083384    
Epoch: [178][   60/  207]    Overall Loss 0.027803    Objective Loss 0.027803                                        LR 0.000250    Time 0.081931    
Epoch: [178][   70/  207]    Overall Loss 0.027773    Objective Loss 0.027773                                        LR 0.000250    Time 0.080385    
Epoch: [178][   80/  207]    Overall Loss 0.028277    Objective Loss 0.028277                                        LR 0.000250    Time 0.079368    
Epoch: [178][   90/  207]    Overall Loss 0.028923    Objective Loss 0.028923                                        LR 0.000250    Time 0.079070    
Epoch: [178][  100/  207]    Overall Loss 0.028635    Objective Loss 0.028635                                        LR 0.000250    Time 0.079274    
Epoch: [178][  110/  207]    Overall Loss 0.030164    Objective Loss 0.030164                                        LR 0.000250    Time 0.078837    
Epoch: [178][  120/  207]    Overall Loss 0.031223    Objective Loss 0.031223                                        LR 0.000250    Time 0.078484    
Epoch: [178][  130/  207]    Overall Loss 0.031564    Objective Loss 0.031564                                        LR 0.000250    Time 0.078060    
Epoch: [178][  140/  207]    Overall Loss 0.033917    Objective Loss 0.033917                                        LR 0.000250    Time 0.077645    
Epoch: [178][  150/  207]    Overall Loss 0.038746    Objective Loss 0.038746                                        LR 0.000250    Time 0.077658    
Epoch: [178][  160/  207]    Overall Loss 0.041691    Objective Loss 0.041691                                        LR 0.000250    Time 0.077954    
Epoch: [178][  170/  207]    Overall Loss 0.043166    Objective Loss 0.043166                                        LR 0.000250    Time 0.078097    
Epoch: [178][  180/  207]    Overall Loss 0.043029    Objective Loss 0.043029                                        LR 0.000250    Time 0.078030    
Epoch: [178][  190/  207]    Overall Loss 0.042972    Objective Loss 0.042972                                        LR 0.000250    Time 0.078150    
Epoch: [178][  200/  207]    Overall Loss 0.042673    Objective Loss 0.042673                                        LR 0.000250    Time 0.078296    
Epoch: [178][  207/  207]    Overall Loss 0.042304    Objective Loss 0.042304    Top1 98.187995    Top5 100.000000    LR 0.000250    Time 0.078077    
--- validate (epoch=178)-----------
5136 samples (512 per mini-batch)
Epoch: [178][   10/   11]    Loss 0.627172    Top1 84.570312    Top5 99.687500    
Epoch: [178][   11/   11]    Loss 0.608916    Top1 84.559969    Top5 99.688474    
==> Top1: 84.560    Top5: 99.688    Loss: 0.609

==> Confusion:
[[283   2   3   2   0   0   1   9]
 [  5 252  32   0   0   3   1   7]
 [  5  18 269   0   0   2   0   6]
 [  0   4   0 739  46   9  14  25]
 [  4   0   0  44 772   3  19  37]
 [ 21   7  14   7  17 757  10  61]
 [  3   0   0   5  31  12 757  29]
 [ 34  15  23  35  61  86  21 514]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [179][   10/  207]    Overall Loss 0.030664    Objective Loss 0.030664                                        LR 0.000250    Time 0.132114    
Epoch: [179][   20/  207]    Overall Loss 0.029145    Objective Loss 0.029145                                        LR 0.000250    Time 0.106226    
Epoch: [179][   30/  207]    Overall Loss 0.027531    Objective Loss 0.027531                                        LR 0.000250    Time 0.097740    
Epoch: [179][   40/  207]    Overall Loss 0.025717    Objective Loss 0.025717                                        LR 0.000250    Time 0.092847    
Epoch: [179][   50/  207]    Overall Loss 0.023972    Objective Loss 0.023972                                        LR 0.000250    Time 0.088844    
Epoch: [179][   60/  207]    Overall Loss 0.023192    Objective Loss 0.023192                                        LR 0.000250    Time 0.085976    
Epoch: [179][   70/  207]    Overall Loss 0.022283    Objective Loss 0.022283                                        LR 0.000250    Time 0.084579    
Epoch: [179][   80/  207]    Overall Loss 0.021679    Objective Loss 0.021679                                        LR 0.000250    Time 0.083124    
Epoch: [179][   90/  207]    Overall Loss 0.021265    Objective Loss 0.021265                                        LR 0.000250    Time 0.082212    
Epoch: [179][  100/  207]    Overall Loss 0.021063    Objective Loss 0.021063                                        LR 0.000250    Time 0.081315    
Epoch: [179][  110/  207]    Overall Loss 0.020773    Objective Loss 0.020773                                        LR 0.000250    Time 0.080516    
Epoch: [179][  120/  207]    Overall Loss 0.020297    Objective Loss 0.020297                                        LR 0.000250    Time 0.079789    
Epoch: [179][  130/  207]    Overall Loss 0.020223    Objective Loss 0.020223                                        LR 0.000250    Time 0.079206    
Epoch: [179][  140/  207]    Overall Loss 0.019858    Objective Loss 0.019858                                        LR 0.000250    Time 0.078725    
Epoch: [179][  150/  207]    Overall Loss 0.019462    Objective Loss 0.019462                                        LR 0.000250    Time 0.078588    
Epoch: [179][  160/  207]    Overall Loss 0.019170    Objective Loss 0.019170                                        LR 0.000250    Time 0.078123    
Epoch: [179][  170/  207]    Overall Loss 0.018839    Objective Loss 0.018839                                        LR 0.000250    Time 0.077810    
Epoch: [179][  180/  207]    Overall Loss 0.018492    Objective Loss 0.018492                                        LR 0.000250    Time 0.077645    
Epoch: [179][  190/  207]    Overall Loss 0.018327    Objective Loss 0.018327                                        LR 0.000250    Time 0.077648    
Epoch: [179][  200/  207]    Overall Loss 0.018262    Objective Loss 0.018262                                        LR 0.000250    Time 0.077586    
Epoch: [179][  207/  207]    Overall Loss 0.018130    Objective Loss 0.018130    Top1 98.640997    Top5 100.000000    LR 0.000250    Time 0.077315    
--- validate (epoch=179)-----------
5136 samples (512 per mini-batch)
Epoch: [179][   10/   11]    Loss 0.605325    Top1 85.683594    Top5 99.765625    
Epoch: [179][   11/   11]    Loss 0.554916    Top1 85.708723    Top5 99.766355    
==> Top1: 85.709    Top5: 99.766    Loss: 0.555

==> Confusion:
[[276   2   3   2   0   0   2  15]
 [  1 261  22   0   0   2   1  13]
 [  2  18 269   1   0   2   0   8]
 [  0   1   1 757  45   9   7  17]
 [  2   0   0  41 782   4  12  38]
 [  6   6   9   9  17 773  14  60]
 [  1   0   0   3  40   8 750  35]
 [ 19  11  10  44  65  84  22 534]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [180][   10/  207]    Overall Loss 0.011438    Objective Loss 0.011438                                        LR 0.000125    Time 0.138098    
Epoch: [180][   20/  207]    Overall Loss 0.011585    Objective Loss 0.011585                                        LR 0.000125    Time 0.104555    
Epoch: [180][   30/  207]    Overall Loss 0.011772    Objective Loss 0.011772                                        LR 0.000125    Time 0.093889    
Epoch: [180][   40/  207]    Overall Loss 0.011583    Objective Loss 0.011583                                        LR 0.000125    Time 0.088257    
Epoch: [180][   50/  207]    Overall Loss 0.011350    Objective Loss 0.011350                                        LR 0.000125    Time 0.084874    
Epoch: [180][   60/  207]    Overall Loss 0.011090    Objective Loss 0.011090                                        LR 0.000125    Time 0.082757    
Epoch: [180][   70/  207]    Overall Loss 0.011149    Objective Loss 0.011149                                        LR 0.000125    Time 0.081185    
Epoch: [180][   80/  207]    Overall Loss 0.011045    Objective Loss 0.011045                                        LR 0.000125    Time 0.079871    
Epoch: [180][   90/  207]    Overall Loss 0.010954    Objective Loss 0.010954                                        LR 0.000125    Time 0.078964    
Epoch: [180][  100/  207]    Overall Loss 0.010998    Objective Loss 0.010998                                        LR 0.000125    Time 0.078279    
Epoch: [180][  110/  207]    Overall Loss 0.010861    Objective Loss 0.010861                                        LR 0.000125    Time 0.077876    
Epoch: [180][  120/  207]    Overall Loss 0.011023    Objective Loss 0.011023                                        LR 0.000125    Time 0.077311    
Epoch: [180][  130/  207]    Overall Loss 0.010991    Objective Loss 0.010991                                        LR 0.000125    Time 0.076782    
Epoch: [180][  140/  207]    Overall Loss 0.011013    Objective Loss 0.011013                                        LR 0.000125    Time 0.076404    
Epoch: [180][  150/  207]    Overall Loss 0.011015    Objective Loss 0.011015                                        LR 0.000125    Time 0.076164    
Epoch: [180][  160/  207]    Overall Loss 0.010944    Objective Loss 0.010944                                        LR 0.000125    Time 0.075831    
Epoch: [180][  170/  207]    Overall Loss 0.010967    Objective Loss 0.010967                                        LR 0.000125    Time 0.075715    
Epoch: [180][  180/  207]    Overall Loss 0.011037    Objective Loss 0.011037                                        LR 0.000125    Time 0.075819    
Epoch: [180][  190/  207]    Overall Loss 0.010987    Objective Loss 0.010987                                        LR 0.000125    Time 0.075627    
Epoch: [180][  200/  207]    Overall Loss 0.011033    Objective Loss 0.011033                                        LR 0.000125    Time 0.075503    
Epoch: [180][  207/  207]    Overall Loss 0.011132    Objective Loss 0.011132    Top1 99.207248    Top5 100.000000    LR 0.000125    Time 0.075247    
--- validate (epoch=180)-----------
5136 samples (512 per mini-batch)
Epoch: [180][   10/   11]    Loss 0.608413    Top1 85.859375    Top5 99.667969    
Epoch: [180][   11/   11]    Loss 0.640201    Top1 85.806075    Top5 99.669003    
==> Top1: 85.806    Top5: 99.669    Loss: 0.640

==> Confusion:
[[274   1   2   3   1   0   2  17]
 [  1 260  23   0   0   2   1  13]
 [  2  20 264   0   0   4   0  10]
 [  0   0   1 754  44   8   7  23]
 [  1   0   0  42 777   4  18  37]
 [  2   5   9   9  17 776  17  59]
 [  1   0   0   4  31   8 758  35]
 [ 15   8  10  37  66  85  24 544]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [181][   10/  207]    Overall Loss 0.012329    Objective Loss 0.012329                                        LR 0.000125    Time 0.133207    
Epoch: [181][   20/  207]    Overall Loss 0.011042    Objective Loss 0.011042                                        LR 0.000125    Time 0.102201    
Epoch: [181][   30/  207]    Overall Loss 0.010634    Objective Loss 0.010634                                        LR 0.000125    Time 0.091942    
Epoch: [181][   40/  207]    Overall Loss 0.010289    Objective Loss 0.010289                                        LR 0.000125    Time 0.087261    
Epoch: [181][   50/  207]    Overall Loss 0.010154    Objective Loss 0.010154                                        LR 0.000125    Time 0.084566    
Epoch: [181][   60/  207]    Overall Loss 0.010210    Objective Loss 0.010210                                        LR 0.000125    Time 0.082632    
Epoch: [181][   70/  207]    Overall Loss 0.010071    Objective Loss 0.010071                                        LR 0.000125    Time 0.081212    
Epoch: [181][   80/  207]    Overall Loss 0.010076    Objective Loss 0.010076                                        LR 0.000125    Time 0.080140    
Epoch: [181][   90/  207]    Overall Loss 0.010098    Objective Loss 0.010098                                        LR 0.000125    Time 0.079486    
Epoch: [181][  100/  207]    Overall Loss 0.010079    Objective Loss 0.010079                                        LR 0.000125    Time 0.078787    
Epoch: [181][  110/  207]    Overall Loss 0.010144    Objective Loss 0.010144                                        LR 0.000125    Time 0.078091    
Epoch: [181][  120/  207]    Overall Loss 0.010060    Objective Loss 0.010060                                        LR 0.000125    Time 0.077951    
Epoch: [181][  130/  207]    Overall Loss 0.010259    Objective Loss 0.010259                                        LR 0.000125    Time 0.078986    
Epoch: [181][  140/  207]    Overall Loss 0.010379    Objective Loss 0.010379                                        LR 0.000125    Time 0.079327    
Epoch: [181][  150/  207]    Overall Loss 0.010285    Objective Loss 0.010285                                        LR 0.000125    Time 0.079642    
Epoch: [181][  160/  207]    Overall Loss 0.010295    Objective Loss 0.010295                                        LR 0.000125    Time 0.079907    
Epoch: [181][  170/  207]    Overall Loss 0.010269    Objective Loss 0.010269                                        LR 0.000125    Time 0.080075    
Epoch: [181][  180/  207]    Overall Loss 0.010418    Objective Loss 0.010418                                        LR 0.000125    Time 0.080408    
Epoch: [181][  190/  207]    Overall Loss 0.010426    Objective Loss 0.010426                                        LR 0.000125    Time 0.080601    
Epoch: [181][  200/  207]    Overall Loss 0.010383    Objective Loss 0.010383                                        LR 0.000125    Time 0.080730    
Epoch: [181][  207/  207]    Overall Loss 0.010318    Objective Loss 0.010318    Top1 99.433749    Top5 100.000000    LR 0.000125    Time 0.080631    
--- validate (epoch=181)-----------
5136 samples (512 per mini-batch)
Epoch: [181][   10/   11]    Loss 0.603547    Top1 85.859375    Top5 99.687500    
Epoch: [181][   11/   11]    Loss 0.556625    Top1 85.864486    Top5 99.688474    
==> Top1: 85.864    Top5: 99.688    Loss: 0.557

==> Confusion:
[[271   2   3   3   1   0   2  18]
 [  1 260  23   0   0   2   1  13]
 [  2  16 269   0   0   4   0   9]
 [  0   0   1 751  41   9   8  27]
 [  1   0   0  43 763   5  22  45]
 [  1   6   8   6  15 790  14  54]
 [  1   0   0   4  25   9 764  34]
 [ 15  11  12  40  54  92  23 542]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [182][   10/  207]    Overall Loss 0.010329    Objective Loss 0.010329                                        LR 0.000125    Time 0.134434    
Epoch: [182][   20/  207]    Overall Loss 0.010203    Objective Loss 0.010203                                        LR 0.000125    Time 0.103765    
Epoch: [182][   30/  207]    Overall Loss 0.009878    Objective Loss 0.009878                                        LR 0.000125    Time 0.094013    
Epoch: [182][   40/  207]    Overall Loss 0.009620    Objective Loss 0.009620                                        LR 0.000125    Time 0.090515    
Epoch: [182][   50/  207]    Overall Loss 0.009594    Objective Loss 0.009594                                        LR 0.000125    Time 0.087922    
Epoch: [182][   60/  207]    Overall Loss 0.009522    Objective Loss 0.009522                                        LR 0.000125    Time 0.086583    
Epoch: [182][   70/  207]    Overall Loss 0.009626    Objective Loss 0.009626                                        LR 0.000125    Time 0.084664    
Epoch: [182][   80/  207]    Overall Loss 0.009614    Objective Loss 0.009614                                        LR 0.000125    Time 0.083692    
Epoch: [182][   90/  207]    Overall Loss 0.009611    Objective Loss 0.009611                                        LR 0.000125    Time 0.082489    
Epoch: [182][  100/  207]    Overall Loss 0.009662    Objective Loss 0.009662                                        LR 0.000125    Time 0.082057    
Epoch: [182][  110/  207]    Overall Loss 0.009659    Objective Loss 0.009659                                        LR 0.000125    Time 0.081488    
Epoch: [182][  120/  207]    Overall Loss 0.009625    Objective Loss 0.009625                                        LR 0.000125    Time 0.080749    
Epoch: [182][  130/  207]    Overall Loss 0.009660    Objective Loss 0.009660                                        LR 0.000125    Time 0.080166    
Epoch: [182][  140/  207]    Overall Loss 0.009629    Objective Loss 0.009629                                        LR 0.000125    Time 0.079609    
Epoch: [182][  150/  207]    Overall Loss 0.009669    Objective Loss 0.009669                                        LR 0.000125    Time 0.079782    
Epoch: [182][  160/  207]    Overall Loss 0.009826    Objective Loss 0.009826                                        LR 0.000125    Time 0.079391    
Epoch: [182][  170/  207]    Overall Loss 0.009826    Objective Loss 0.009826                                        LR 0.000125    Time 0.078941    
Epoch: [182][  180/  207]    Overall Loss 0.009812    Objective Loss 0.009812                                        LR 0.000125    Time 0.078643    
Epoch: [182][  190/  207]    Overall Loss 0.009865    Objective Loss 0.009865                                        LR 0.000125    Time 0.078249    
Epoch: [182][  200/  207]    Overall Loss 0.009886    Objective Loss 0.009886                                        LR 0.000125    Time 0.078140    
Epoch: [182][  207/  207]    Overall Loss 0.009860    Objective Loss 0.009860    Top1 98.980747    Top5 100.000000    LR 0.000125    Time 0.077823    
--- validate (epoch=182)-----------
5136 samples (512 per mini-batch)
Epoch: [182][   10/   11]    Loss 0.595155    Top1 85.957031    Top5 99.667969    
Epoch: [182][   11/   11]    Loss 0.602477    Top1 85.942368    Top5 99.669003    
==> Top1: 85.942    Top5: 99.669    Loss: 0.602

==> Confusion:
[[276   2   3   3   1   0   2  13]
 [  1 262  22   0   0   2   1  12]
 [  2  17 268   1   0   3   0   9]
 [  0   1   1 752  47   8   8  20]
 [  1   0   0  42 771   6  20  39]
 [  5   5   9   6  16 788  13  52]
 [  1   0   0   4  26  10 765  31]
 [ 17  14  11  38  62  92  23 532]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [183][   10/  207]    Overall Loss 0.009967    Objective Loss 0.009967                                        LR 0.000125    Time 0.136192    
Epoch: [183][   20/  207]    Overall Loss 0.009927    Objective Loss 0.009927                                        LR 0.000125    Time 0.104599    
Epoch: [183][   30/  207]    Overall Loss 0.009441    Objective Loss 0.009441                                        LR 0.000125    Time 0.093209    
Epoch: [183][   40/  207]    Overall Loss 0.009076    Objective Loss 0.009076                                        LR 0.000125    Time 0.087678    
Epoch: [183][   50/  207]    Overall Loss 0.009053    Objective Loss 0.009053                                        LR 0.000125    Time 0.084586    
Epoch: [183][   60/  207]    Overall Loss 0.009108    Objective Loss 0.009108                                        LR 0.000125    Time 0.082509    
Epoch: [183][   70/  207]    Overall Loss 0.009349    Objective Loss 0.009349                                        LR 0.000125    Time 0.081681    
Epoch: [183][   80/  207]    Overall Loss 0.009357    Objective Loss 0.009357                                        LR 0.000125    Time 0.080730    
Epoch: [183][   90/  207]    Overall Loss 0.009446    Objective Loss 0.009446                                        LR 0.000125    Time 0.080232    
Epoch: [183][  100/  207]    Overall Loss 0.009428    Objective Loss 0.009428                                        LR 0.000125    Time 0.079357    
Epoch: [183][  110/  207]    Overall Loss 0.009578    Objective Loss 0.009578                                        LR 0.000125    Time 0.078662    
Epoch: [183][  120/  207]    Overall Loss 0.009714    Objective Loss 0.009714                                        LR 0.000125    Time 0.077999    
Epoch: [183][  130/  207]    Overall Loss 0.009783    Objective Loss 0.009783                                        LR 0.000125    Time 0.077460    
Epoch: [183][  140/  207]    Overall Loss 0.009725    Objective Loss 0.009725                                        LR 0.000125    Time 0.077000    
Epoch: [183][  150/  207]    Overall Loss 0.009701    Objective Loss 0.009701                                        LR 0.000125    Time 0.076976    
Epoch: [183][  160/  207]    Overall Loss 0.009618    Objective Loss 0.009618                                        LR 0.000125    Time 0.076633    
Epoch: [183][  170/  207]    Overall Loss 0.009651    Objective Loss 0.009651                                        LR 0.000125    Time 0.076591    
Epoch: [183][  180/  207]    Overall Loss 0.009638    Objective Loss 0.009638                                        LR 0.000125    Time 0.076512    
Epoch: [183][  190/  207]    Overall Loss 0.009633    Objective Loss 0.009633                                        LR 0.000125    Time 0.076390    
Epoch: [183][  200/  207]    Overall Loss 0.009654    Objective Loss 0.009654                                        LR 0.000125    Time 0.076146    
Epoch: [183][  207/  207]    Overall Loss 0.009616    Objective Loss 0.009616    Top1 99.546999    Top5 100.000000    LR 0.000125    Time 0.075865    
--- validate (epoch=183)-----------
5136 samples (512 per mini-batch)
Epoch: [183][   10/   11]    Loss 0.597889    Top1 85.898438    Top5 99.707031    
Epoch: [183][   11/   11]    Loss 0.598278    Top1 85.903427    Top5 99.707944    
==> Top1: 85.903    Top5: 99.708    Loss: 0.598

==> Confusion:
[[275   1   4   4   1   1   1  13]
 [  1 261  22   0   0   2   1  13]
 [  2  20 267   0   0   4   0   7]
 [  0   1   1 757  41   8   7  22]
 [  2   0   0  42 770   6  17  42]
 [  3   6   8   7  15 790  11  54]
 [  1   1   0   5  25  12 756  37]
 [ 16  11  13  46  60  87  20 536]]

==> Best [Top1: 85.981   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 149]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [184][   10/  207]    Overall Loss 0.009402    Objective Loss 0.009402                                        LR 0.000125    Time 0.129051    
Epoch: [184][   20/  207]    Overall Loss 0.009681    Objective Loss 0.009681                                        LR 0.000125    Time 0.100883    
Epoch: [184][   30/  207]    Overall Loss 0.009085    Objective Loss 0.009085                                        LR 0.000125    Time 0.091660    
Epoch: [184][   40/  207]    Overall Loss 0.008776    Objective Loss 0.008776                                        LR 0.000125    Time 0.086506    
Epoch: [184][   50/  207]    Overall Loss 0.008917    Objective Loss 0.008917                                        LR 0.000125    Time 0.083504    
Epoch: [184][   60/  207]    Overall Loss 0.009068    Objective Loss 0.009068                                        LR 0.000125    Time 0.081795    
Epoch: [184][   70/  207]    Overall Loss 0.009178    Objective Loss 0.009178                                        LR 0.000125    Time 0.081156    
Epoch: [184][   80/  207]    Overall Loss 0.009178    Objective Loss 0.009178                                        LR 0.000125    Time 0.080594    
Epoch: [184][   90/  207]    Overall Loss 0.009306    Objective Loss 0.009306                                        LR 0.000125    Time 0.079540    
Epoch: [184][  100/  207]    Overall Loss 0.009306    Objective Loss 0.009306                                        LR 0.000125    Time 0.078745    
Epoch: [184][  110/  207]    Overall Loss 0.009451    Objective Loss 0.009451                                        LR 0.000125    Time 0.078018    
Epoch: [184][  120/  207]    Overall Loss 0.009343    Objective Loss 0.009343                                        LR 0.000125    Time 0.077484    
Epoch: [184][  130/  207]    Overall Loss 0.009253    Objective Loss 0.009253                                        LR 0.000125    Time 0.077077    
Epoch: [184][  140/  207]    Overall Loss 0.009240    Objective Loss 0.009240                                        LR 0.000125    Time 0.076679    
Epoch: [184][  150/  207]    Overall Loss 0.009189    Objective Loss 0.009189                                        LR 0.000125    Time 0.076644    
Epoch: [184][  160/  207]    Overall Loss 0.009258    Objective Loss 0.009258                                        LR 0.000125    Time 0.076337    
Epoch: [184][  170/  207]    Overall Loss 0.009318    Objective Loss 0.009318                                        LR 0.000125    Time 0.076119    
Epoch: [184][  180/  207]    Overall Loss 0.009353    Objective Loss 0.009353                                        LR 0.000125    Time 0.075793    
Epoch: [184][  190/  207]    Overall Loss 0.009363    Objective Loss 0.009363                                        LR 0.000125    Time 0.075565    
Epoch: [184][  200/  207]    Overall Loss 0.009389    Objective Loss 0.009389                                        LR 0.000125    Time 0.075401    
Epoch: [184][  207/  207]    Overall Loss 0.009396    Objective Loss 0.009396    Top1 99.207248    Top5 100.000000    LR 0.000125    Time 0.075129    
--- validate (epoch=184)-----------
5136 samples (512 per mini-batch)
Epoch: [184][   10/   11]    Loss 0.597291    Top1 86.093750    Top5 99.746094    
Epoch: [184][   11/   11]    Loss 0.622734    Top1 86.078660    Top5 99.727414    
==> Top1: 86.079    Top5: 99.727    Loss: 0.623

==> Confusion:
[[277   1   5   3   1   0   1  12]
 [  1 264  19   0   0   2   1  13]
 [  2  21 266   1   0   3   0   7]
 [  0   0   1 751  48   7   6  24]
 [  2   0   0  38 779   5  18  37]
 [  5   6   9   7  17 780  14  56]
 [  1   0   0   5  29   8 762  32]
 [ 18  11  10  43  64  79  22 542]]

==> Best [Top1: 86.079   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 184]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [185][   10/  207]    Overall Loss 0.009134    Objective Loss 0.009134                                        LR 0.000125    Time 0.142718    
Epoch: [185][   20/  207]    Overall Loss 0.008610    Objective Loss 0.008610                                        LR 0.000125    Time 0.107827    
Epoch: [185][   30/  207]    Overall Loss 0.008584    Objective Loss 0.008584                                        LR 0.000125    Time 0.098049    
Epoch: [185][   40/  207]    Overall Loss 0.009053    Objective Loss 0.009053                                        LR 0.000125    Time 0.092227    
Epoch: [185][   50/  207]    Overall Loss 0.008892    Objective Loss 0.008892                                        LR 0.000125    Time 0.087948    
Epoch: [185][   60/  207]    Overall Loss 0.009222    Objective Loss 0.009222                                        LR 0.000125    Time 0.085958    
Epoch: [185][   70/  207]    Overall Loss 0.009054    Objective Loss 0.009054                                        LR 0.000125    Time 0.084951    
Epoch: [185][   80/  207]    Overall Loss 0.009029    Objective Loss 0.009029                                        LR 0.000125    Time 0.083720    
Epoch: [185][   90/  207]    Overall Loss 0.008946    Objective Loss 0.008946                                        LR 0.000125    Time 0.082494    
Epoch: [185][  100/  207]    Overall Loss 0.008973    Objective Loss 0.008973                                        LR 0.000125    Time 0.081375    
Epoch: [185][  110/  207]    Overall Loss 0.008973    Objective Loss 0.008973                                        LR 0.000125    Time 0.080814    
Epoch: [185][  120/  207]    Overall Loss 0.009141    Objective Loss 0.009141                                        LR 0.000125    Time 0.080100    
Epoch: [185][  130/  207]    Overall Loss 0.009111    Objective Loss 0.009111                                        LR 0.000125    Time 0.079476    
Epoch: [185][  140/  207]    Overall Loss 0.009140    Objective Loss 0.009140                                        LR 0.000125    Time 0.078896    
Epoch: [185][  150/  207]    Overall Loss 0.009108    Objective Loss 0.009108                                        LR 0.000125    Time 0.078374    
Epoch: [185][  160/  207]    Overall Loss 0.009093    Objective Loss 0.009093                                        LR 0.000125    Time 0.078319    
Epoch: [185][  170/  207]    Overall Loss 0.009009    Objective Loss 0.009009                                        LR 0.000125    Time 0.077928    
Epoch: [185][  180/  207]    Overall Loss 0.009051    Objective Loss 0.009051                                        LR 0.000125    Time 0.077755    
Epoch: [185][  190/  207]    Overall Loss 0.009092    Objective Loss 0.009092                                        LR 0.000125    Time 0.077638    
Epoch: [185][  200/  207]    Overall Loss 0.009143    Objective Loss 0.009143                                        LR 0.000125    Time 0.077676    
Epoch: [185][  207/  207]    Overall Loss 0.009131    Objective Loss 0.009131    Top1 99.320498    Top5 100.000000    LR 0.000125    Time 0.077493    
--- validate (epoch=185)-----------
5136 samples (512 per mini-batch)
Epoch: [185][   10/   11]    Loss 0.614788    Top1 86.152344    Top5 99.687500    
Epoch: [185][   11/   11]    Loss 0.570677    Top1 86.156542    Top5 99.688474    
==> Top1: 86.157    Top5: 99.688    Loss: 0.571

==> Confusion:
[[273   0   4   4   1   0   2  16]
 [  1 264  19   0   0   3   1  12]
 [  1  20 265   0   0   4   0  10]
 [  0   0   0 751  44   7   6  29]
 [  1   0   0  41 764   3  23  47]
 [  3   5   9   7  15 777  14  64]
 [  1   0   0   6  23  10 766  31]
 [ 17  10  10  36  56  75  20 565]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [186][   10/  207]    Overall Loss 0.008059    Objective Loss 0.008059                                        LR 0.000125    Time 0.135698    
Epoch: [186][   20/  207]    Overall Loss 0.007955    Objective Loss 0.007955                                        LR 0.000125    Time 0.103480    
Epoch: [186][   30/  207]    Overall Loss 0.008408    Objective Loss 0.008408                                        LR 0.000125    Time 0.092576    
Epoch: [186][   40/  207]    Overall Loss 0.008750    Objective Loss 0.008750                                        LR 0.000125    Time 0.087676    
Epoch: [186][   50/  207]    Overall Loss 0.008857    Objective Loss 0.008857                                        LR 0.000125    Time 0.084110    
Epoch: [186][   60/  207]    Overall Loss 0.008938    Objective Loss 0.008938                                        LR 0.000125    Time 0.082038    
Epoch: [186][   70/  207]    Overall Loss 0.008964    Objective Loss 0.008964                                        LR 0.000125    Time 0.080684    
Epoch: [186][   80/  207]    Overall Loss 0.009110    Objective Loss 0.009110                                        LR 0.000125    Time 0.079495    
Epoch: [186][   90/  207]    Overall Loss 0.009146    Objective Loss 0.009146                                        LR 0.000125    Time 0.078866    
Epoch: [186][  100/  207]    Overall Loss 0.009171    Objective Loss 0.009171                                        LR 0.000125    Time 0.078005    
Epoch: [186][  110/  207]    Overall Loss 0.009236    Objective Loss 0.009236                                        LR 0.000125    Time 0.077464    
Epoch: [186][  120/  207]    Overall Loss 0.009330    Objective Loss 0.009330                                        LR 0.000125    Time 0.077260    
Epoch: [186][  130/  207]    Overall Loss 0.009329    Objective Loss 0.009329                                        LR 0.000125    Time 0.076955    
Epoch: [186][  140/  207]    Overall Loss 0.009263    Objective Loss 0.009263                                        LR 0.000125    Time 0.076568    
Epoch: [186][  150/  207]    Overall Loss 0.009221    Objective Loss 0.009221                                        LR 0.000125    Time 0.076187    
Epoch: [186][  160/  207]    Overall Loss 0.009162    Objective Loss 0.009162                                        LR 0.000125    Time 0.075953    
Epoch: [186][  170/  207]    Overall Loss 0.009049    Objective Loss 0.009049                                        LR 0.000125    Time 0.075748    
Epoch: [186][  180/  207]    Overall Loss 0.009103    Objective Loss 0.009103                                        LR 0.000125    Time 0.075501    
Epoch: [186][  190/  207]    Overall Loss 0.009072    Objective Loss 0.009072                                        LR 0.000125    Time 0.075334    
Epoch: [186][  200/  207]    Overall Loss 0.009095    Objective Loss 0.009095                                        LR 0.000125    Time 0.075162    
Epoch: [186][  207/  207]    Overall Loss 0.009112    Objective Loss 0.009112    Top1 99.546999    Top5 100.000000    LR 0.000125    Time 0.074905    
--- validate (epoch=186)-----------
5136 samples (512 per mini-batch)
Epoch: [186][   10/   11]    Loss 0.618716    Top1 85.996094    Top5 99.687500    
Epoch: [186][   11/   11]    Loss 0.562612    Top1 86.039720    Top5 99.688474    
==> Top1: 86.040    Top5: 99.688    Loss: 0.563

==> Confusion:
[[273   2   3   4   2   0   2  14]
 [  1 260  20   0   0   2   1  16]
 [  2  19 267   0   0   4   0   8]
 [  0   2   1 753  43   8   6  24]
 [  2   0   0  37 772   6  19  43]
 [  4   6   9   5  15 784  12  59]
 [  1   0   0   6  23  11 763  33]
 [ 17   9  11  40  66  82  17 547]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [187][   10/  207]    Overall Loss 0.008735    Objective Loss 0.008735                                        LR 0.000125    Time 0.144706    
Epoch: [187][   20/  207]    Overall Loss 0.009354    Objective Loss 0.009354                                        LR 0.000125    Time 0.109081    
Epoch: [187][   30/  207]    Overall Loss 0.009921    Objective Loss 0.009921                                        LR 0.000125    Time 0.098310    
Epoch: [187][   40/  207]    Overall Loss 0.010036    Objective Loss 0.010036                                        LR 0.000125    Time 0.093231    
Epoch: [187][   50/  207]    Overall Loss 0.010245    Objective Loss 0.010245                                        LR 0.000125    Time 0.089659    
Epoch: [187][   60/  207]    Overall Loss 0.009927    Objective Loss 0.009927                                        LR 0.000125    Time 0.088107    
Epoch: [187][   70/  207]    Overall Loss 0.009740    Objective Loss 0.009740                                        LR 0.000125    Time 0.086990    
Epoch: [187][   80/  207]    Overall Loss 0.009716    Objective Loss 0.009716                                        LR 0.000125    Time 0.085081    
Epoch: [187][   90/  207]    Overall Loss 0.009773    Objective Loss 0.009773                                        LR 0.000125    Time 0.083421    
Epoch: [187][  100/  207]    Overall Loss 0.009763    Objective Loss 0.009763                                        LR 0.000125    Time 0.082164    
Epoch: [187][  110/  207]    Overall Loss 0.009838    Objective Loss 0.009838                                        LR 0.000125    Time 0.081269    
Epoch: [187][  120/  207]    Overall Loss 0.009859    Objective Loss 0.009859                                        LR 0.000125    Time 0.080674    
Epoch: [187][  130/  207]    Overall Loss 0.009800    Objective Loss 0.009800                                        LR 0.000125    Time 0.080032    
Epoch: [187][  140/  207]    Overall Loss 0.009640    Objective Loss 0.009640                                        LR 0.000125    Time 0.079487    
Epoch: [187][  150/  207]    Overall Loss 0.009507    Objective Loss 0.009507                                        LR 0.000125    Time 0.078980    
Epoch: [187][  160/  207]    Overall Loss 0.009407    Objective Loss 0.009407                                        LR 0.000125    Time 0.078542    
Epoch: [187][  170/  207]    Overall Loss 0.009372    Objective Loss 0.009372                                        LR 0.000125    Time 0.078357    
Epoch: [187][  180/  207]    Overall Loss 0.009361    Objective Loss 0.009361                                        LR 0.000125    Time 0.078352    
Epoch: [187][  190/  207]    Overall Loss 0.009317    Objective Loss 0.009317                                        LR 0.000125    Time 0.078377    
Epoch: [187][  200/  207]    Overall Loss 0.009301    Objective Loss 0.009301                                        LR 0.000125    Time 0.078320    
Epoch: [187][  207/  207]    Overall Loss 0.009328    Objective Loss 0.009328    Top1 98.980747    Top5 100.000000    LR 0.000125    Time 0.077972    
--- validate (epoch=187)-----------
5136 samples (512 per mini-batch)
Epoch: [187][   10/   11]    Loss 0.634373    Top1 85.703125    Top5 99.628906    
Epoch: [187][   11/   11]    Loss 0.585501    Top1 85.689252    Top5 99.630062    
==> Top1: 85.689    Top5: 99.630    Loss: 0.586

==> Confusion:
[[273   2   3   3   2   0   2  15]
 [  1 264  18   0   0   2   1  14]
 [  2  20 266   1   0   3   0   8]
 [  0   2   0 747  45   5   8  30]
 [  2   0   0  40 772   4  18  43]
 [  3   7   9   9  20 763  15  68]
 [  1   0   0   5  29   8 763  31]
 [ 15  12  11  41  67  70  20 553]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [188][   10/  207]    Overall Loss 0.008260    Objective Loss 0.008260                                        LR 0.000125    Time 0.142083    
Epoch: [188][   20/  207]    Overall Loss 0.008358    Objective Loss 0.008358                                        LR 0.000125    Time 0.107475    
Epoch: [188][   30/  207]    Overall Loss 0.008424    Objective Loss 0.008424                                        LR 0.000125    Time 0.095460    
Epoch: [188][   40/  207]    Overall Loss 0.008474    Objective Loss 0.008474                                        LR 0.000125    Time 0.090240    
Epoch: [188][   50/  207]    Overall Loss 0.008728    Objective Loss 0.008728                                        LR 0.000125    Time 0.086860    
Epoch: [188][   60/  207]    Overall Loss 0.008959    Objective Loss 0.008959                                        LR 0.000125    Time 0.084810    
Epoch: [188][   70/  207]    Overall Loss 0.009091    Objective Loss 0.009091                                        LR 0.000125    Time 0.082844    
Epoch: [188][   80/  207]    Overall Loss 0.009138    Objective Loss 0.009138                                        LR 0.000125    Time 0.081547    
Epoch: [188][   90/  207]    Overall Loss 0.009204    Objective Loss 0.009204                                        LR 0.000125    Time 0.080943    
Epoch: [188][  100/  207]    Overall Loss 0.009209    Objective Loss 0.009209                                        LR 0.000125    Time 0.080461    
Epoch: [188][  110/  207]    Overall Loss 0.009157    Objective Loss 0.009157                                        LR 0.000125    Time 0.079764    
Epoch: [188][  120/  207]    Overall Loss 0.009179    Objective Loss 0.009179                                        LR 0.000125    Time 0.079318    
Epoch: [188][  130/  207]    Overall Loss 0.009320    Objective Loss 0.009320                                        LR 0.000125    Time 0.078770    
Epoch: [188][  140/  207]    Overall Loss 0.009377    Objective Loss 0.009377                                        LR 0.000125    Time 0.078339    
Epoch: [188][  150/  207]    Overall Loss 0.009313    Objective Loss 0.009313                                        LR 0.000125    Time 0.078241    
Epoch: [188][  160/  207]    Overall Loss 0.009281    Objective Loss 0.009281                                        LR 0.000125    Time 0.078526    
Epoch: [188][  170/  207]    Overall Loss 0.009306    Objective Loss 0.009306                                        LR 0.000125    Time 0.078858    
Epoch: [188][  180/  207]    Overall Loss 0.009325    Objective Loss 0.009325                                        LR 0.000125    Time 0.078480    
Epoch: [188][  190/  207]    Overall Loss 0.009311    Objective Loss 0.009311                                        LR 0.000125    Time 0.078214    
Epoch: [188][  200/  207]    Overall Loss 0.009310    Objective Loss 0.009310                                        LR 0.000125    Time 0.077910    
Epoch: [188][  207/  207]    Overall Loss 0.009351    Objective Loss 0.009351    Top1 99.093998    Top5 100.000000    LR 0.000125    Time 0.077577    
--- validate (epoch=188)-----------
5136 samples (512 per mini-batch)
Epoch: [188][   10/   11]    Loss 0.622995    Top1 85.722656    Top5 99.687500    
Epoch: [188][   11/   11]    Loss 0.566632    Top1 85.767134    Top5 99.688474    
==> Top1: 85.767    Top5: 99.688    Loss: 0.567

==> Confusion:
[[271   1   4   4   2   0   2  16]
 [  1 256  26   0   0   4   0  13]
 [  2  18 268   1   0   3   0   8]
 [  0   0   0 760  42   7   6  22]
 [  1   0   0  50 768   3  18  39]
 [  3   4   8   7  19 786  10  57]
 [  1   0   0   3  29  12 759  33]
 [ 13  11  12  51  69  78  18 537]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [189][   10/  207]    Overall Loss 0.007335    Objective Loss 0.007335                                        LR 0.000125    Time 0.136223    
Epoch: [189][   20/  207]    Overall Loss 0.008169    Objective Loss 0.008169                                        LR 0.000125    Time 0.103879    
Epoch: [189][   30/  207]    Overall Loss 0.008484    Objective Loss 0.008484                                        LR 0.000125    Time 0.093042    
Epoch: [189][   40/  207]    Overall Loss 0.008410    Objective Loss 0.008410                                        LR 0.000125    Time 0.088370    
Epoch: [189][   50/  207]    Overall Loss 0.008516    Objective Loss 0.008516                                        LR 0.000125    Time 0.085270    
Epoch: [189][   60/  207]    Overall Loss 0.008685    Objective Loss 0.008685                                        LR 0.000125    Time 0.083585    
Epoch: [189][   70/  207]    Overall Loss 0.008807    Objective Loss 0.008807                                        LR 0.000125    Time 0.081712    
Epoch: [189][   80/  207]    Overall Loss 0.008789    Objective Loss 0.008789                                        LR 0.000125    Time 0.080514    
Epoch: [189][   90/  207]    Overall Loss 0.008949    Objective Loss 0.008949                                        LR 0.000125    Time 0.079420    
Epoch: [189][  100/  207]    Overall Loss 0.008969    Objective Loss 0.008969                                        LR 0.000125    Time 0.078523    
Epoch: [189][  110/  207]    Overall Loss 0.009049    Objective Loss 0.009049                                        LR 0.000125    Time 0.077859    
Epoch: [189][  120/  207]    Overall Loss 0.009236    Objective Loss 0.009236                                        LR 0.000125    Time 0.077384    
Epoch: [189][  130/  207]    Overall Loss 0.009234    Objective Loss 0.009234                                        LR 0.000125    Time 0.076880    
Epoch: [189][  140/  207]    Overall Loss 0.009178    Objective Loss 0.009178                                        LR 0.000125    Time 0.076598    
Epoch: [189][  150/  207]    Overall Loss 0.009158    Objective Loss 0.009158                                        LR 0.000125    Time 0.076225    
Epoch: [189][  160/  207]    Overall Loss 0.009168    Objective Loss 0.009168                                        LR 0.000125    Time 0.075973    
Epoch: [189][  170/  207]    Overall Loss 0.009169    Objective Loss 0.009169                                        LR 0.000125    Time 0.075816    
Epoch: [189][  180/  207]    Overall Loss 0.009218    Objective Loss 0.009218                                        LR 0.000125    Time 0.075590    
Epoch: [189][  190/  207]    Overall Loss 0.009254    Objective Loss 0.009254                                        LR 0.000125    Time 0.075479    
Epoch: [189][  200/  207]    Overall Loss 0.009283    Objective Loss 0.009283                                        LR 0.000125    Time 0.075378    
Epoch: [189][  207/  207]    Overall Loss 0.009284    Objective Loss 0.009284    Top1 98.980747    Top5 100.000000    LR 0.000125    Time 0.075099    
--- validate (epoch=189)-----------
5136 samples (512 per mini-batch)
Epoch: [189][   10/   11]    Loss 0.660173    Top1 86.015625    Top5 99.687500    
Epoch: [189][   11/   11]    Loss 0.610880    Top1 86.020249    Top5 99.688474    
==> Top1: 86.020    Top5: 99.688    Loss: 0.611

==> Confusion:
[[275   1   4   3   2   0   2  13]
 [  2 265  17   0   0   2   0  14]
 [  2  21 263   1   0   3   0  10]
 [  0   0   0 743  49   6   9  30]
 [  1   0   0  30 785   3  16  44]
 [  4   4   8   7  19 770  12  70]
 [  1   0   0   3  34   8 751  40]
 [ 16  10  11  31  64  73  18 566]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [190][   10/  207]    Overall Loss 0.009258    Objective Loss 0.009258                                        LR 0.000063    Time 0.135101    
Epoch: [190][   20/  207]    Overall Loss 0.009160    Objective Loss 0.009160                                        LR 0.000063    Time 0.103966    
Epoch: [190][   30/  207]    Overall Loss 0.009540    Objective Loss 0.009540                                        LR 0.000063    Time 0.093118    
Epoch: [190][   40/  207]    Overall Loss 0.009154    Objective Loss 0.009154                                        LR 0.000063    Time 0.088020    
Epoch: [190][   50/  207]    Overall Loss 0.009021    Objective Loss 0.009021                                        LR 0.000063    Time 0.085307    
Epoch: [190][   60/  207]    Overall Loss 0.008750    Objective Loss 0.008750                                        LR 0.000063    Time 0.083449    
Epoch: [190][   70/  207]    Overall Loss 0.008479    Objective Loss 0.008479                                        LR 0.000063    Time 0.082549    
Epoch: [190][   80/  207]    Overall Loss 0.008618    Objective Loss 0.008618                                        LR 0.000063    Time 0.082179    
Epoch: [190][   90/  207]    Overall Loss 0.008739    Objective Loss 0.008739                                        LR 0.000063    Time 0.081607    
Epoch: [190][  100/  207]    Overall Loss 0.008712    Objective Loss 0.008712                                        LR 0.000063    Time 0.081008    
Epoch: [190][  110/  207]    Overall Loss 0.008603    Objective Loss 0.008603                                        LR 0.000063    Time 0.080384    
Epoch: [190][  120/  207]    Overall Loss 0.008606    Objective Loss 0.008606                                        LR 0.000063    Time 0.079696    
Epoch: [190][  130/  207]    Overall Loss 0.008705    Objective Loss 0.008705                                        LR 0.000063    Time 0.079070    
Epoch: [190][  140/  207]    Overall Loss 0.008626    Objective Loss 0.008626                                        LR 0.000063    Time 0.078626    
Epoch: [190][  150/  207]    Overall Loss 0.008573    Objective Loss 0.008573                                        LR 0.000063    Time 0.078609    
Epoch: [190][  160/  207]    Overall Loss 0.008655    Objective Loss 0.008655                                        LR 0.000063    Time 0.078197    
Epoch: [190][  170/  207]    Overall Loss 0.008609    Objective Loss 0.008609                                        LR 0.000063    Time 0.077877    
Epoch: [190][  180/  207]    Overall Loss 0.008597    Objective Loss 0.008597                                        LR 0.000063    Time 0.077699    
Epoch: [190][  190/  207]    Overall Loss 0.008548    Objective Loss 0.008548                                        LR 0.000063    Time 0.077514    
Epoch: [190][  200/  207]    Overall Loss 0.008565    Objective Loss 0.008565                                        LR 0.000063    Time 0.077251    
Epoch: [190][  207/  207]    Overall Loss 0.008595    Objective Loss 0.008595    Top1 98.754247    Top5 99.886750    LR 0.000063    Time 0.076913    
--- validate (epoch=190)-----------
5136 samples (512 per mini-batch)
Epoch: [190][   10/   11]    Loss 0.596948    Top1 85.800781    Top5 99.707031    
Epoch: [190][   11/   11]    Loss 0.559530    Top1 85.825545    Top5 99.707944    
==> Top1: 85.826    Top5: 99.708    Loss: 0.560

==> Confusion:
[[277   2   3   3   1   0   2  12]
 [  1 265  20   0   0   2   0  12]
 [  2  22 267   1   0   3   0   5]
 [  0   1   0 757  40   7   7  25]
 [  2   0   0  41 771   5  20  40]
 [  5   6   9   8  19 781  14  52]
 [  1   0   0   6  27  12 759  32]
 [ 19  14  14  51  62  79  19 531]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [191][   10/  207]    Overall Loss 0.006982    Objective Loss 0.006982                                        LR 0.000063    Time 0.128003    
Epoch: [191][   20/  207]    Overall Loss 0.007706    Objective Loss 0.007706                                        LR 0.000063    Time 0.099117    
Epoch: [191][   30/  207]    Overall Loss 0.007975    Objective Loss 0.007975                                        LR 0.000063    Time 0.089984    
Epoch: [191][   40/  207]    Overall Loss 0.008099    Objective Loss 0.008099                                        LR 0.000063    Time 0.085326    
Epoch: [191][   50/  207]    Overall Loss 0.008078    Objective Loss 0.008078                                        LR 0.000063    Time 0.082465    
Epoch: [191][   60/  207]    Overall Loss 0.008015    Objective Loss 0.008015                                        LR 0.000063    Time 0.080785    
Epoch: [191][   70/  207]    Overall Loss 0.008085    Objective Loss 0.008085                                        LR 0.000063    Time 0.079177    
Epoch: [191][   80/  207]    Overall Loss 0.008225    Objective Loss 0.008225                                        LR 0.000063    Time 0.078484    
Epoch: [191][   90/  207]    Overall Loss 0.008279    Objective Loss 0.008279                                        LR 0.000063    Time 0.077716    
Epoch: [191][  100/  207]    Overall Loss 0.008254    Objective Loss 0.008254                                        LR 0.000063    Time 0.077155    
Epoch: [191][  110/  207]    Overall Loss 0.008232    Objective Loss 0.008232                                        LR 0.000063    Time 0.076495    
Epoch: [191][  120/  207]    Overall Loss 0.008243    Objective Loss 0.008243                                        LR 0.000063    Time 0.076264    
Epoch: [191][  130/  207]    Overall Loss 0.008322    Objective Loss 0.008322                                        LR 0.000063    Time 0.075782    
Epoch: [191][  140/  207]    Overall Loss 0.008361    Objective Loss 0.008361                                        LR 0.000063    Time 0.075475    
Epoch: [191][  150/  207]    Overall Loss 0.008423    Objective Loss 0.008423                                        LR 0.000063    Time 0.075215    
Epoch: [191][  160/  207]    Overall Loss 0.008362    Objective Loss 0.008362                                        LR 0.000063    Time 0.075782    
Epoch: [191][  170/  207]    Overall Loss 0.008423    Objective Loss 0.008423                                        LR 0.000063    Time 0.075594    
Epoch: [191][  180/  207]    Overall Loss 0.008420    Objective Loss 0.008420                                        LR 0.000063    Time 0.075341    
Epoch: [191][  190/  207]    Overall Loss 0.008529    Objective Loss 0.008529                                        LR 0.000063    Time 0.075285    
Epoch: [191][  200/  207]    Overall Loss 0.008535    Objective Loss 0.008535                                        LR 0.000063    Time 0.075018    
Epoch: [191][  207/  207]    Overall Loss 0.008507    Objective Loss 0.008507    Top1 99.207248    Top5 100.000000    LR 0.000063    Time 0.074786    
--- validate (epoch=191)-----------
5136 samples (512 per mini-batch)
Epoch: [191][   10/   11]    Loss 0.627147    Top1 85.820312    Top5 99.667969    
Epoch: [191][   11/   11]    Loss 0.585825    Top1 85.825545    Top5 99.669003    
==> Top1: 85.826    Top5: 99.669    Loss: 0.586

==> Confusion:
[[272   1   4   4   2   0   2  15]
 [  1 262  22   0   0   2   0  13]
 [  2  21 265   1   0   2   0   9]
 [  0   0   0 747  44   8   9  29]
 [  1   0   0  37 781   3  18  39]
 [  3   6   7   9  20 776  15  58]
 [  1   0   0   4  29   9 758  36]
 [ 17  12  10  39  67  76  21 547]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [192][   10/  207]    Overall Loss 0.007763    Objective Loss 0.007763                                        LR 0.000063    Time 0.137905    
Epoch: [192][   20/  207]    Overall Loss 0.008582    Objective Loss 0.008582                                        LR 0.000063    Time 0.103658    
Epoch: [192][   30/  207]    Overall Loss 0.008067    Objective Loss 0.008067                                        LR 0.000063    Time 0.092873    
Epoch: [192][   40/  207]    Overall Loss 0.008235    Objective Loss 0.008235                                        LR 0.000063    Time 0.087272    
Epoch: [192][   50/  207]    Overall Loss 0.008136    Objective Loss 0.008136                                        LR 0.000063    Time 0.084274    
Epoch: [192][   60/  207]    Overall Loss 0.008208    Objective Loss 0.008208                                        LR 0.000063    Time 0.082406    
Epoch: [192][   70/  207]    Overall Loss 0.008058    Objective Loss 0.008058                                        LR 0.000063    Time 0.082587    
Epoch: [192][   80/  207]    Overall Loss 0.008163    Objective Loss 0.008163                                        LR 0.000063    Time 0.081899    
Epoch: [192][   90/  207]    Overall Loss 0.008214    Objective Loss 0.008214                                        LR 0.000063    Time 0.081865    
Epoch: [192][  100/  207]    Overall Loss 0.008199    Objective Loss 0.008199                                        LR 0.000063    Time 0.081151    
Epoch: [192][  110/  207]    Overall Loss 0.008162    Objective Loss 0.008162                                        LR 0.000063    Time 0.080483    
Epoch: [192][  120/  207]    Overall Loss 0.008160    Objective Loss 0.008160                                        LR 0.000063    Time 0.080310    
Epoch: [192][  130/  207]    Overall Loss 0.008199    Objective Loss 0.008199                                        LR 0.000063    Time 0.080581    
Epoch: [192][  140/  207]    Overall Loss 0.008220    Objective Loss 0.008220                                        LR 0.000063    Time 0.080418    
Epoch: [192][  150/  207]    Overall Loss 0.008373    Objective Loss 0.008373                                        LR 0.000063    Time 0.080371    
Epoch: [192][  160/  207]    Overall Loss 0.008430    Objective Loss 0.008430                                        LR 0.000063    Time 0.079720    
Epoch: [192][  170/  207]    Overall Loss 0.008467    Objective Loss 0.008467                                        LR 0.000063    Time 0.079182    
Epoch: [192][  180/  207]    Overall Loss 0.008440    Objective Loss 0.008440                                        LR 0.000063    Time 0.078705    
Epoch: [192][  190/  207]    Overall Loss 0.008381    Objective Loss 0.008381                                        LR 0.000063    Time 0.078287    
Epoch: [192][  200/  207]    Overall Loss 0.008421    Objective Loss 0.008421                                        LR 0.000063    Time 0.077977    
Epoch: [192][  207/  207]    Overall Loss 0.008451    Objective Loss 0.008451    Top1 99.433749    Top5 100.000000    LR 0.000063    Time 0.077716    
--- validate (epoch=192)-----------
5136 samples (512 per mini-batch)
Epoch: [192][   10/   11]    Loss 0.652878    Top1 85.683594    Top5 99.687500    
Epoch: [192][   11/   11]    Loss 0.632118    Top1 85.689252    Top5 99.688474    
==> Top1: 85.689    Top5: 99.688    Loss: 0.632

==> Confusion:
[[271   0   4   4   1   0   2  18]
 [  1 262  19   0   0   1   1  16]
 [  1  21 265   1   0   1   0  11]
 [  0   1   0 748  45   6   6  31]
 [  1   0   0  38 775   6  18  41]
 [  4   7   9   9  19 764  13  69]
 [  1   0   0   4  31   9 754  38]
 [ 14  10  12  41  61  69  20 562]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [193][   10/  207]    Overall Loss 0.007422    Objective Loss 0.007422                                        LR 0.000063    Time 0.130933    
Epoch: [193][   20/  207]    Overall Loss 0.008141    Objective Loss 0.008141                                        LR 0.000063    Time 0.101104    
Epoch: [193][   30/  207]    Overall Loss 0.008058    Objective Loss 0.008058                                        LR 0.000063    Time 0.091909    
Epoch: [193][   40/  207]    Overall Loss 0.008112    Objective Loss 0.008112                                        LR 0.000063    Time 0.087473    
Epoch: [193][   50/  207]    Overall Loss 0.008410    Objective Loss 0.008410                                        LR 0.000063    Time 0.084573    
Epoch: [193][   60/  207]    Overall Loss 0.008627    Objective Loss 0.008627                                        LR 0.000063    Time 0.082769    
Epoch: [193][   70/  207]    Overall Loss 0.008734    Objective Loss 0.008734                                        LR 0.000063    Time 0.081525    
Epoch: [193][   80/  207]    Overall Loss 0.008669    Objective Loss 0.008669                                        LR 0.000063    Time 0.080586    
Epoch: [193][   90/  207]    Overall Loss 0.008665    Objective Loss 0.008665                                        LR 0.000063    Time 0.080020    
Epoch: [193][  100/  207]    Overall Loss 0.008648    Objective Loss 0.008648                                        LR 0.000063    Time 0.079503    
Epoch: [193][  110/  207]    Overall Loss 0.008596    Objective Loss 0.008596                                        LR 0.000063    Time 0.078852    
Epoch: [193][  120/  207]    Overall Loss 0.008533    Objective Loss 0.008533                                        LR 0.000063    Time 0.078239    
Epoch: [193][  130/  207]    Overall Loss 0.008544    Objective Loss 0.008544                                        LR 0.000063    Time 0.077735    
Epoch: [193][  140/  207]    Overall Loss 0.008504    Objective Loss 0.008504                                        LR 0.000063    Time 0.077289    
Epoch: [193][  150/  207]    Overall Loss 0.008501    Objective Loss 0.008501                                        LR 0.000063    Time 0.077024    
Epoch: [193][  160/  207]    Overall Loss 0.008542    Objective Loss 0.008542                                        LR 0.000063    Time 0.076888    
Epoch: [193][  170/  207]    Overall Loss 0.008542    Objective Loss 0.008542                                        LR 0.000063    Time 0.076541    
Epoch: [193][  180/  207]    Overall Loss 0.008524    Objective Loss 0.008524                                        LR 0.000063    Time 0.076204    
Epoch: [193][  190/  207]    Overall Loss 0.008539    Objective Loss 0.008539                                        LR 0.000063    Time 0.076060    
Epoch: [193][  200/  207]    Overall Loss 0.008578    Objective Loss 0.008578                                        LR 0.000063    Time 0.076758    
Epoch: [193][  207/  207]    Overall Loss 0.008537    Objective Loss 0.008537    Top1 99.773499    Top5 100.000000    LR 0.000063    Time 0.076434    
--- validate (epoch=193)-----------
5136 samples (512 per mini-batch)
Epoch: [193][   10/   11]    Loss 0.629303    Top1 85.800781    Top5 99.746094    
Epoch: [193][   11/   11]    Loss 0.646303    Top1 85.786604    Top5 99.746885    
==> Top1: 85.787    Top5: 99.747    Loss: 0.646

==> Confusion:
[[274   2   3   4   0   0   2  15]
 [  2 258  22   0   0   4   0  14]
 [  2  20 264   0   0   4   0  10]
 [  0   1   0 741  45   8   7  35]
 [  2   0   0  33 770   5  24  45]
 [  2   5   8   6  16 788  12  57]
 [  1   0   0   3  26  12 756  39]
 [ 16  11  11  38  57  81  20 555]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [194][   10/  207]    Overall Loss 0.007963    Objective Loss 0.007963                                        LR 0.000063    Time 0.132475    
Epoch: [194][   20/  207]    Overall Loss 0.008134    Objective Loss 0.008134                                        LR 0.000063    Time 0.102371    
Epoch: [194][   30/  207]    Overall Loss 0.009084    Objective Loss 0.009084                                        LR 0.000063    Time 0.092649    
Epoch: [194][   40/  207]    Overall Loss 0.008349    Objective Loss 0.008349                                        LR 0.000063    Time 0.087385    
Epoch: [194][   50/  207]    Overall Loss 0.008315    Objective Loss 0.008315                                        LR 0.000063    Time 0.084679    
Epoch: [194][   60/  207]    Overall Loss 0.008210    Objective Loss 0.008210                                        LR 0.000063    Time 0.083140    
Epoch: [194][   70/  207]    Overall Loss 0.008231    Objective Loss 0.008231                                        LR 0.000063    Time 0.081683    
Epoch: [194][   80/  207]    Overall Loss 0.008372    Objective Loss 0.008372                                        LR 0.000063    Time 0.080415    
Epoch: [194][   90/  207]    Overall Loss 0.008310    Objective Loss 0.008310                                        LR 0.000063    Time 0.079541    
Epoch: [194][  100/  207]    Overall Loss 0.008378    Objective Loss 0.008378                                        LR 0.000063    Time 0.078667    
Epoch: [194][  110/  207]    Overall Loss 0.008431    Objective Loss 0.008431                                        LR 0.000063    Time 0.078030    
Epoch: [194][  120/  207]    Overall Loss 0.008482    Objective Loss 0.008482                                        LR 0.000063    Time 0.077937    
Epoch: [194][  130/  207]    Overall Loss 0.008508    Objective Loss 0.008508                                        LR 0.000063    Time 0.077381    
Epoch: [194][  140/  207]    Overall Loss 0.008476    Objective Loss 0.008476                                        LR 0.000063    Time 0.076956    
Epoch: [194][  150/  207]    Overall Loss 0.008428    Objective Loss 0.008428                                        LR 0.000063    Time 0.076631    
Epoch: [194][  160/  207]    Overall Loss 0.008433    Objective Loss 0.008433                                        LR 0.000063    Time 0.076266    
Epoch: [194][  170/  207]    Overall Loss 0.008514    Objective Loss 0.008514                                        LR 0.000063    Time 0.075975    
Epoch: [194][  180/  207]    Overall Loss 0.008543    Objective Loss 0.008543                                        LR 0.000063    Time 0.075814    
Epoch: [194][  190/  207]    Overall Loss 0.008559    Objective Loss 0.008559                                        LR 0.000063    Time 0.075498    
Epoch: [194][  200/  207]    Overall Loss 0.008531    Objective Loss 0.008531                                        LR 0.000063    Time 0.075266    
Epoch: [194][  207/  207]    Overall Loss 0.008550    Objective Loss 0.008550    Top1 99.320498    Top5 100.000000    LR 0.000063    Time 0.075025    
--- validate (epoch=194)-----------
5136 samples (512 per mini-batch)
Epoch: [194][   10/   11]    Loss 0.638507    Top1 85.761719    Top5 99.648438    
Epoch: [194][   11/   11]    Loss 0.715298    Top1 85.747664    Top5 99.649533    
==> Top1: 85.748    Top5: 99.650    Loss: 0.715

==> Confusion:
[[271   2   4   4   2   0   1  16]
 [  1 262  20   0   0   2   1  14]
 [  2  22 261   0   0   3   0  12]
 [  0   0   0 752  43   7   6  29]
 [  2   0   0  35 777   3  18  44]
 [  3   6   8   8  19 762  13  75]
 [  1   0   0   5  28   9 758  36]
 [ 15   9  11  42  65  68  18 561]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [195][   10/  207]    Overall Loss 0.005961    Objective Loss 0.005961                                        LR 0.000031    Time 0.110797    
Epoch: [195][   20/  207]    Overall Loss 0.006966    Objective Loss 0.006966                                        LR 0.000031    Time 0.091373    
Epoch: [195][   30/  207]    Overall Loss 0.007249    Objective Loss 0.007249                                        LR 0.000031    Time 0.084540    
Epoch: [195][   40/  207]    Overall Loss 0.007165    Objective Loss 0.007165                                        LR 0.000031    Time 0.080864    
Epoch: [195][   50/  207]    Overall Loss 0.007458    Objective Loss 0.007458                                        LR 0.000031    Time 0.079153    
Epoch: [195][   60/  207]    Overall Loss 0.007687    Objective Loss 0.007687                                        LR 0.000031    Time 0.077854    
Epoch: [195][   70/  207]    Overall Loss 0.007800    Objective Loss 0.007800                                        LR 0.000031    Time 0.076741    
Epoch: [195][   80/  207]    Overall Loss 0.007815    Objective Loss 0.007815                                        LR 0.000031    Time 0.076371    
Epoch: [195][   90/  207]    Overall Loss 0.007819    Objective Loss 0.007819                                        LR 0.000031    Time 0.075811    
Epoch: [195][  100/  207]    Overall Loss 0.007773    Objective Loss 0.007773                                        LR 0.000031    Time 0.075401    
Epoch: [195][  110/  207]    Overall Loss 0.007860    Objective Loss 0.007860                                        LR 0.000031    Time 0.075036    
Epoch: [195][  120/  207]    Overall Loss 0.007877    Objective Loss 0.007877                                        LR 0.000031    Time 0.074684    
Epoch: [195][  130/  207]    Overall Loss 0.007877    Objective Loss 0.007877                                        LR 0.000031    Time 0.074542    
Epoch: [195][  140/  207]    Overall Loss 0.007919    Objective Loss 0.007919                                        LR 0.000031    Time 0.074286    
Epoch: [195][  150/  207]    Overall Loss 0.008027    Objective Loss 0.008027                                        LR 0.000031    Time 0.074375    
Epoch: [195][  160/  207]    Overall Loss 0.007998    Objective Loss 0.007998                                        LR 0.000031    Time 0.074160    
Epoch: [195][  170/  207]    Overall Loss 0.007990    Objective Loss 0.007990                                        LR 0.000031    Time 0.073995    
Epoch: [195][  180/  207]    Overall Loss 0.008040    Objective Loss 0.008040                                        LR 0.000031    Time 0.073861    
Epoch: [195][  190/  207]    Overall Loss 0.008173    Objective Loss 0.008173                                        LR 0.000031    Time 0.073824    
Epoch: [195][  200/  207]    Overall Loss 0.008145    Objective Loss 0.008145                                        LR 0.000031    Time 0.073858    
Epoch: [195][  207/  207]    Overall Loss 0.008115    Objective Loss 0.008115    Top1 99.207248    Top5 100.000000    LR 0.000031    Time 0.073627    
--- validate (epoch=195)-----------
5136 samples (512 per mini-batch)
Epoch: [195][   10/   11]    Loss 0.606477    Top1 86.015625    Top5 99.726562    
Epoch: [195][   11/   11]    Loss 0.558245    Top1 86.020249    Top5 99.727414    
==> Top1: 86.020    Top5: 99.727    Loss: 0.558

==> Confusion:
[[276   1   4   3   1   0   2  13]
 [  3 261  21   0   0   3   0  12]
 [  2  20 269   0   0   3   0   6]
 [  0   2   0 749  42   9   8  27]
 [  2   0   0  38 774   3  21  41]
 [  4   6   7   6  18 785  13  55]
 [  1   0   0   6  26  12 763  29]
 [ 19  12  14  36  66  78  23 541]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [196][   10/  207]    Overall Loss 0.006807    Objective Loss 0.006807                                        LR 0.000031    Time 0.130609    
Epoch: [196][   20/  207]    Overall Loss 0.007863    Objective Loss 0.007863                                        LR 0.000031    Time 0.100814    
Epoch: [196][   30/  207]    Overall Loss 0.007871    Objective Loss 0.007871                                        LR 0.000031    Time 0.090991    
Epoch: [196][   40/  207]    Overall Loss 0.007630    Objective Loss 0.007630                                        LR 0.000031    Time 0.086592    
Epoch: [196][   50/  207]    Overall Loss 0.007901    Objective Loss 0.007901                                        LR 0.000031    Time 0.083724    
Epoch: [196][   60/  207]    Overall Loss 0.007908    Objective Loss 0.007908                                        LR 0.000031    Time 0.082408    
Epoch: [196][   70/  207]    Overall Loss 0.007958    Objective Loss 0.007958                                        LR 0.000031    Time 0.080853    
Epoch: [196][   80/  207]    Overall Loss 0.008249    Objective Loss 0.008249                                        LR 0.000031    Time 0.079915    
Epoch: [196][   90/  207]    Overall Loss 0.008224    Objective Loss 0.008224                                        LR 0.000031    Time 0.079372    
Epoch: [196][  100/  207]    Overall Loss 0.008117    Objective Loss 0.008117                                        LR 0.000031    Time 0.079105    
Epoch: [196][  110/  207]    Overall Loss 0.008056    Objective Loss 0.008056                                        LR 0.000031    Time 0.078735    
Epoch: [196][  120/  207]    Overall Loss 0.008082    Objective Loss 0.008082                                        LR 0.000031    Time 0.078436    
Epoch: [196][  130/  207]    Overall Loss 0.008190    Objective Loss 0.008190                                        LR 0.000031    Time 0.078121    
Epoch: [196][  140/  207]    Overall Loss 0.008178    Objective Loss 0.008178                                        LR 0.000031    Time 0.077778    
Epoch: [196][  150/  207]    Overall Loss 0.008117    Objective Loss 0.008117                                        LR 0.000031    Time 0.077412    
Epoch: [196][  160/  207]    Overall Loss 0.008206    Objective Loss 0.008206                                        LR 0.000031    Time 0.077043    
Epoch: [196][  170/  207]    Overall Loss 0.008188    Objective Loss 0.008188                                        LR 0.000031    Time 0.076908    
Epoch: [196][  180/  207]    Overall Loss 0.008147    Objective Loss 0.008147                                        LR 0.000031    Time 0.076800    
Epoch: [196][  190/  207]    Overall Loss 0.008101    Objective Loss 0.008101                                        LR 0.000031    Time 0.076694    
Epoch: [196][  200/  207]    Overall Loss 0.008089    Objective Loss 0.008089                                        LR 0.000031    Time 0.076656    
Epoch: [196][  207/  207]    Overall Loss 0.008106    Objective Loss 0.008106    Top1 99.093998    Top5 100.000000    LR 0.000031    Time 0.076367    
--- validate (epoch=196)-----------
5136 samples (512 per mini-batch)
Epoch: [196][   10/   11]    Loss 0.623402    Top1 85.878906    Top5 99.726562    
Epoch: [196][   11/   11]    Loss 0.581673    Top1 85.903427    Top5 99.727414    
==> Top1: 85.903    Top5: 99.727    Loss: 0.582

==> Confusion:
[[272   2   3   4   1   0   2  16]
 [  2 262  19   0   0   2   0  15]
 [  2  22 261   1   0   3   0  11]
 [  0   1   0 745  45   8   9  29]
 [  2   0   0  35 777   4  20  41]
 [  2   5   7   8  18 786  13  55]
 [  1   0   0   4  26  11 760  35]
 [ 16  11  11  39  64  79  20 549]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [197][   10/  207]    Overall Loss 0.008382    Objective Loss 0.008382                                        LR 0.000031    Time 0.129693    
Epoch: [197][   20/  207]    Overall Loss 0.008518    Objective Loss 0.008518                                        LR 0.000031    Time 0.100644    
Epoch: [197][   30/  207]    Overall Loss 0.008572    Objective Loss 0.008572                                        LR 0.000031    Time 0.090729    
Epoch: [197][   40/  207]    Overall Loss 0.008440    Objective Loss 0.008440                                        LR 0.000031    Time 0.085568    
Epoch: [197][   50/  207]    Overall Loss 0.008188    Objective Loss 0.008188                                        LR 0.000031    Time 0.083074    
Epoch: [197][   60/  207]    Overall Loss 0.007950    Objective Loss 0.007950                                        LR 0.000031    Time 0.081132    
Epoch: [197][   70/  207]    Overall Loss 0.008064    Objective Loss 0.008064                                        LR 0.000031    Time 0.079720    
Epoch: [197][   80/  207]    Overall Loss 0.008022    Objective Loss 0.008022                                        LR 0.000031    Time 0.079138    
Epoch: [197][   90/  207]    Overall Loss 0.008105    Objective Loss 0.008105                                        LR 0.000031    Time 0.078685    
Epoch: [197][  100/  207]    Overall Loss 0.008095    Objective Loss 0.008095                                        LR 0.000031    Time 0.078295    
Epoch: [197][  110/  207]    Overall Loss 0.008081    Objective Loss 0.008081                                        LR 0.000031    Time 0.078209    
Epoch: [197][  120/  207]    Overall Loss 0.008072    Objective Loss 0.008072                                        LR 0.000031    Time 0.077654    
Epoch: [197][  130/  207]    Overall Loss 0.008175    Objective Loss 0.008175                                        LR 0.000031    Time 0.077241    
Epoch: [197][  140/  207]    Overall Loss 0.008047    Objective Loss 0.008047                                        LR 0.000031    Time 0.076886    
Epoch: [197][  150/  207]    Overall Loss 0.008027    Objective Loss 0.008027                                        LR 0.000031    Time 0.076877    
Epoch: [197][  160/  207]    Overall Loss 0.008128    Objective Loss 0.008128                                        LR 0.000031    Time 0.076674    
Epoch: [197][  170/  207]    Overall Loss 0.008073    Objective Loss 0.008073                                        LR 0.000031    Time 0.076329    
Epoch: [197][  180/  207]    Overall Loss 0.008077    Objective Loss 0.008077                                        LR 0.000031    Time 0.076213    
Epoch: [197][  190/  207]    Overall Loss 0.008098    Objective Loss 0.008098                                        LR 0.000031    Time 0.075952    
Epoch: [197][  200/  207]    Overall Loss 0.008093    Objective Loss 0.008093                                        LR 0.000031    Time 0.075701    
Epoch: [197][  207/  207]    Overall Loss 0.008113    Objective Loss 0.008113    Top1 99.433749    Top5 100.000000    LR 0.000031    Time 0.075429    
--- validate (epoch=197)-----------
5136 samples (512 per mini-batch)
Epoch: [197][   10/   11]    Loss 0.667933    Top1 85.839844    Top5 99.648438    
Epoch: [197][   11/   11]    Loss 0.616872    Top1 85.845016    Top5 99.649533    
==> Top1: 85.845    Top5: 99.650    Loss: 0.617

==> Confusion:
[[273   2   3   4   2   0   2  14]
 [  2 256  23   0   0   1   1  17]
 [  2  20 262   0   0   3   0  13]
 [  0   0   0 747  45   6   7  32]
 [  2   0   0  38 770   5  20  44]
 [  3   6   7   6  21 769  12  70]
 [  1   0   0   4  27   8 760  37]
 [ 13  10  11  36  57  68  22 572]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [198][   10/  207]    Overall Loss 0.009677    Objective Loss 0.009677                                        LR 0.000031    Time 0.131927    
Epoch: [198][   20/  207]    Overall Loss 0.008018    Objective Loss 0.008018                                        LR 0.000031    Time 0.101954    
Epoch: [198][   30/  207]    Overall Loss 0.008134    Objective Loss 0.008134                                        LR 0.000031    Time 0.092974    
Epoch: [198][   40/  207]    Overall Loss 0.007941    Objective Loss 0.007941                                        LR 0.000031    Time 0.087223    
Epoch: [198][   50/  207]    Overall Loss 0.007875    Objective Loss 0.007875                                        LR 0.000031    Time 0.085223    
Epoch: [198][   60/  207]    Overall Loss 0.007710    Objective Loss 0.007710                                        LR 0.000031    Time 0.083514    
Epoch: [198][   70/  207]    Overall Loss 0.007957    Objective Loss 0.007957                                        LR 0.000031    Time 0.081930    
Epoch: [198][   80/  207]    Overall Loss 0.008046    Objective Loss 0.008046                                        LR 0.000031    Time 0.081122    
Epoch: [198][   90/  207]    Overall Loss 0.008335    Objective Loss 0.008335                                        LR 0.000031    Time 0.080212    
Epoch: [198][  100/  207]    Overall Loss 0.008269    Objective Loss 0.008269                                        LR 0.000031    Time 0.079489    
Epoch: [198][  110/  207]    Overall Loss 0.008216    Objective Loss 0.008216                                        LR 0.000031    Time 0.079254    
Epoch: [198][  120/  207]    Overall Loss 0.008259    Objective Loss 0.008259                                        LR 0.000031    Time 0.078885    
Epoch: [198][  130/  207]    Overall Loss 0.008336    Objective Loss 0.008336                                        LR 0.000031    Time 0.078612    
Epoch: [198][  140/  207]    Overall Loss 0.008294    Objective Loss 0.008294                                        LR 0.000031    Time 0.078205    
Epoch: [198][  150/  207]    Overall Loss 0.008283    Objective Loss 0.008283                                        LR 0.000031    Time 0.077680    
Epoch: [198][  160/  207]    Overall Loss 0.008282    Objective Loss 0.008282                                        LR 0.000031    Time 0.077386    
Epoch: [198][  170/  207]    Overall Loss 0.008166    Objective Loss 0.008166                                        LR 0.000031    Time 0.077088    
Epoch: [198][  180/  207]    Overall Loss 0.008119    Objective Loss 0.008119                                        LR 0.000031    Time 0.077060    
Epoch: [198][  190/  207]    Overall Loss 0.008085    Objective Loss 0.008085                                        LR 0.000031    Time 0.076957    
Epoch: [198][  200/  207]    Overall Loss 0.008088    Objective Loss 0.008088                                        LR 0.000031    Time 0.076712    
Epoch: [198][  207/  207]    Overall Loss 0.008097    Objective Loss 0.008097    Top1 99.320498    Top5 100.000000    LR 0.000031    Time 0.076584    
--- validate (epoch=198)-----------
5136 samples (512 per mini-batch)
Epoch: [198][   10/   11]    Loss 0.616293    Top1 86.035156    Top5 99.707031    
Epoch: [198][   11/   11]    Loss 0.622669    Top1 86.020249    Top5 99.707944    
==> Top1: 86.020    Top5: 99.708    Loss: 0.623

==> Confusion:
[[273   1   4   4   2   0   2  14]
 [  3 259  22   0   0   3   0  13]
 [  2  20 266   1   0   3   0   8]
 [  0   2   0 750  44   6   7  28]
 [  1   0   0  31 781   5  20  41]
 [  7   6   7   6  17 771  16  64]
 [  1   0   0   6  27   7 766  30]
 [ 17  11  11  39  63  73  23 552]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [199][   10/  207]    Overall Loss 0.008039    Objective Loss 0.008039                                        LR 0.000031    Time 0.132300    
Epoch: [199][   20/  207]    Overall Loss 0.007388    Objective Loss 0.007388                                        LR 0.000031    Time 0.101576    
Epoch: [199][   30/  207]    Overall Loss 0.007745    Objective Loss 0.007745                                        LR 0.000031    Time 0.091695    
Epoch: [199][   40/  207]    Overall Loss 0.008255    Objective Loss 0.008255                                        LR 0.000031    Time 0.086487    
Epoch: [199][   50/  207]    Overall Loss 0.008145    Objective Loss 0.008145                                        LR 0.000031    Time 0.083522    
Epoch: [199][   60/  207]    Overall Loss 0.008364    Objective Loss 0.008364                                        LR 0.000031    Time 0.081257    
Epoch: [199][   70/  207]    Overall Loss 0.008265    Objective Loss 0.008265                                        LR 0.000031    Time 0.080150    
Epoch: [199][   80/  207]    Overall Loss 0.008270    Objective Loss 0.008270                                        LR 0.000031    Time 0.079429    
Epoch: [199][   90/  207]    Overall Loss 0.008131    Objective Loss 0.008131                                        LR 0.000031    Time 0.078597    
Epoch: [199][  100/  207]    Overall Loss 0.008109    Objective Loss 0.008109                                        LR 0.000031    Time 0.078464    
Epoch: [199][  110/  207]    Overall Loss 0.008069    Objective Loss 0.008069                                        LR 0.000031    Time 0.077938    
Epoch: [199][  120/  207]    Overall Loss 0.007984    Objective Loss 0.007984                                        LR 0.000031    Time 0.077361    
Epoch: [199][  130/  207]    Overall Loss 0.007988    Objective Loss 0.007988                                        LR 0.000031    Time 0.076925    
Epoch: [199][  140/  207]    Overall Loss 0.008052    Objective Loss 0.008052                                        LR 0.000031    Time 0.076478    
Epoch: [199][  150/  207]    Overall Loss 0.008013    Objective Loss 0.008013                                        LR 0.000031    Time 0.076239    
Epoch: [199][  160/  207]    Overall Loss 0.007943    Objective Loss 0.007943                                        LR 0.000031    Time 0.075911    
Epoch: [199][  170/  207]    Overall Loss 0.007907    Objective Loss 0.007907                                        LR 0.000031    Time 0.076218    
Epoch: [199][  180/  207]    Overall Loss 0.007991    Objective Loss 0.007991                                        LR 0.000031    Time 0.075916    
Epoch: [199][  190/  207]    Overall Loss 0.008052    Objective Loss 0.008052                                        LR 0.000031    Time 0.075864    
Epoch: [199][  200/  207]    Overall Loss 0.008038    Objective Loss 0.008038                                        LR 0.000031    Time 0.075729    
Epoch: [199][  207/  207]    Overall Loss 0.008086    Objective Loss 0.008086    Top1 98.980747    Top5 100.000000    LR 0.000031    Time 0.075501    
--- validate (epoch=199)-----------
5136 samples (512 per mini-batch)
Epoch: [199][   10/   11]    Loss 0.635764    Top1 85.898438    Top5 99.687500    
Epoch: [199][   11/   11]    Loss 0.580134    Top1 85.942368    Top5 99.688474    
==> Top1: 85.942    Top5: 99.688    Loss: 0.580

==> Confusion:
[[272   2   3   4   2   0   2  15]
 [  2 263  20   0   0   1   1  13]
 [  2  21 262   0   0   4   0  11]
 [  0   0   0 750  44   7   7  29]
 [  1   0   0  37 779   4  15  43]
 [  4   5   8   8  17 768  13  71]
 [  1   0   0   6  27   7 762  34]
 [ 14  12  13  45  58  69  20 558]]

==> Best [Top1: 86.157   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 185]
Saving checkpoint to: logs/2024.01.15-120305/qat_checkpoint.pth.tar
--- test ---------------------
5631 samples (512 per mini-batch)
Test: [   10/   11]    Loss 0.656917    Top1 85.527344    Top5 99.628906    
Test: [   11/   11]    Loss 0.648015    Top1 85.704138    Top5 99.662582    
==> Top1: 85.704    Top5: 99.663    Loss: 0.648

==> Confusion:
[[272   3   3   2   0   0   2  18]
 [  1 254  23   0   0   0   0  22]
 [  3  25 260   1   0   2   0   9]
 [  1   1   0 818  73  12   6  31]
 [  3   0   1  41 832  13  14  65]
 [  4   1  20   5   9 779  13  72]
 [  0   0   0   4  22  28 893  31]
 [ 17  16   9  38  33  79  29 718]]


Log file for this run: /mnt/c/Users/Daniel/Github/ai8x-training/logs/2024.01.15-120305/2024.01.15-120305.log
