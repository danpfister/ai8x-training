Configuring device: MAX78000, simulate=False.
Log file for this run: /mnt/c/Users/Daniel/Github/ai8x-training/logs/2024.01.15-145725/2024.01.15-145725.log
{'start_epoch': 10, 'weight_bits': 8}
Optimizer Type: <class 'torch.optim.adam.Adam'>
Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False}
No key `stretch` in input augmentation dictionary! Using defaults: [Min: 0.8, Max: 1.3]

Processing train...
train set: 105843 elements
validation set: 5136 elements
Class red (# 25): 2793 elements
Class green (# 13): 2697 elements
Class blue (# 3): 3054 elements
Class on (# 23): 8058 elements
Class off (# 22): 8031 elements
Class go (# 12): 8097 elements
Class stop (# 30): 8022 elements
Class UNKNOWN: 70227 elements

Processing test...
test set: 5631 elements
Class red (# 25): 300 elements
Class green (# 13): 300 elements
Class blue (# 3): 300 elements
Class on (# 23): 942 elements
Class off (# 22): 969 elements
Class go (# 12): 903 elements
Class stop (# 30): 978 elements
Class UNKNOWN: 939 elements
Dataset sizes:
	training=105843
	validation=5136
	test=5631
Reading compression schedule from: policies/schedule_kws20-90ep.yaml


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [0][   10/  207]    Overall Loss 2.062907    Objective Loss 2.062907                                        LR 0.001000    Time 0.344974    
Epoch: [0][   20/  207]    Overall Loss 2.015338    Objective Loss 2.015338                                        LR 0.001000    Time 0.200080    
Epoch: [0][   30/  207]    Overall Loss 1.979691    Objective Loss 1.979691                                        LR 0.001000    Time 0.151249    
Epoch: [0][   40/  207]    Overall Loss 1.955405    Objective Loss 1.955405                                        LR 0.001000    Time 0.127496    
Epoch: [0][   50/  207]    Overall Loss 1.928042    Objective Loss 1.928042                                        LR 0.001000    Time 0.113524    
Epoch: [0][   60/  207]    Overall Loss 1.905567    Objective Loss 1.905567                                        LR 0.001000    Time 0.103380    
Epoch: [0][   70/  207]    Overall Loss 1.881534    Objective Loss 1.881534                                        LR 0.001000    Time 0.096197    
Epoch: [0][   80/  207]    Overall Loss 1.836626    Objective Loss 1.836626                                        LR 0.001000    Time 0.090811    
Epoch: [0][   90/  207]    Overall Loss 1.798299    Objective Loss 1.798299                                        LR 0.001000    Time 0.086630    
Epoch: [0][  100/  207]    Overall Loss 1.764576    Objective Loss 1.764576                                        LR 0.001000    Time 0.083251    
Epoch: [0][  110/  207]    Overall Loss 1.735279    Objective Loss 1.735279                                        LR 0.001000    Time 0.080574    
Epoch: [0][  120/  207]    Overall Loss 1.705176    Objective Loss 1.705176                                        LR 0.001000    Time 0.078276    
Epoch: [0][  130/  207]    Overall Loss 1.677600    Objective Loss 1.677600                                        LR 0.001000    Time 0.076315    
Epoch: [0][  140/  207]    Overall Loss 1.652258    Objective Loss 1.652258                                        LR 0.001000    Time 0.074641    
Epoch: [0][  150/  207]    Overall Loss 1.629160    Objective Loss 1.629160                                        LR 0.001000    Time 0.073249    
Epoch: [0][  160/  207]    Overall Loss 1.605695    Objective Loss 1.605695                                        LR 0.001000    Time 0.071987    
Epoch: [0][  170/  207]    Overall Loss 1.584945    Objective Loss 1.584945                                        LR 0.001000    Time 0.070855    
Epoch: [0][  180/  207]    Overall Loss 1.562800    Objective Loss 1.562800                                        LR 0.001000    Time 0.069914    
Epoch: [0][  190/  207]    Overall Loss 1.543104    Objective Loss 1.543104                                        LR 0.001000    Time 0.069023    
Epoch: [0][  200/  207]    Overall Loss 1.523060    Objective Loss 1.523060                                        LR 0.001000    Time 0.068236    
Epoch: [0][  207/  207]    Overall Loss 1.510019    Objective Loss 1.510019    Top1 67.497169    Top5 95.356738    LR 0.001000    Time 0.067793    
--- validate (epoch=0)-----------
5136 samples (512 per mini-batch)
Epoch: [0][   10/   11]    Loss 1.149724    Top1 45.917969    Top5 91.933594    
Epoch: [0][   11/   11]    Loss 1.159708    Top1 45.872274    Top5 91.939252    
==> Top1: 45.872    Top5: 91.939    Loss: 1.160

==> Confusion:
[[207  22   6   4   3  31   4  23]
 [ 22 145 111   2   0   2   0  18]
 [  8 112 164   1   0   4   0  11]
 [ 49   1  10 571 110  18  58  20]
 [ 48   1   0  74 515  20 193  28]
 [258  29  27  55  40 298 151  36]
 [ 68   1   0  19 225  77 420  27]
 [180  45  47 111  71 180 119  36]]

==> Best [Top1: 45.872   Top5: 91.939   Sparsity:0.00   Params: 117200 on epoch: 0]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [1][   10/  207]    Overall Loss 1.095002    Objective Loss 1.095002                                        LR 0.001000    Time 0.122938    
Epoch: [1][   20/  207]    Overall Loss 1.100280    Objective Loss 1.100280                                        LR 0.001000    Time 0.088639    
Epoch: [1][   30/  207]    Overall Loss 1.081595    Objective Loss 1.081595                                        LR 0.001000    Time 0.076780    
Epoch: [1][   40/  207]    Overall Loss 1.084887    Objective Loss 1.084887                                        LR 0.001000    Time 0.070849    
Epoch: [1][   50/  207]    Overall Loss 1.084705    Objective Loss 1.084705                                        LR 0.001000    Time 0.067385    
Epoch: [1][   60/  207]    Overall Loss 1.074958    Objective Loss 1.074958                                        LR 0.001000    Time 0.065052    
Epoch: [1][   70/  207]    Overall Loss 1.063846    Objective Loss 1.063846                                        LR 0.001000    Time 0.063363    
Epoch: [1][   80/  207]    Overall Loss 1.058675    Objective Loss 1.058675                                        LR 0.001000    Time 0.062089    
Epoch: [1][   90/  207]    Overall Loss 1.055306    Objective Loss 1.055306                                        LR 0.001000    Time 0.061071    
Epoch: [1][  100/  207]    Overall Loss 1.050745    Objective Loss 1.050745                                        LR 0.001000    Time 0.060288    
Epoch: [1][  110/  207]    Overall Loss 1.045614    Objective Loss 1.045614                                        LR 0.001000    Time 0.059651    
Epoch: [1][  120/  207]    Overall Loss 1.042715    Objective Loss 1.042715                                        LR 0.001000    Time 0.059103    
Epoch: [1][  130/  207]    Overall Loss 1.043102    Objective Loss 1.043102                                        LR 0.001000    Time 0.058722    
Epoch: [1][  140/  207]    Overall Loss 1.035848    Objective Loss 1.035848                                        LR 0.001000    Time 0.058334    
Epoch: [1][  150/  207]    Overall Loss 1.032471    Objective Loss 1.032471                                        LR 0.001000    Time 0.057984    
Epoch: [1][  160/  207]    Overall Loss 1.029049    Objective Loss 1.029049                                        LR 0.001000    Time 0.057684    
Epoch: [1][  170/  207]    Overall Loss 1.024437    Objective Loss 1.024437                                        LR 0.001000    Time 0.057426    
Epoch: [1][  180/  207]    Overall Loss 1.018875    Objective Loss 1.018875                                        LR 0.001000    Time 0.057184    
Epoch: [1][  190/  207]    Overall Loss 1.011931    Objective Loss 1.011931                                        LR 0.001000    Time 0.056979    
Epoch: [1][  200/  207]    Overall Loss 1.004825    Objective Loss 1.004825                                        LR 0.001000    Time 0.056780    
Epoch: [1][  207/  207]    Overall Loss 1.001541    Objective Loss 1.001541    Top1 70.554926    Top5 97.508494    LR 0.001000    Time 0.056644    
--- validate (epoch=1)-----------
5136 samples (512 per mini-batch)
Epoch: [1][   10/   11]    Loss 0.928183    Top1 56.035156    Top5 96.054688    
Epoch: [1][   11/   11]    Loss 0.964999    Top1 56.035826    Top5 96.066978    
==> Top1: 56.036    Top5: 96.067    Loss: 0.965

==> Confusion:
[[184  18  27   6   1  36   6  22]
 [  3 137 153   2   0   0   0   5]
 [  4  50 233   1   0   3   0   9]
 [ 11  19   3 677  92  13  13   9]
 [ 17   5   0 136 622  29  50  20]
 [222  23  48  54  34 415  87  11]
 [ 38   9   1   7  78  91 584  29]
 [136  42  71 141  78 227  68  26]]

==> Best [Top1: 56.036   Top5: 96.067   Sparsity:0.00   Params: 117200 on epoch: 1]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [2][   10/  207]    Overall Loss 0.882580    Objective Loss 0.882580                                        LR 0.001000    Time 0.107702    
Epoch: [2][   20/  207]    Overall Loss 0.887844    Objective Loss 0.887844                                        LR 0.001000    Time 0.080337    
Epoch: [2][   30/  207]    Overall Loss 0.888676    Objective Loss 0.888676                                        LR 0.001000    Time 0.071395    
Epoch: [2][   40/  207]    Overall Loss 0.885321    Objective Loss 0.885321                                        LR 0.001000    Time 0.066902    
Epoch: [2][   50/  207]    Overall Loss 0.887627    Objective Loss 0.887627                                        LR 0.001000    Time 0.064197    
Epoch: [2][   60/  207]    Overall Loss 0.887162    Objective Loss 0.887162                                        LR 0.001000    Time 0.062508    
Epoch: [2][   70/  207]    Overall Loss 0.899714    Objective Loss 0.899714                                        LR 0.001000    Time 0.061335    
Epoch: [2][   80/  207]    Overall Loss 0.896822    Objective Loss 0.896822                                        LR 0.001000    Time 0.060305    
Epoch: [2][   90/  207]    Overall Loss 0.893529    Objective Loss 0.893529                                        LR 0.001000    Time 0.059526    
Epoch: [2][  100/  207]    Overall Loss 0.887986    Objective Loss 0.887986                                        LR 0.001000    Time 0.058880    
Epoch: [2][  110/  207]    Overall Loss 0.883603    Objective Loss 0.883603                                        LR 0.001000    Time 0.058374    
Epoch: [2][  120/  207]    Overall Loss 0.882189    Objective Loss 0.882189                                        LR 0.001000    Time 0.057947    
Epoch: [2][  130/  207]    Overall Loss 0.878551    Objective Loss 0.878551                                        LR 0.001000    Time 0.057582    
Epoch: [2][  140/  207]    Overall Loss 0.869933    Objective Loss 0.869933                                        LR 0.001000    Time 0.057268    
Epoch: [2][  150/  207]    Overall Loss 0.867528    Objective Loss 0.867528                                        LR 0.001000    Time 0.057021    
Epoch: [2][  160/  207]    Overall Loss 0.862959    Objective Loss 0.862959                                        LR 0.001000    Time 0.056745    
Epoch: [2][  170/  207]    Overall Loss 0.861135    Objective Loss 0.861135                                        LR 0.001000    Time 0.056552    
Epoch: [2][  180/  207]    Overall Loss 0.857726    Objective Loss 0.857726                                        LR 0.001000    Time 0.056357    
Epoch: [2][  190/  207]    Overall Loss 0.855393    Objective Loss 0.855393                                        LR 0.001000    Time 0.056200    
Epoch: [2][  200/  207]    Overall Loss 0.850747    Objective Loss 0.850747                                        LR 0.001000    Time 0.056066    
Epoch: [2][  207/  207]    Overall Loss 0.849890    Objective Loss 0.849890    Top1 72.366931    Top5 98.640997    LR 0.001000    Time 0.055963    
--- validate (epoch=2)-----------
5136 samples (512 per mini-batch)
Epoch: [2][   10/   11]    Loss 0.799339    Top1 60.605469    Top5 97.441406    
Epoch: [2][   11/   11]    Loss 0.809434    Top1 60.514019    Top5 97.449377    
==> Top1: 60.514    Top5: 97.449    Loss: 0.809

==> Confusion:
[[216   7  10   1   0  41   5  20]
 [ 18 222  49   2   0   4   0   5]
 [  8  96 181   2   0   5   0   8]
 [ 12   3   0 643 126  29  21   3]
 [ 13   1   0 102 676  24  47  16]
 [218  11  20  30  20 520  66   9]
 [ 37   2   0   2  69  91 610  26]
 [155  33  38 108  83 261  71  40]]

==> Best [Top1: 60.514   Top5: 97.449   Sparsity:0.00   Params: 117200 on epoch: 2]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [3][   10/  207]    Overall Loss 0.760894    Objective Loss 0.760894                                        LR 0.001000    Time 0.111305    
Epoch: [3][   20/  207]    Overall Loss 0.767912    Objective Loss 0.767912                                        LR 0.001000    Time 0.082652    
Epoch: [3][   30/  207]    Overall Loss 0.750903    Objective Loss 0.750903                                        LR 0.001000    Time 0.072820    
Epoch: [3][   40/  207]    Overall Loss 0.747191    Objective Loss 0.747191                                        LR 0.001000    Time 0.067901    
Epoch: [3][   50/  207]    Overall Loss 0.749942    Objective Loss 0.749942                                        LR 0.001000    Time 0.064981    
Epoch: [3][   60/  207]    Overall Loss 0.753041    Objective Loss 0.753041                                        LR 0.001000    Time 0.063082    
Epoch: [3][   70/  207]    Overall Loss 0.753713    Objective Loss 0.753713                                        LR 0.001000    Time 0.061649    
Epoch: [3][   80/  207]    Overall Loss 0.749954    Objective Loss 0.749954                                        LR 0.001000    Time 0.060589    
Epoch: [3][   90/  207]    Overall Loss 0.746156    Objective Loss 0.746156                                        LR 0.001000    Time 0.059784    
Epoch: [3][  100/  207]    Overall Loss 0.743590    Objective Loss 0.743590                                        LR 0.001000    Time 0.059137    
Epoch: [3][  110/  207]    Overall Loss 0.741988    Objective Loss 0.741988                                        LR 0.001000    Time 0.058574    
Epoch: [3][  120/  207]    Overall Loss 0.740738    Objective Loss 0.740738                                        LR 0.001000    Time 0.058152    
Epoch: [3][  130/  207]    Overall Loss 0.738235    Objective Loss 0.738235                                        LR 0.001000    Time 0.057742    
Epoch: [3][  140/  207]    Overall Loss 0.736187    Objective Loss 0.736187                                        LR 0.001000    Time 0.057401    
Epoch: [3][  150/  207]    Overall Loss 0.732473    Objective Loss 0.732473                                        LR 0.001000    Time 0.057159    
Epoch: [3][  160/  207]    Overall Loss 0.730171    Objective Loss 0.730171                                        LR 0.001000    Time 0.056896    
Epoch: [3][  170/  207]    Overall Loss 0.728998    Objective Loss 0.728998                                        LR 0.001000    Time 0.056696    
Epoch: [3][  180/  207]    Overall Loss 0.726429    Objective Loss 0.726429                                        LR 0.001000    Time 0.056497    
Epoch: [3][  190/  207]    Overall Loss 0.725701    Objective Loss 0.725701                                        LR 0.001000    Time 0.056312    
Epoch: [3][  200/  207]    Overall Loss 0.723667    Objective Loss 0.723667                                        LR 0.001000    Time 0.056181    
Epoch: [3][  207/  207]    Overall Loss 0.723561    Objective Loss 0.723561    Top1 73.952435    Top5 98.867497    LR 0.001000    Time 0.056074    
--- validate (epoch=3)-----------
5136 samples (512 per mini-batch)
Epoch: [3][   10/   11]    Loss 0.754838    Top1 61.210937    Top5 97.792969    
Epoch: [3][   11/   11]    Loss 0.780923    Top1 61.176012    Top5 97.799844    
==> Top1: 61.176    Top5: 97.800    Loss: 0.781

==> Confusion:
[[232  11  28   0   0  12   3  14]
 [  6 212  77   0   0   1   0   4]
 [  5  53 237   0   0   1   0   4]
 [ 15  41   1 610 112  28  23   7]
 [ 29   6   0  68 683  20  56  17]
 [276  18  48  15  19 458  53   7]
 [ 51   8   1   2  35  45 672  23]
 [212  59  70  68  68 188  86  38]]

==> Best [Top1: 61.176   Top5: 97.800   Sparsity:0.00   Params: 117200 on epoch: 3]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [4][   10/  207]    Overall Loss 0.708475    Objective Loss 0.708475                                        LR 0.001000    Time 0.094162    
Epoch: [4][   20/  207]    Overall Loss 0.696723    Objective Loss 0.696723                                        LR 0.001000    Time 0.074014    
Epoch: [4][   30/  207]    Overall Loss 0.682257    Objective Loss 0.682257                                        LR 0.001000    Time 0.067114    
Epoch: [4][   40/  207]    Overall Loss 0.670678    Objective Loss 0.670678                                        LR 0.001000    Time 0.063664    
Epoch: [4][   50/  207]    Overall Loss 0.672859    Objective Loss 0.672859                                        LR 0.001000    Time 0.061551    
Epoch: [4][   60/  207]    Overall Loss 0.671930    Objective Loss 0.671930                                        LR 0.001000    Time 0.060166    
Epoch: [4][   70/  207]    Overall Loss 0.668391    Objective Loss 0.668391                                        LR 0.001000    Time 0.059176    
Epoch: [4][   80/  207]    Overall Loss 0.663140    Objective Loss 0.663140                                        LR 0.001000    Time 0.058435    
Epoch: [4][   90/  207]    Overall Loss 0.659553    Objective Loss 0.659553                                        LR 0.001000    Time 0.057861    
Epoch: [4][  100/  207]    Overall Loss 0.658142    Objective Loss 0.658142                                        LR 0.001000    Time 0.057391    
Epoch: [4][  110/  207]    Overall Loss 0.652568    Objective Loss 0.652568                                        LR 0.001000    Time 0.057021    
Epoch: [4][  120/  207]    Overall Loss 0.650264    Objective Loss 0.650264                                        LR 0.001000    Time 0.056713    
Epoch: [4][  130/  207]    Overall Loss 0.649348    Objective Loss 0.649348                                        LR 0.001000    Time 0.056469    
Epoch: [4][  140/  207]    Overall Loss 0.650984    Objective Loss 0.650984                                        LR 0.001000    Time 0.056226    
Epoch: [4][  150/  207]    Overall Loss 0.650503    Objective Loss 0.650503                                        LR 0.001000    Time 0.056030    
Epoch: [4][  160/  207]    Overall Loss 0.648637    Objective Loss 0.648637                                        LR 0.001000    Time 0.055864    
Epoch: [4][  170/  207]    Overall Loss 0.646263    Objective Loss 0.646263                                        LR 0.001000    Time 0.055712    
Epoch: [4][  180/  207]    Overall Loss 0.644058    Objective Loss 0.644058                                        LR 0.001000    Time 0.055583    
Epoch: [4][  190/  207]    Overall Loss 0.642213    Objective Loss 0.642213                                        LR 0.001000    Time 0.055470    
Epoch: [4][  200/  207]    Overall Loss 0.640144    Objective Loss 0.640144                                        LR 0.001000    Time 0.055354    
Epoch: [4][  207/  207]    Overall Loss 0.640292    Objective Loss 0.640292    Top1 77.576444    Top5 99.207248    LR 0.001000    Time 0.055281    
--- validate (epoch=4)-----------
5136 samples (512 per mini-batch)
Epoch: [4][   10/   11]    Loss 0.655891    Top1 66.855469    Top5 98.515625    
Epoch: [4][   11/   11]    Loss 0.653542    Top1 66.822430    Top5 98.520249    
==> Top1: 66.822    Top5: 98.520    Loss: 0.654

==> Confusion:
[[256   9  14   0   0  12   0   9]
 [  9 258  31   0   0   1   0   1]
 [  7  81 206   0   0   2   0   4]
 [ 12   6   0 657  92  28  29  13]
 [ 10   1   0  68 638  21 117  24]
 [135  14  31  18  16 614  60   6]
 [ 22   3   0   1   7  37 746  21]
 [153  45  60  64  52 262  96  57]]

==> Best [Top1: 66.822   Top5: 98.520   Sparsity:0.00   Params: 117200 on epoch: 4]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [5][   10/  207]    Overall Loss 0.558835    Objective Loss 0.558835                                        LR 0.001000    Time 0.108556    
Epoch: [5][   20/  207]    Overall Loss 0.571810    Objective Loss 0.571810                                        LR 0.001000    Time 0.080986    
Epoch: [5][   30/  207]    Overall Loss 0.570465    Objective Loss 0.570465                                        LR 0.001000    Time 0.072196    
Epoch: [5][   40/  207]    Overall Loss 0.566648    Objective Loss 0.566648                                        LR 0.001000    Time 0.067522    
Epoch: [5][   50/  207]    Overall Loss 0.574750    Objective Loss 0.574750                                        LR 0.001000    Time 0.064687    
Epoch: [5][   60/  207]    Overall Loss 0.573853    Objective Loss 0.573853                                        LR 0.001000    Time 0.062800    
Epoch: [5][   70/  207]    Overall Loss 0.569741    Objective Loss 0.569741                                        LR 0.001000    Time 0.061448    
Epoch: [5][   80/  207]    Overall Loss 0.566788    Objective Loss 0.566788                                        LR 0.001000    Time 0.060421    
Epoch: [5][   90/  207]    Overall Loss 0.564430    Objective Loss 0.564430                                        LR 0.001000    Time 0.059622    
Epoch: [5][  100/  207]    Overall Loss 0.562458    Objective Loss 0.562458                                        LR 0.001000    Time 0.059010    
Epoch: [5][  110/  207]    Overall Loss 0.564709    Objective Loss 0.564709                                        LR 0.001000    Time 0.058557    
Epoch: [5][  120/  207]    Overall Loss 0.566979    Objective Loss 0.566979                                        LR 0.001000    Time 0.058098    
Epoch: [5][  130/  207]    Overall Loss 0.565712    Objective Loss 0.565712                                        LR 0.001000    Time 0.057805    
Epoch: [5][  140/  207]    Overall Loss 0.562480    Objective Loss 0.562480                                        LR 0.001000    Time 0.057550    
Epoch: [5][  150/  207]    Overall Loss 0.561010    Objective Loss 0.561010                                        LR 0.001000    Time 0.057263    
Epoch: [5][  160/  207]    Overall Loss 0.559413    Objective Loss 0.559413                                        LR 0.001000    Time 0.057024    
Epoch: [5][  170/  207]    Overall Loss 0.560877    Objective Loss 0.560877                                        LR 0.001000    Time 0.056813    
Epoch: [5][  180/  207]    Overall Loss 0.562014    Objective Loss 0.562014                                        LR 0.001000    Time 0.056619    
Epoch: [5][  190/  207]    Overall Loss 0.561473    Objective Loss 0.561473                                        LR 0.001000    Time 0.056451    
Epoch: [5][  200/  207]    Overall Loss 0.560818    Objective Loss 0.560818                                        LR 0.001000    Time 0.056304    
Epoch: [5][  207/  207]    Overall Loss 0.560779    Objective Loss 0.560779    Top1 76.783692    Top5 99.207248    LR 0.001000    Time 0.056186    
--- validate (epoch=5)-----------
5136 samples (512 per mini-batch)
Epoch: [5][   10/   11]    Loss 0.567648    Top1 71.386719    Top5 98.886719    
Epoch: [5][   11/   11]    Loss 0.565175    Top1 71.339564    Top5 98.890187    
==> Top1: 71.340    Top5: 98.890    Loss: 0.565

==> Confusion:
[[252   5   7   1   0  22   0  13]
 [ 11 209  68   3   0   5   0   4]
 [ 10  35 243   1   0   7   0   4]
 [  7   2   0 690 102  17  17   2]
 [  4   0   0  60 757   7  32  19]
 [ 31   6  19  23  36 723  51   5]
 [  7   0   1   5  41  40 727  16]
 [120  21  49 110  86 266  74  63]]

==> Best [Top1: 71.340   Top5: 98.890   Sparsity:0.00   Params: 117200 on epoch: 5]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [6][   10/  207]    Overall Loss 0.499608    Objective Loss 0.499608                                        LR 0.001000    Time 0.110718    
Epoch: [6][   20/  207]    Overall Loss 0.506583    Objective Loss 0.506583                                        LR 0.001000    Time 0.083731    
Epoch: [6][   30/  207]    Overall Loss 0.506598    Objective Loss 0.506598                                        LR 0.001000    Time 0.074033    
Epoch: [6][   40/  207]    Overall Loss 0.514299    Objective Loss 0.514299                                        LR 0.001000    Time 0.068800    
Epoch: [6][   50/  207]    Overall Loss 0.515021    Objective Loss 0.515021                                        LR 0.001000    Time 0.065666    
Epoch: [6][   60/  207]    Overall Loss 0.508042    Objective Loss 0.508042                                        LR 0.001000    Time 0.063606    
Epoch: [6][   70/  207]    Overall Loss 0.503883    Objective Loss 0.503883                                        LR 0.001000    Time 0.062105    
Epoch: [6][   80/  207]    Overall Loss 0.500005    Objective Loss 0.500005                                        LR 0.001000    Time 0.060970    
Epoch: [6][   90/  207]    Overall Loss 0.502114    Objective Loss 0.502114                                        LR 0.001000    Time 0.060124    
Epoch: [6][  100/  207]    Overall Loss 0.503095    Objective Loss 0.503095                                        LR 0.001000    Time 0.059424    
Epoch: [6][  110/  207]    Overall Loss 0.505232    Objective Loss 0.505232                                        LR 0.001000    Time 0.058859    
Epoch: [6][  120/  207]    Overall Loss 0.503871    Objective Loss 0.503871                                        LR 0.001000    Time 0.058349    
Epoch: [6][  130/  207]    Overall Loss 0.502480    Objective Loss 0.502480                                        LR 0.001000    Time 0.057958    
Epoch: [6][  140/  207]    Overall Loss 0.501916    Objective Loss 0.501916                                        LR 0.001000    Time 0.057666    
Epoch: [6][  150/  207]    Overall Loss 0.502403    Objective Loss 0.502403                                        LR 0.001000    Time 0.057378    
Epoch: [6][  160/  207]    Overall Loss 0.504187    Objective Loss 0.504187                                        LR 0.001000    Time 0.057122    
Epoch: [6][  170/  207]    Overall Loss 0.503758    Objective Loss 0.503758                                        LR 0.001000    Time 0.056904    
Epoch: [6][  180/  207]    Overall Loss 0.504877    Objective Loss 0.504877                                        LR 0.001000    Time 0.056716    
Epoch: [6][  190/  207]    Overall Loss 0.504263    Objective Loss 0.504263                                        LR 0.001000    Time 0.056552    
Epoch: [6][  200/  207]    Overall Loss 0.501637    Objective Loss 0.501637                                        LR 0.001000    Time 0.056415    
Epoch: [6][  207/  207]    Overall Loss 0.501260    Objective Loss 0.501260    Top1 80.634202    Top5 99.320498    LR 0.001000    Time 0.056310    
--- validate (epoch=6)-----------
5136 samples (512 per mini-batch)
Epoch: [6][   10/   11]    Loss 0.601558    Top1 69.765625    Top5 99.160156    
Epoch: [6][   11/   11]    Loss 0.626562    Top1 69.762461    Top5 99.143302    
==> Top1: 69.762    Top5: 99.143    Loss: 0.627

==> Confusion:
[[263   3   8   0   0  11   1  14]
 [ 16 181  99   0   0   1   0   3]
 [ 14  21 258   0   0   3   0   4]
 [ 12  11   0 642 116  17  23  16]
 [  4   0   0  37 752  12  49  25]
 [ 62   5  36  13  28 661  76  13]
 [ 10   1   0   1  40  17 745  23]
 [162  40  59  60  82 207  98  81]]

==> Best [Top1: 71.340   Top5: 98.890   Sparsity:0.00   Params: 117200 on epoch: 5]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [7][   10/  207]    Overall Loss 0.485999    Objective Loss 0.485999                                        LR 0.001000    Time 0.111015    
Epoch: [7][   20/  207]    Overall Loss 0.479306    Objective Loss 0.479306                                        LR 0.001000    Time 0.082429    
Epoch: [7][   30/  207]    Overall Loss 0.473725    Objective Loss 0.473725                                        LR 0.001000    Time 0.072941    
Epoch: [7][   40/  207]    Overall Loss 0.469711    Objective Loss 0.469711                                        LR 0.001000    Time 0.068150    
Epoch: [7][   50/  207]    Overall Loss 0.469584    Objective Loss 0.469584                                        LR 0.001000    Time 0.065246    
Epoch: [7][   60/  207]    Overall Loss 0.467814    Objective Loss 0.467814                                        LR 0.001000    Time 0.063253    
Epoch: [7][   70/  207]    Overall Loss 0.464785    Objective Loss 0.464785                                        LR 0.001000    Time 0.061844    
Epoch: [7][   80/  207]    Overall Loss 0.458014    Objective Loss 0.458014                                        LR 0.001000    Time 0.060775    
Epoch: [7][   90/  207]    Overall Loss 0.459108    Objective Loss 0.459108                                        LR 0.001000    Time 0.059964    
Epoch: [7][  100/  207]    Overall Loss 0.458509    Objective Loss 0.458509                                        LR 0.001000    Time 0.059309    
Epoch: [7][  110/  207]    Overall Loss 0.461050    Objective Loss 0.461050                                        LR 0.001000    Time 0.058778    
Epoch: [7][  120/  207]    Overall Loss 0.461410    Objective Loss 0.461410                                        LR 0.001000    Time 0.058363    
Epoch: [7][  130/  207]    Overall Loss 0.460104    Objective Loss 0.460104                                        LR 0.001000    Time 0.057957    
Epoch: [7][  140/  207]    Overall Loss 0.458432    Objective Loss 0.458432                                        LR 0.001000    Time 0.057621    
Epoch: [7][  150/  207]    Overall Loss 0.458301    Objective Loss 0.458301                                        LR 0.001000    Time 0.057339    
Epoch: [7][  160/  207]    Overall Loss 0.457437    Objective Loss 0.457437                                        LR 0.001000    Time 0.057087    
Epoch: [7][  170/  207]    Overall Loss 0.458804    Objective Loss 0.458804                                        LR 0.001000    Time 0.056868    
Epoch: [7][  180/  207]    Overall Loss 0.458849    Objective Loss 0.458849                                        LR 0.001000    Time 0.056672    
Epoch: [7][  190/  207]    Overall Loss 0.458742    Objective Loss 0.458742                                        LR 0.001000    Time 0.056501    
Epoch: [7][  200/  207]    Overall Loss 0.459179    Objective Loss 0.459179                                        LR 0.001000    Time 0.056358    
Epoch: [7][  207/  207]    Overall Loss 0.457913    Objective Loss 0.457913    Top1 84.711212    Top5 99.660249    LR 0.001000    Time 0.056251    
--- validate (epoch=7)-----------
5136 samples (512 per mini-batch)
Epoch: [7][   10/   11]    Loss 0.577210    Top1 71.464844    Top5 99.335938    
Epoch: [7][   11/   11]    Loss 0.609254    Top1 71.436916    Top5 99.338006    
==> Top1: 71.437    Top5: 99.338    Loss: 0.609

==> Confusion:
[[258  12   1   0   1   7   1  20]
 [  7 283   5   0   0   2   1   2]
 [ 15 108 168   0   0   2   0   7]
 [  4   5   0 691  93  12  18  14]
 [  4   0   0  58 736   9  53  19]
 [ 42  19  10  19  28 660 100  16]
 [  5   1   0   4  20  11 777  19]
 [118  70  16  96  72 206 115  96]]

==> Best [Top1: 71.437   Top5: 99.338   Sparsity:0.00   Params: 117200 on epoch: 7]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [8][   10/  207]    Overall Loss 0.423078    Objective Loss 0.423078                                        LR 0.001000    Time 0.094231    
Epoch: [8][   20/  207]    Overall Loss 0.427428    Objective Loss 0.427428                                        LR 0.001000    Time 0.073873    
Epoch: [8][   30/  207]    Overall Loss 0.432186    Objective Loss 0.432186                                        LR 0.001000    Time 0.067174    
Epoch: [8][   40/  207]    Overall Loss 0.422152    Objective Loss 0.422152                                        LR 0.001000    Time 0.063760    
Epoch: [8][   50/  207]    Overall Loss 0.426607    Objective Loss 0.426607                                        LR 0.001000    Time 0.061694    
Epoch: [8][   60/  207]    Overall Loss 0.426151    Objective Loss 0.426151                                        LR 0.001000    Time 0.060335    
Epoch: [8][   70/  207]    Overall Loss 0.423692    Objective Loss 0.423692                                        LR 0.001000    Time 0.059311    
Epoch: [8][   80/  207]    Overall Loss 0.422953    Objective Loss 0.422953                                        LR 0.001000    Time 0.058587    
Epoch: [8][   90/  207]    Overall Loss 0.423350    Objective Loss 0.423350                                        LR 0.001000    Time 0.058023    
Epoch: [8][  100/  207]    Overall Loss 0.426327    Objective Loss 0.426327                                        LR 0.001000    Time 0.057588    
Epoch: [8][  110/  207]    Overall Loss 0.425797    Objective Loss 0.425797                                        LR 0.001000    Time 0.057187    
Epoch: [8][  120/  207]    Overall Loss 0.427662    Objective Loss 0.427662                                        LR 0.001000    Time 0.056905    
Epoch: [8][  130/  207]    Overall Loss 0.428114    Objective Loss 0.428114                                        LR 0.001000    Time 0.056633    
Epoch: [8][  140/  207]    Overall Loss 0.431805    Objective Loss 0.431805                                        LR 0.001000    Time 0.056396    
Epoch: [8][  150/  207]    Overall Loss 0.431570    Objective Loss 0.431570                                        LR 0.001000    Time 0.056208    
Epoch: [8][  160/  207]    Overall Loss 0.430359    Objective Loss 0.430359                                        LR 0.001000    Time 0.056023    
Epoch: [8][  170/  207]    Overall Loss 0.430006    Objective Loss 0.430006                                        LR 0.001000    Time 0.055872    
Epoch: [8][  180/  207]    Overall Loss 0.428745    Objective Loss 0.428745                                        LR 0.001000    Time 0.055735    
Epoch: [8][  190/  207]    Overall Loss 0.429447    Objective Loss 0.429447                                        LR 0.001000    Time 0.055625    
Epoch: [8][  200/  207]    Overall Loss 0.428113    Objective Loss 0.428113                                        LR 0.001000    Time 0.055532    
Epoch: [8][  207/  207]    Overall Loss 0.426115    Objective Loss 0.426115    Top1 86.409966    Top5 99.660249    LR 0.001000    Time 0.055455    
--- validate (epoch=8)-----------
5136 samples (512 per mini-batch)
Epoch: [8][   10/   11]    Loss 0.508816    Top1 73.066406    Top5 99.472656    
Epoch: [8][   11/   11]    Loss 0.510628    Top1 73.072430    Top5 99.474299    
==> Top1: 73.072    Top5: 99.474    Loss: 0.511

==> Confusion:
[[252   6   4   2   1  15   1  19]
 [ 10 264  16   1   0   2   0   7]
 [  2  69 214   0   0   9   0   6]
 [  2   5   0 684  98  19  25   4]
 [  0   0   0  51 738  10  62  18]
 [ 17   8  11  16  25 729  80   8]
 [  3   0   0   4  25   8 781  16]
 [ 89  53  27  92  71 257 109  91]]

==> Best [Top1: 73.072   Top5: 99.474   Sparsity:0.00   Params: 117200 on epoch: 8]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [9][   10/  207]    Overall Loss 0.395679    Objective Loss 0.395679                                        LR 0.001000    Time 0.107447    
Epoch: [9][   20/  207]    Overall Loss 0.403933    Objective Loss 0.403933                                        LR 0.001000    Time 0.080911    
Epoch: [9][   30/  207]    Overall Loss 0.405179    Objective Loss 0.405179                                        LR 0.001000    Time 0.071763    
Epoch: [9][   40/  207]    Overall Loss 0.426101    Objective Loss 0.426101                                        LR 0.001000    Time 0.067163    
Epoch: [9][   50/  207]    Overall Loss 0.431537    Objective Loss 0.431537                                        LR 0.001000    Time 0.064429    
Epoch: [9][   60/  207]    Overall Loss 0.433547    Objective Loss 0.433547                                        LR 0.001000    Time 0.062629    
Epoch: [9][   70/  207]    Overall Loss 0.430325    Objective Loss 0.430325                                        LR 0.001000    Time 0.061345    
Epoch: [9][   80/  207]    Overall Loss 0.427417    Objective Loss 0.427417                                        LR 0.001000    Time 0.060359    
Epoch: [9][   90/  207]    Overall Loss 0.425216    Objective Loss 0.425216                                        LR 0.001000    Time 0.059671    
Epoch: [9][  100/  207]    Overall Loss 0.420595    Objective Loss 0.420595                                        LR 0.001000    Time 0.059127    
Epoch: [9][  110/  207]    Overall Loss 0.420805    Objective Loss 0.420805                                        LR 0.001000    Time 0.058596    
Epoch: [9][  120/  207]    Overall Loss 0.419139    Objective Loss 0.419139                                        LR 0.001000    Time 0.058157    
Epoch: [9][  130/  207]    Overall Loss 0.415610    Objective Loss 0.415610                                        LR 0.001000    Time 0.057810    
Epoch: [9][  140/  207]    Overall Loss 0.412487    Objective Loss 0.412487                                        LR 0.001000    Time 0.057482    
Epoch: [9][  150/  207]    Overall Loss 0.409266    Objective Loss 0.409266                                        LR 0.001000    Time 0.057232    
Epoch: [9][  160/  207]    Overall Loss 0.405424    Objective Loss 0.405424                                        LR 0.001000    Time 0.057000    
Epoch: [9][  170/  207]    Overall Loss 0.405323    Objective Loss 0.405323                                        LR 0.001000    Time 0.056802    
Epoch: [9][  180/  207]    Overall Loss 0.403676    Objective Loss 0.403676                                        LR 0.001000    Time 0.056627    
Epoch: [9][  190/  207]    Overall Loss 0.403411    Objective Loss 0.403411                                        LR 0.001000    Time 0.056464    
Epoch: [9][  200/  207]    Overall Loss 0.402270    Objective Loss 0.402270                                        LR 0.001000    Time 0.056339    
Epoch: [9][  207/  207]    Overall Loss 0.405914    Objective Loss 0.405914    Top1 79.501699    Top5 99.207248    LR 0.001000    Time 0.056244    
--- validate (epoch=9)-----------
5136 samples (512 per mini-batch)
Epoch: [9][   10/   11]    Loss 0.499415    Top1 72.949219    Top5 99.160156    
Epoch: [9][   11/   11]    Loss 0.469050    Top1 72.955607    Top5 99.123832    
==> Top1: 72.956    Top5: 99.124    Loss: 0.469

==> Confusion:
[[244   9  12   1   1  26   0   7]
 [  3 232  61   0   0   3   0   1]
 [  1  27 264   1   0   6   0   1]
 [  1   5   0 724  88   9  10   0]
 [  0   0   0  80 772  12  11   4]
 [  7   8   9  36  34 775  21   4]
 [  5   2   0   6  78  41 693  12]
 [ 62  51  45 144  86 308  50  43]]

==> Best [Top1: 73.072   Top5: 99.474   Sparsity:0.00   Params: 117200 on epoch: 8]
Saving checkpoint to: logs/2024.01.15-145725/checkpoint.pth.tar


Initiating quantization aware training (QAT)...


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [10][   10/  207]    Overall Loss 0.583015    Objective Loss 0.583015                                        LR 0.001000    Time 0.137561    
Epoch: [10][   20/  207]    Overall Loss 0.535738    Objective Loss 0.535738                                        LR 0.001000    Time 0.104304    
Epoch: [10][   30/  207]    Overall Loss 0.509671    Objective Loss 0.509671                                        LR 0.001000    Time 0.093866    
Epoch: [10][   40/  207]    Overall Loss 0.509086    Objective Loss 0.509086                                        LR 0.001000    Time 0.088435    
Epoch: [10][   50/  207]    Overall Loss 0.493770    Objective Loss 0.493770                                        LR 0.001000    Time 0.085020    
Epoch: [10][   60/  207]    Overall Loss 0.486876    Objective Loss 0.486876                                        LR 0.001000    Time 0.082951    
Epoch: [10][   70/  207]    Overall Loss 0.472383    Objective Loss 0.472383                                        LR 0.001000    Time 0.081569    
Epoch: [10][   80/  207]    Overall Loss 0.463120    Objective Loss 0.463120                                        LR 0.001000    Time 0.080428    
Epoch: [10][   90/  207]    Overall Loss 0.454082    Objective Loss 0.454082                                        LR 0.001000    Time 0.079472    
Epoch: [10][  100/  207]    Overall Loss 0.446907    Objective Loss 0.446907                                        LR 0.001000    Time 0.078632    
Epoch: [10][  110/  207]    Overall Loss 0.443797    Objective Loss 0.443797                                        LR 0.001000    Time 0.078045    
Epoch: [10][  120/  207]    Overall Loss 0.438885    Objective Loss 0.438885                                        LR 0.001000    Time 0.077423    
Epoch: [10][  130/  207]    Overall Loss 0.436381    Objective Loss 0.436381                                        LR 0.001000    Time 0.077101    
Epoch: [10][  140/  207]    Overall Loss 0.433594    Objective Loss 0.433594                                        LR 0.001000    Time 0.076763    
Epoch: [10][  150/  207]    Overall Loss 0.429609    Objective Loss 0.429609                                        LR 0.001000    Time 0.076479    
Epoch: [10][  160/  207]    Overall Loss 0.424178    Objective Loss 0.424178                                        LR 0.001000    Time 0.076270    
Epoch: [10][  170/  207]    Overall Loss 0.420744    Objective Loss 0.420744                                        LR 0.001000    Time 0.076056    
Epoch: [10][  180/  207]    Overall Loss 0.418554    Objective Loss 0.418554                                        LR 0.001000    Time 0.075876    
Epoch: [10][  190/  207]    Overall Loss 0.419609    Objective Loss 0.419609                                        LR 0.001000    Time 0.075739    
Epoch: [10][  200/  207]    Overall Loss 0.418263    Objective Loss 0.418263                                        LR 0.001000    Time 0.075590    
Epoch: [10][  207/  207]    Overall Loss 0.416319    Objective Loss 0.416319    Top1 87.202718    Top5 99.773499    LR 0.001000    Time 0.075303    
--- validate (epoch=10)-----------
5136 samples (512 per mini-batch)
Epoch: [10][   10/   11]    Loss 0.522980    Top1 75.117188    Top5 99.511719    
Epoch: [10][   11/   11]    Loss 0.514600    Top1 75.155763    Top5 99.513240    
==> Top1: 75.156    Top5: 99.513    Loss: 0.515

==> Confusion:
[[268   6   4   0   1   7   0  14]
 [  9 241  45   0   0   2   0   3]
 [  5  34 252   1   0   4   0   4]
 [  3   5   1 719  73  13   8  15]
 [  2   2   0  76 741  12  23  23]
 [ 34   5  13  18  26 759  18  21]
 [  6   0   1   7  40  36 715  32]
 [111  32  33  99  72 236  41 165]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [11][   10/  207]    Overall Loss 0.376898    Objective Loss 0.376898                                        LR 0.001000    Time 0.132762    
Epoch: [11][   20/  207]    Overall Loss 0.370503    Objective Loss 0.370503                                        LR 0.001000    Time 0.101599    
Epoch: [11][   30/  207]    Overall Loss 0.371471    Objective Loss 0.371471                                        LR 0.001000    Time 0.095683    
Epoch: [11][   40/  207]    Overall Loss 0.380091    Objective Loss 0.380091                                        LR 0.001000    Time 0.089690    
Epoch: [11][   50/  207]    Overall Loss 0.372491    Objective Loss 0.372491                                        LR 0.001000    Time 0.086243    
Epoch: [11][   60/  207]    Overall Loss 0.369560    Objective Loss 0.369560                                        LR 0.001000    Time 0.083585    
Epoch: [11][   70/  207]    Overall Loss 0.371351    Objective Loss 0.371351                                        LR 0.001000    Time 0.082058    
Epoch: [11][   80/  207]    Overall Loss 0.367474    Objective Loss 0.367474                                        LR 0.001000    Time 0.080995    
Epoch: [11][   90/  207]    Overall Loss 0.364754    Objective Loss 0.364754                                        LR 0.001000    Time 0.079965    
Epoch: [11][  100/  207]    Overall Loss 0.362277    Objective Loss 0.362277                                        LR 0.001000    Time 0.079020    
Epoch: [11][  110/  207]    Overall Loss 0.363479    Objective Loss 0.363479                                        LR 0.001000    Time 0.078440    
Epoch: [11][  120/  207]    Overall Loss 0.362907    Objective Loss 0.362907                                        LR 0.001000    Time 0.077879    
Epoch: [11][  130/  207]    Overall Loss 0.359190    Objective Loss 0.359190                                        LR 0.001000    Time 0.077607    
Epoch: [11][  140/  207]    Overall Loss 0.356593    Objective Loss 0.356593                                        LR 0.001000    Time 0.077382    
Epoch: [11][  150/  207]    Overall Loss 0.357693    Objective Loss 0.357693                                        LR 0.001000    Time 0.077100    
Epoch: [11][  160/  207]    Overall Loss 0.359606    Objective Loss 0.359606                                        LR 0.001000    Time 0.076681    
Epoch: [11][  170/  207]    Overall Loss 0.358266    Objective Loss 0.358266                                        LR 0.001000    Time 0.076329    
Epoch: [11][  180/  207]    Overall Loss 0.357729    Objective Loss 0.357729                                        LR 0.001000    Time 0.076088    
Epoch: [11][  190/  207]    Overall Loss 0.358346    Objective Loss 0.358346                                        LR 0.001000    Time 0.075950    
Epoch: [11][  200/  207]    Overall Loss 0.357731    Objective Loss 0.357731                                        LR 0.001000    Time 0.075843    
Epoch: [11][  207/  207]    Overall Loss 0.356977    Objective Loss 0.356977    Top1 82.106455    Top5 99.320498    LR 0.001000    Time 0.075548    
--- validate (epoch=11)-----------
5136 samples (512 per mini-batch)
Epoch: [11][   10/   11]    Loss 0.488414    Top1 75.058594    Top5 99.511719    
Epoch: [11][   11/   11]    Loss 0.512902    Top1 75.038941    Top5 99.513240    
==> Top1: 75.039    Top5: 99.513    Loss: 0.513

==> Confusion:
[[262   6   3   1   1  10   0  17]
 [  6 249  33   1   0   2   2   7]
 [  4  41 244   1   0   4   0   6]
 [  1   4   0 689 114  10  14   5]
 [  3   0   0  48 759  11  42  16]
 [ 15   3  15  22  25 754  44  16]
 [  4   0   0   4  34  23 751  21]
 [ 72  26  24 120  76 245  80 146]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [12][   10/  207]    Overall Loss 0.318392    Objective Loss 0.318392                                        LR 0.001000    Time 0.111252    
Epoch: [12][   20/  207]    Overall Loss 0.303403    Objective Loss 0.303403                                        LR 0.001000    Time 0.091567    
Epoch: [12][   30/  207]    Overall Loss 0.305075    Objective Loss 0.305075                                        LR 0.001000    Time 0.085399    
Epoch: [12][   40/  207]    Overall Loss 0.308316    Objective Loss 0.308316                                        LR 0.001000    Time 0.081682    
Epoch: [12][   50/  207]    Overall Loss 0.319528    Objective Loss 0.319528                                        LR 0.001000    Time 0.080137    
Epoch: [12][   60/  207]    Overall Loss 0.320811    Objective Loss 0.320811                                        LR 0.001000    Time 0.078407    
Epoch: [12][   70/  207]    Overall Loss 0.326136    Objective Loss 0.326136                                        LR 0.001000    Time 0.077368    
Epoch: [12][   80/  207]    Overall Loss 0.324590    Objective Loss 0.324590                                        LR 0.001000    Time 0.076606    
Epoch: [12][   90/  207]    Overall Loss 0.323180    Objective Loss 0.323180                                        LR 0.001000    Time 0.076375    
Epoch: [12][  100/  207]    Overall Loss 0.324163    Objective Loss 0.324163                                        LR 0.001000    Time 0.075877    
Epoch: [12][  110/  207]    Overall Loss 0.323928    Objective Loss 0.323928                                        LR 0.001000    Time 0.075683    
Epoch: [12][  120/  207]    Overall Loss 0.321103    Objective Loss 0.321103                                        LR 0.001000    Time 0.075390    
Epoch: [12][  130/  207]    Overall Loss 0.320926    Objective Loss 0.320926                                        LR 0.001000    Time 0.075041    
Epoch: [12][  140/  207]    Overall Loss 0.319798    Objective Loss 0.319798                                        LR 0.001000    Time 0.075023    
Epoch: [12][  150/  207]    Overall Loss 0.318627    Objective Loss 0.318627                                        LR 0.001000    Time 0.074853    
Epoch: [12][  160/  207]    Overall Loss 0.319156    Objective Loss 0.319156                                        LR 0.001000    Time 0.074685    
Epoch: [12][  170/  207]    Overall Loss 0.318790    Objective Loss 0.318790                                        LR 0.001000    Time 0.074475    
Epoch: [12][  180/  207]    Overall Loss 0.318571    Objective Loss 0.318571                                        LR 0.001000    Time 0.074357    
Epoch: [12][  190/  207]    Overall Loss 0.318359    Objective Loss 0.318359                                        LR 0.001000    Time 0.074223    
Epoch: [12][  200/  207]    Overall Loss 0.319532    Objective Loss 0.319532                                        LR 0.001000    Time 0.074161    
Epoch: [12][  207/  207]    Overall Loss 0.320915    Objective Loss 0.320915    Top1 88.674972    Top5 99.660249    LR 0.001000    Time 0.073946    
--- validate (epoch=12)-----------
5136 samples (512 per mini-batch)
Epoch: [12][   10/   11]    Loss 0.457821    Top1 74.921875    Top5 99.472656    
Epoch: [12][   11/   11]    Loss 0.442698    Top1 74.961059    Top5 99.474299    
==> Top1: 74.961    Top5: 99.474    Loss: 0.443

==> Confusion:
[[280   6   4   0   0   2   0   8]
 [ 12 252  30   0   0   2   2   2]
 [ 14  34 243   1   0   5   0   3]
 [  3   5   1 709  92  13  11   3]
 [  4   1   0  68 758  14  16  18]
 [ 36   6  10  15  43 756  20   8]
 [  5   0   0   4  61  30 719  18]
 [106  30  23  83  95 264  55 133]]

==> Best [Top1: 75.156   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 10]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [13][   10/  207]    Overall Loss 0.356180    Objective Loss 0.356180                                        LR 0.001000    Time 0.130702    
Epoch: [13][   20/  207]    Overall Loss 0.332678    Objective Loss 0.332678                                        LR 0.001000    Time 0.100565    
Epoch: [13][   30/  207]    Overall Loss 0.328884    Objective Loss 0.328884                                        LR 0.001000    Time 0.090945    
Epoch: [13][   40/  207]    Overall Loss 0.323702    Objective Loss 0.323702                                        LR 0.001000    Time 0.085951    
Epoch: [13][   50/  207]    Overall Loss 0.325560    Objective Loss 0.325560                                        LR 0.001000    Time 0.083098    
Epoch: [13][   60/  207]    Overall Loss 0.325074    Objective Loss 0.325074                                        LR 0.001000    Time 0.081370    
Epoch: [13][   70/  207]    Overall Loss 0.322832    Objective Loss 0.322832                                        LR 0.001000    Time 0.079804    
Epoch: [13][   80/  207]    Overall Loss 0.317631    Objective Loss 0.317631                                        LR 0.001000    Time 0.078841    
Epoch: [13][   90/  207]    Overall Loss 0.317330    Objective Loss 0.317330                                        LR 0.001000    Time 0.077917    
Epoch: [13][  100/  207]    Overall Loss 0.319407    Objective Loss 0.319407                                        LR 0.001000    Time 0.077376    
Epoch: [13][  110/  207]    Overall Loss 0.318232    Objective Loss 0.318232                                        LR 0.001000    Time 0.076776    
Epoch: [13][  120/  207]    Overall Loss 0.315669    Objective Loss 0.315669                                        LR 0.001000    Time 0.076282    
Epoch: [13][  130/  207]    Overall Loss 0.315036    Objective Loss 0.315036                                        LR 0.001000    Time 0.075910    
Epoch: [13][  140/  207]    Overall Loss 0.314467    Objective Loss 0.314467                                        LR 0.001000    Time 0.075762    
Epoch: [13][  150/  207]    Overall Loss 0.314876    Objective Loss 0.314876                                        LR 0.001000    Time 0.075593    
Epoch: [13][  160/  207]    Overall Loss 0.314212    Objective Loss 0.314212                                        LR 0.001000    Time 0.075354    
Epoch: [13][  170/  207]    Overall Loss 0.313005    Objective Loss 0.313005                                        LR 0.001000    Time 0.075196    
Epoch: [13][  180/  207]    Overall Loss 0.313547    Objective Loss 0.313547                                        LR 0.001000    Time 0.075152    
Epoch: [13][  190/  207]    Overall Loss 0.313715    Objective Loss 0.313715                                        LR 0.001000    Time 0.075052    
Epoch: [13][  200/  207]    Overall Loss 0.312451    Objective Loss 0.312451                                        LR 0.001000    Time 0.074962    
Epoch: [13][  207/  207]    Overall Loss 0.311887    Objective Loss 0.311887    Top1 87.995470    Top5 99.886750    LR 0.001000    Time 0.074691    
--- validate (epoch=13)-----------
5136 samples (512 per mini-batch)
Epoch: [13][   10/   11]    Loss 0.500172    Top1 76.621094    Top5 99.628906    
Epoch: [13][   11/   11]    Loss 0.478965    Top1 76.616044    Top5 99.630062    
==> Top1: 76.616    Top5: 99.630    Loss: 0.479

==> Confusion:
[[277   8   2   0   0   1   0  12]
 [  7 276  11   1   0   0   0   5]
 [ 11  64 216   1   0   1   0   7]
 [  2   6   1 723  73   9   9  14]
 [  5   1   0  59 736  11  40  27]
 [ 29   8  16  20  23 721  41  36]
 [  6   0   0   8  24  24 746  29]
 [ 91  44  14  84  58 187  71 240]]

==> Best [Top1: 76.616   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 13]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [14][   10/  207]    Overall Loss 0.294264    Objective Loss 0.294264                                        LR 0.001000    Time 0.130949    
Epoch: [14][   20/  207]    Overall Loss 0.275154    Objective Loss 0.275154                                        LR 0.001000    Time 0.101568    
Epoch: [14][   30/  207]    Overall Loss 0.275127    Objective Loss 0.275127                                        LR 0.001000    Time 0.091796    
Epoch: [14][   40/  207]    Overall Loss 0.271858    Objective Loss 0.271858                                        LR 0.001000    Time 0.086895    
Epoch: [14][   50/  207]    Overall Loss 0.275755    Objective Loss 0.275755                                        LR 0.001000    Time 0.083950    
Epoch: [14][   60/  207]    Overall Loss 0.274251    Objective Loss 0.274251                                        LR 0.001000    Time 0.081671    
Epoch: [14][   70/  207]    Overall Loss 0.279270    Objective Loss 0.279270                                        LR 0.001000    Time 0.080266    
Epoch: [14][   80/  207]    Overall Loss 0.287145    Objective Loss 0.287145                                        LR 0.001000    Time 0.079297    
Epoch: [14][   90/  207]    Overall Loss 0.289724    Objective Loss 0.289724                                        LR 0.001000    Time 0.078386    
Epoch: [14][  100/  207]    Overall Loss 0.291298    Objective Loss 0.291298                                        LR 0.001000    Time 0.077635    
Epoch: [14][  110/  207]    Overall Loss 0.294486    Objective Loss 0.294486                                        LR 0.001000    Time 0.077242    
Epoch: [14][  120/  207]    Overall Loss 0.296026    Objective Loss 0.296026                                        LR 0.001000    Time 0.076935    
Epoch: [14][  130/  207]    Overall Loss 0.296524    Objective Loss 0.296524                                        LR 0.001000    Time 0.076452    
Epoch: [14][  140/  207]    Overall Loss 0.298387    Objective Loss 0.298387                                        LR 0.001000    Time 0.076009    
Epoch: [14][  150/  207]    Overall Loss 0.296361    Objective Loss 0.296361                                        LR 0.001000    Time 0.075593    
Epoch: [14][  160/  207]    Overall Loss 0.296916    Objective Loss 0.296916                                        LR 0.001000    Time 0.075347    
Epoch: [14][  170/  207]    Overall Loss 0.299547    Objective Loss 0.299547                                        LR 0.001000    Time 0.074993    
Epoch: [14][  180/  207]    Overall Loss 0.301758    Objective Loss 0.301758                                        LR 0.001000    Time 0.074777    
Epoch: [14][  190/  207]    Overall Loss 0.304098    Objective Loss 0.304098                                        LR 0.001000    Time 0.074636    
Epoch: [14][  200/  207]    Overall Loss 0.302959    Objective Loss 0.302959                                        LR 0.001000    Time 0.074516    
Epoch: [14][  207/  207]    Overall Loss 0.303916    Objective Loss 0.303916    Top1 82.785957    Top5 99.433749    LR 0.001000    Time 0.074290    
--- validate (epoch=14)-----------
5136 samples (512 per mini-batch)
Epoch: [14][   10/   11]    Loss 0.565935    Top1 74.433594    Top5 99.589844    
Epoch: [14][   11/   11]    Loss 0.584503    Top1 74.338006    Top5 99.571651    
==> Top1: 74.338    Top5: 99.572    Loss: 0.585

==> Confusion:
[[285   2   1   0   1   0   1  10]
 [ 17 218  57   0   0   4   1   3]
 [ 12  10 265   1   1   4   0   7]
 [  3   4   0 642 154  14  11   9]
 [  4   0   0  24 804   6  24  17]
 [ 73   2  19   7  48 678  49  18]
 [  9   0   0   2  52  10 738  26]
 [167  19  31  41 132 148  63 188]]

==> Best [Top1: 76.616   Top5: 99.630   Sparsity:0.00   Params: 117200 on epoch: 13]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [15][   10/  207]    Overall Loss 0.314211    Objective Loss 0.314211                                        LR 0.001000    Time 0.128417    
Epoch: [15][   20/  207]    Overall Loss 0.313555    Objective Loss 0.313555                                        LR 0.001000    Time 0.099682    
Epoch: [15][   30/  207]    Overall Loss 0.305406    Objective Loss 0.305406                                        LR 0.001000    Time 0.090002    
Epoch: [15][   40/  207]    Overall Loss 0.294127    Objective Loss 0.294127                                        LR 0.001000    Time 0.085132    
Epoch: [15][   50/  207]    Overall Loss 0.294872    Objective Loss 0.294872                                        LR 0.001000    Time 0.082753    
Epoch: [15][   60/  207]    Overall Loss 0.293732    Objective Loss 0.293732                                        LR 0.001000    Time 0.080902    
Epoch: [15][   70/  207]    Overall Loss 0.290348    Objective Loss 0.290348                                        LR 0.001000    Time 0.079434    
Epoch: [15][   80/  207]    Overall Loss 0.287303    Objective Loss 0.287303                                        LR 0.001000    Time 0.078432    
Epoch: [15][   90/  207]    Overall Loss 0.285417    Objective Loss 0.285417                                        LR 0.001000    Time 0.077695    
Epoch: [15][  100/  207]    Overall Loss 0.284825    Objective Loss 0.284825                                        LR 0.001000    Time 0.077109    
Epoch: [15][  110/  207]    Overall Loss 0.284320    Objective Loss 0.284320                                        LR 0.001000    Time 0.076501    
Epoch: [15][  120/  207]    Overall Loss 0.285158    Objective Loss 0.285158                                        LR 0.001000    Time 0.076076    
Epoch: [15][  130/  207]    Overall Loss 0.286301    Objective Loss 0.286301                                        LR 0.001000    Time 0.075831    
Epoch: [15][  140/  207]    Overall Loss 0.286627    Objective Loss 0.286627                                        LR 0.001000    Time 0.075602    
Epoch: [15][  150/  207]    Overall Loss 0.286152    Objective Loss 0.286152                                        LR 0.001000    Time 0.075285    
Epoch: [15][  160/  207]    Overall Loss 0.286865    Objective Loss 0.286865                                        LR 0.001000    Time 0.075019    
Epoch: [15][  170/  207]    Overall Loss 0.286711    Objective Loss 0.286711                                        LR 0.001000    Time 0.074852    
Epoch: [15][  180/  207]    Overall Loss 0.286212    Objective Loss 0.286212                                        LR 0.001000    Time 0.074637    
Epoch: [15][  190/  207]    Overall Loss 0.285401    Objective Loss 0.285401                                        LR 0.001000    Time 0.074413    
Epoch: [15][  200/  207]    Overall Loss 0.283864    Objective Loss 0.283864                                        LR 0.001000    Time 0.074330    
Epoch: [15][  207/  207]    Overall Loss 0.283905    Objective Loss 0.283905    Top1 88.901472    Top5 100.000000    LR 0.001000    Time 0.074103    
--- validate (epoch=15)-----------
5136 samples (512 per mini-batch)
Epoch: [15][   10/   11]    Loss 0.426429    Top1 77.226562    Top5 99.648438    
Epoch: [15][   11/   11]    Loss 0.410154    Top1 77.239097    Top5 99.649533    
==> Top1: 77.239    Top5: 99.650    Loss: 0.410

==> Confusion:
[[270   6   3   1   1   5   2  12]
 [  8 256  27   0   0   2   0   7]
 [  4  31 255   1   0   4   1   4]
 [  0   5   1 750  54  12  12   3]
 [  3   1   0  68 746   9  40  12]
 [ 15   4  14  17  24 764  46  10]
 [  4   1   0   8  28  10 771  15]
 [ 73  31  17 110  82 230  91 155]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [16][   10/  207]    Overall Loss 0.250688    Objective Loss 0.250688                                        LR 0.001000    Time 0.128600    
Epoch: [16][   20/  207]    Overall Loss 0.264612    Objective Loss 0.264612                                        LR 0.001000    Time 0.099571    
Epoch: [16][   30/  207]    Overall Loss 0.259927    Objective Loss 0.259927                                        LR 0.001000    Time 0.090629    
Epoch: [16][   40/  207]    Overall Loss 0.260306    Objective Loss 0.260306                                        LR 0.001000    Time 0.085664    
Epoch: [16][   50/  207]    Overall Loss 0.255677    Objective Loss 0.255677                                        LR 0.001000    Time 0.082702    
Epoch: [16][   60/  207]    Overall Loss 0.255847    Objective Loss 0.255847                                        LR 0.001000    Time 0.080944    
Epoch: [16][   70/  207]    Overall Loss 0.258546    Objective Loss 0.258546                                        LR 0.001000    Time 0.079745    
Epoch: [16][   80/  207]    Overall Loss 0.264180    Objective Loss 0.264180                                        LR 0.001000    Time 0.079004    
Epoch: [16][   90/  207]    Overall Loss 0.268650    Objective Loss 0.268650                                        LR 0.001000    Time 0.077986    
Epoch: [16][  100/  207]    Overall Loss 0.270293    Objective Loss 0.270293                                        LR 0.001000    Time 0.077281    
Epoch: [16][  110/  207]    Overall Loss 0.270007    Objective Loss 0.270007                                        LR 0.001000    Time 0.076804    
Epoch: [16][  120/  207]    Overall Loss 0.272812    Objective Loss 0.272812                                        LR 0.001000    Time 0.076378    
Epoch: [16][  130/  207]    Overall Loss 0.271060    Objective Loss 0.271060                                        LR 0.001000    Time 0.076014    
Epoch: [16][  140/  207]    Overall Loss 0.271467    Objective Loss 0.271467                                        LR 0.001000    Time 0.075774    
Epoch: [16][  150/  207]    Overall Loss 0.269177    Objective Loss 0.269177                                        LR 0.001000    Time 0.075528    
Epoch: [16][  160/  207]    Overall Loss 0.267935    Objective Loss 0.267935                                        LR 0.001000    Time 0.075268    
Epoch: [16][  170/  207]    Overall Loss 0.269269    Objective Loss 0.269269                                        LR 0.001000    Time 0.075185    
Epoch: [16][  180/  207]    Overall Loss 0.269224    Objective Loss 0.269224                                        LR 0.001000    Time 0.075029    
Epoch: [16][  190/  207]    Overall Loss 0.268856    Objective Loss 0.268856                                        LR 0.001000    Time 0.074978    
Epoch: [16][  200/  207]    Overall Loss 0.269226    Objective Loss 0.269226                                        LR 0.001000    Time 0.074915    
Epoch: [16][  207/  207]    Overall Loss 0.269548    Objective Loss 0.269548    Top1 83.918460    Top5 99.773499    LR 0.001000    Time 0.074686    
--- validate (epoch=16)-----------
5136 samples (512 per mini-batch)
Epoch: [16][   10/   11]    Loss 0.511894    Top1 75.527344    Top5 99.472656    
Epoch: [16][   11/   11]    Loss 0.511468    Top1 75.525701    Top5 99.474299    
==> Top1: 75.526    Top5: 99.474    Loss: 0.511

==> Confusion:
[[239   2  11   1   4  30   5   8]
 [  1 227  60   1   0   8   0   3]
 [  0  14 270   1   1  12   0   2]
 [  0   4   1 685 127   8  12   0]
 [  0   0   0  31 810   9  26   3]
 [  5   4  11  12  32 767  60   3]
 [  2   0   0   6  34  12 778   5]
 [ 43  19  26  89 115 294 100 103]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [17][   10/  207]    Overall Loss 0.267886    Objective Loss 0.267886                                        LR 0.001000    Time 0.129010    
Epoch: [17][   20/  207]    Overall Loss 0.276784    Objective Loss 0.276784                                        LR 0.001000    Time 0.099940    
Epoch: [17][   30/  207]    Overall Loss 0.263090    Objective Loss 0.263090                                        LR 0.001000    Time 0.090609    
Epoch: [17][   40/  207]    Overall Loss 0.259204    Objective Loss 0.259204                                        LR 0.001000    Time 0.086130    
Epoch: [17][   50/  207]    Overall Loss 0.261765    Objective Loss 0.261765                                        LR 0.001000    Time 0.083061    
Epoch: [17][   60/  207]    Overall Loss 0.259998    Objective Loss 0.259998                                        LR 0.001000    Time 0.081257    
Epoch: [17][   70/  207]    Overall Loss 0.257289    Objective Loss 0.257289                                        LR 0.001000    Time 0.079993    
Epoch: [17][   80/  207]    Overall Loss 0.257394    Objective Loss 0.257394                                        LR 0.001000    Time 0.079246    
Epoch: [17][   90/  207]    Overall Loss 0.262155    Objective Loss 0.262155                                        LR 0.001000    Time 0.078309    
Epoch: [17][  100/  207]    Overall Loss 0.264065    Objective Loss 0.264065                                        LR 0.001000    Time 0.077592    
Epoch: [17][  110/  207]    Overall Loss 0.261878    Objective Loss 0.261878                                        LR 0.001000    Time 0.077034    
Epoch: [17][  120/  207]    Overall Loss 0.262174    Objective Loss 0.262174                                        LR 0.001000    Time 0.076461    
Epoch: [17][  130/  207]    Overall Loss 0.263404    Objective Loss 0.263404                                        LR 0.001000    Time 0.076098    
Epoch: [17][  140/  207]    Overall Loss 0.264743    Objective Loss 0.264743                                        LR 0.001000    Time 0.075667    
Epoch: [17][  150/  207]    Overall Loss 0.266683    Objective Loss 0.266683                                        LR 0.001000    Time 0.075361    
Epoch: [17][  160/  207]    Overall Loss 0.266572    Objective Loss 0.266572                                        LR 0.001000    Time 0.075117    
Epoch: [17][  170/  207]    Overall Loss 0.267018    Objective Loss 0.267018                                        LR 0.001000    Time 0.075017    
Epoch: [17][  180/  207]    Overall Loss 0.268064    Objective Loss 0.268064                                        LR 0.001000    Time 0.074969    
Epoch: [17][  190/  207]    Overall Loss 0.268408    Objective Loss 0.268408                                        LR 0.001000    Time 0.074862    
Epoch: [17][  200/  207]    Overall Loss 0.268298    Objective Loss 0.268298                                        LR 0.001000    Time 0.074684    
Epoch: [17][  207/  207]    Overall Loss 0.267725    Objective Loss 0.267725    Top1 89.127973    Top5 99.773499    LR 0.001000    Time 0.074434    
--- validate (epoch=17)-----------
5136 samples (512 per mini-batch)
Epoch: [17][   10/   11]    Loss 0.410365    Top1 76.816406    Top5 99.707031    
Epoch: [17][   11/   11]    Loss 0.444480    Top1 76.830218    Top5 99.707944    
==> Top1: 76.830    Top5: 99.708    Loss: 0.444

==> Confusion:
[[274   7   4   0   1   5   0   9]
 [  5 259  33   0   0   1   0   2]
 [  8  34 253   0   0   2   0   3]
 [  1   4   0 703  99  19   9   2]
 [  3   0   0  43 781  19  23  10]
 [ 30   6  16  11  18 777  29   7]
 [  5   0   0   4  40  22 751  15]
 [ 96  42  24  74  79 271  55 148]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [18][   10/  207]    Overall Loss 0.243582    Objective Loss 0.243582                                        LR 0.001000    Time 0.128438    
Epoch: [18][   20/  207]    Overall Loss 0.231455    Objective Loss 0.231455                                        LR 0.001000    Time 0.099974    
Epoch: [18][   30/  207]    Overall Loss 0.227363    Objective Loss 0.227363                                        LR 0.001000    Time 0.090608    
Epoch: [18][   40/  207]    Overall Loss 0.230836    Objective Loss 0.230836                                        LR 0.001000    Time 0.086127    
Epoch: [18][   50/  207]    Overall Loss 0.228919    Objective Loss 0.228919                                        LR 0.001000    Time 0.083270    
Epoch: [18][   60/  207]    Overall Loss 0.229228    Objective Loss 0.229228                                        LR 0.001000    Time 0.081261    
Epoch: [18][   70/  207]    Overall Loss 0.231489    Objective Loss 0.231489                                        LR 0.001000    Time 0.079948    
Epoch: [18][   80/  207]    Overall Loss 0.237213    Objective Loss 0.237213                                        LR 0.001000    Time 0.078730    
Epoch: [18][   90/  207]    Overall Loss 0.237555    Objective Loss 0.237555                                        LR 0.001000    Time 0.077961    
Epoch: [18][  100/  207]    Overall Loss 0.237266    Objective Loss 0.237266                                        LR 0.001000    Time 0.077485    
Epoch: [18][  110/  207]    Overall Loss 0.237198    Objective Loss 0.237198                                        LR 0.001000    Time 0.076817    
Epoch: [18][  120/  207]    Overall Loss 0.236641    Objective Loss 0.236641                                        LR 0.001000    Time 0.076447    
Epoch: [18][  130/  207]    Overall Loss 0.236457    Objective Loss 0.236457                                        LR 0.001000    Time 0.076005    
Epoch: [18][  140/  207]    Overall Loss 0.238856    Objective Loss 0.238856                                        LR 0.001000    Time 0.075843    
Epoch: [18][  150/  207]    Overall Loss 0.239765    Objective Loss 0.239765                                        LR 0.001000    Time 0.075560    
Epoch: [18][  160/  207]    Overall Loss 0.242236    Objective Loss 0.242236                                        LR 0.001000    Time 0.075319    
Epoch: [18][  170/  207]    Overall Loss 0.245223    Objective Loss 0.245223                                        LR 0.001000    Time 0.075009    
Epoch: [18][  180/  207]    Overall Loss 0.245896    Objective Loss 0.245896                                        LR 0.001000    Time 0.074784    
Epoch: [18][  190/  207]    Overall Loss 0.246739    Objective Loss 0.246739                                        LR 0.001000    Time 0.074651    
Epoch: [18][  200/  207]    Overall Loss 0.249254    Objective Loss 0.249254                                        LR 0.001000    Time 0.074407    
Epoch: [18][  207/  207]    Overall Loss 0.249314    Objective Loss 0.249314    Top1 84.258211    Top5 99.773499    LR 0.001000    Time 0.074184    
--- validate (epoch=18)-----------
5136 samples (512 per mini-batch)
Epoch: [18][   10/   11]    Loss 0.443400    Top1 76.933594    Top5 99.433594    
Epoch: [18][   11/   11]    Loss 0.461887    Top1 76.927570    Top5 99.435358    
==> Top1: 76.928    Top5: 99.435    Loss: 0.462

==> Confusion:
[[260  10  12   0   0   5   1  12]
 [  2 270  26   0   0   1   0   1]
 [  0  34 265   0   0   0   0   1]
 [  0   7   1 701  85  14  20   9]
 [  5   0   0  42 755  11  51  15]
 [ 15   9  35  11  19 752  37  16]
 [  4   0   0   3  25  21 773  11]
 [ 61  54  55  79  71 219  75 175]]

==> Best [Top1: 77.239   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 15]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [19][   10/  207]    Overall Loss 0.231804    Objective Loss 0.231804                                        LR 0.001000    Time 0.129863    
Epoch: [19][   20/  207]    Overall Loss 0.238639    Objective Loss 0.238639                                        LR 0.001000    Time 0.101194    
Epoch: [19][   30/  207]    Overall Loss 0.234766    Objective Loss 0.234766                                        LR 0.001000    Time 0.091692    
Epoch: [19][   40/  207]    Overall Loss 0.231077    Objective Loss 0.231077                                        LR 0.001000    Time 0.086712    
Epoch: [19][   50/  207]    Overall Loss 0.231474    Objective Loss 0.231474                                        LR 0.001000    Time 0.083415    
Epoch: [19][   60/  207]    Overall Loss 0.230370    Objective Loss 0.230370                                        LR 0.001000    Time 0.081777    
Epoch: [19][   70/  207]    Overall Loss 0.232002    Objective Loss 0.232002                                        LR 0.001000    Time 0.080253    
Epoch: [19][   80/  207]    Overall Loss 0.235333    Objective Loss 0.235333                                        LR 0.001000    Time 0.079104    
Epoch: [19][   90/  207]    Overall Loss 0.236265    Objective Loss 0.236265                                        LR 0.001000    Time 0.078139    
Epoch: [19][  100/  207]    Overall Loss 0.240644    Objective Loss 0.240644                                        LR 0.001000    Time 0.077544    
Epoch: [19][  110/  207]    Overall Loss 0.242354    Objective Loss 0.242354                                        LR 0.001000    Time 0.077162    
Epoch: [19][  120/  207]    Overall Loss 0.239068    Objective Loss 0.239068                                        LR 0.001000    Time 0.076811    
Epoch: [19][  130/  207]    Overall Loss 0.237839    Objective Loss 0.237839                                        LR 0.001000    Time 0.076480    
Epoch: [19][  140/  207]    Overall Loss 0.239438    Objective Loss 0.239438                                        LR 0.001000    Time 0.076096    
Epoch: [19][  150/  207]    Overall Loss 0.239193    Objective Loss 0.239193                                        LR 0.001000    Time 0.075874    
Epoch: [19][  160/  207]    Overall Loss 0.239025    Objective Loss 0.239025                                        LR 0.001000    Time 0.075588    
Epoch: [19][  170/  207]    Overall Loss 0.240705    Objective Loss 0.240705                                        LR 0.001000    Time 0.075246    
Epoch: [19][  180/  207]    Overall Loss 0.240435    Objective Loss 0.240435                                        LR 0.001000    Time 0.075074    
Epoch: [19][  190/  207]    Overall Loss 0.240489    Objective Loss 0.240489                                        LR 0.001000    Time 0.074945    
Epoch: [19][  200/  207]    Overall Loss 0.241032    Objective Loss 0.241032                                        LR 0.001000    Time 0.074836    
Epoch: [19][  207/  207]    Overall Loss 0.241791    Objective Loss 0.241791    Top1 89.580974    Top5 99.886750    LR 0.001000    Time 0.074614    
--- validate (epoch=19)-----------
5136 samples (512 per mini-batch)
Epoch: [19][   10/   11]    Loss 0.471617    Top1 78.632812    Top5 99.550781    
Epoch: [19][   11/   11]    Loss 0.456836    Top1 78.621495    Top5 99.552181    
==> Top1: 78.621    Top5: 99.552    Loss: 0.457

==> Confusion:
[[276   5   3   0   1   1   0  14]
 [  7 255  29   1   0   1   0   7]
 [  5  24 261   1   0   3   0   6]
 [  1   5   1 737  51  12  17  13]
 [  4   3   0  63 730  16  37  26]
 [ 23  10  15  15  18 761  27  25]
 [  5   0   0   8  21  23 759  21]
 [ 79  29  26  86  53 197  60 259]]

==> Best [Top1: 78.621   Top5: 99.552   Sparsity:0.00   Params: 117200 on epoch: 19]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [20][   10/  207]    Overall Loss 0.194061    Objective Loss 0.194061                                        LR 0.001000    Time 0.128425    
Epoch: [20][   20/  207]    Overall Loss 0.210630    Objective Loss 0.210630                                        LR 0.001000    Time 0.099915    
Epoch: [20][   30/  207]    Overall Loss 0.214299    Objective Loss 0.214299                                        LR 0.001000    Time 0.090716    
Epoch: [20][   40/  207]    Overall Loss 0.223578    Objective Loss 0.223578                                        LR 0.001000    Time 0.085554    
Epoch: [20][   50/  207]    Overall Loss 0.227937    Objective Loss 0.227937                                        LR 0.001000    Time 0.082398    
Epoch: [20][   60/  207]    Overall Loss 0.228952    Objective Loss 0.228952                                        LR 0.001000    Time 0.080652    
Epoch: [20][   70/  207]    Overall Loss 0.230667    Objective Loss 0.230667                                        LR 0.001000    Time 0.079301    
Epoch: [20][   80/  207]    Overall Loss 0.230956    Objective Loss 0.230956                                        LR 0.001000    Time 0.078347    
Epoch: [20][   90/  207]    Overall Loss 0.229086    Objective Loss 0.229086                                        LR 0.001000    Time 0.077746    
Epoch: [20][  100/  207]    Overall Loss 0.228147    Objective Loss 0.228147                                        LR 0.001000    Time 0.077088    
Epoch: [20][  110/  207]    Overall Loss 0.228118    Objective Loss 0.228118                                        LR 0.001000    Time 0.076444    
Epoch: [20][  120/  207]    Overall Loss 0.232718    Objective Loss 0.232718                                        LR 0.001000    Time 0.076074    
Epoch: [20][  130/  207]    Overall Loss 0.233846    Objective Loss 0.233846                                        LR 0.001000    Time 0.075732    
Epoch: [20][  140/  207]    Overall Loss 0.234446    Objective Loss 0.234446                                        LR 0.001000    Time 0.075326    
Epoch: [20][  150/  207]    Overall Loss 0.234488    Objective Loss 0.234488                                        LR 0.001000    Time 0.075041    
Epoch: [20][  160/  207]    Overall Loss 0.238344    Objective Loss 0.238344                                        LR 0.001000    Time 0.074818    
Epoch: [20][  170/  207]    Overall Loss 0.238853    Objective Loss 0.238853                                        LR 0.001000    Time 0.074683    
Epoch: [20][  180/  207]    Overall Loss 0.238453    Objective Loss 0.238453                                        LR 0.001000    Time 0.074523    
Epoch: [20][  190/  207]    Overall Loss 0.238292    Objective Loss 0.238292                                        LR 0.001000    Time 0.074338    
Epoch: [20][  200/  207]    Overall Loss 0.237836    Objective Loss 0.237836                                        LR 0.001000    Time 0.074147    
Epoch: [20][  207/  207]    Overall Loss 0.237395    Objective Loss 0.237395    Top1 89.807475    Top5 99.773499    LR 0.001000    Time 0.073950    
--- validate (epoch=20)-----------
5136 samples (512 per mini-batch)
Epoch: [20][   10/   11]    Loss 0.494830    Top1 78.710938    Top5 99.511719    
Epoch: [20][   11/   11]    Loss 0.586379    Top1 78.699377    Top5 99.513240    
==> Top1: 78.699    Top5: 99.513    Loss: 0.586

==> Confusion:
[[247  16   4   0   1  11   1  20]
 [  2 281  12   0   0   1   0   4]
 [  2  51 242   0   1   1   0   3]
 [  0   6   0 737  64  11   9  10]
 [  2   2   0  64 772   7  17  15]
 [  9  10  14  14  28 761  30  28]
 [  3   0   0   7  43  17 750  17]
 [ 42  53  22  90  80 208  42 252]]

==> Best [Top1: 78.699   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 20]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [21][   10/  207]    Overall Loss 0.230572    Objective Loss 0.230572                                        LR 0.001000    Time 0.129943    
Epoch: [21][   20/  207]    Overall Loss 0.235101    Objective Loss 0.235101                                        LR 0.001000    Time 0.100332    
Epoch: [21][   30/  207]    Overall Loss 0.229601    Objective Loss 0.229601                                        LR 0.001000    Time 0.090339    
Epoch: [21][   40/  207]    Overall Loss 0.231632    Objective Loss 0.231632                                        LR 0.001000    Time 0.086297    
Epoch: [21][   50/  207]    Overall Loss 0.228684    Objective Loss 0.228684                                        LR 0.001000    Time 0.083391    
Epoch: [21][   60/  207]    Overall Loss 0.228142    Objective Loss 0.228142                                        LR 0.001000    Time 0.081833    
Epoch: [21][   70/  207]    Overall Loss 0.229269    Objective Loss 0.229269                                        LR 0.001000    Time 0.080571    
Epoch: [21][   80/  207]    Overall Loss 0.228898    Objective Loss 0.228898                                        LR 0.001000    Time 0.079571    
Epoch: [21][   90/  207]    Overall Loss 0.230985    Objective Loss 0.230985                                        LR 0.001000    Time 0.079053    
Epoch: [21][  100/  207]    Overall Loss 0.233009    Objective Loss 0.233009                                        LR 0.001000    Time 0.080405    
Epoch: [21][  110/  207]    Overall Loss 0.234950    Objective Loss 0.234950                                        LR 0.001000    Time 0.081006    
Epoch: [21][  120/  207]    Overall Loss 0.236862    Objective Loss 0.236862                                        LR 0.001000    Time 0.080383    
Epoch: [21][  130/  207]    Overall Loss 0.235303    Objective Loss 0.235303                                        LR 0.001000    Time 0.079959    
Epoch: [21][  140/  207]    Overall Loss 0.238090    Objective Loss 0.238090                                        LR 0.001000    Time 0.079576    
Epoch: [21][  150/  207]    Overall Loss 0.239635    Objective Loss 0.239635                                        LR 0.001000    Time 0.079186    
Epoch: [21][  160/  207]    Overall Loss 0.239640    Objective Loss 0.239640                                        LR 0.001000    Time 0.078683    
Epoch: [21][  170/  207]    Overall Loss 0.239143    Objective Loss 0.239143                                        LR 0.001000    Time 0.078310    
Epoch: [21][  180/  207]    Overall Loss 0.238466    Objective Loss 0.238466                                        LR 0.001000    Time 0.077912    
Epoch: [21][  190/  207]    Overall Loss 0.238222    Objective Loss 0.238222                                        LR 0.001000    Time 0.077560    
Epoch: [21][  200/  207]    Overall Loss 0.238371    Objective Loss 0.238371                                        LR 0.001000    Time 0.077297    
Epoch: [21][  207/  207]    Overall Loss 0.238441    Objective Loss 0.238441    Top1 83.691959    Top5 100.000000    LR 0.001000    Time 0.076973    
--- validate (epoch=21)-----------
5136 samples (512 per mini-batch)
Epoch: [21][   10/   11]    Loss 0.517648    Top1 76.933594    Top5 99.550781    
Epoch: [21][   11/   11]    Loss 0.477283    Top1 76.966511    Top5 99.552181    
==> Top1: 76.967    Top5: 99.552    Loss: 0.477

==> Confusion:
[[246   6  10   3   3  11   4  17]
 [  2 238  50   4   0   2   1   3]
 [  1  20 266   2   0   7   0   4]
 [  0   3   1 729  93   3   6   2]
 [  0   0   0  39 814   4  17   5]
 [  3   1  12  39  37 737  54  11]
 [  2   0   0   8  53  10 755   9]
 [ 41  18  27 138 105 217  75 168]]

==> Best [Top1: 78.699   Top5: 99.513   Sparsity:0.00   Params: 117200 on epoch: 20]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [22][   10/  207]    Overall Loss 0.253451    Objective Loss 0.253451                                        LR 0.001000    Time 0.128762    
Epoch: [22][   20/  207]    Overall Loss 0.237993    Objective Loss 0.237993                                        LR 0.001000    Time 0.099877    
Epoch: [22][   30/  207]    Overall Loss 0.232718    Objective Loss 0.232718                                        LR 0.001000    Time 0.090171    
Epoch: [22][   40/  207]    Overall Loss 0.228314    Objective Loss 0.228314                                        LR 0.001000    Time 0.085493    
Epoch: [22][   50/  207]    Overall Loss 0.225994    Objective Loss 0.225994                                        LR 0.001000    Time 0.082431    
Epoch: [22][   60/  207]    Overall Loss 0.226683    Objective Loss 0.226683                                        LR 0.001000    Time 0.080840    
Epoch: [22][   70/  207]    Overall Loss 0.222447    Objective Loss 0.222447                                        LR 0.001000    Time 0.079494    
Epoch: [22][   80/  207]    Overall Loss 0.221618    Objective Loss 0.221618                                        LR 0.001000    Time 0.078442    
Epoch: [22][   90/  207]    Overall Loss 0.219262    Objective Loss 0.219262                                        LR 0.001000    Time 0.077826    
Epoch: [22][  100/  207]    Overall Loss 0.219112    Objective Loss 0.219112                                        LR 0.001000    Time 0.077572    
Epoch: [22][  110/  207]    Overall Loss 0.218069    Objective Loss 0.218069                                        LR 0.001000    Time 0.077317    
Epoch: [22][  120/  207]    Overall Loss 0.220238    Objective Loss 0.220238                                        LR 0.001000    Time 0.077009    
Epoch: [22][  130/  207]    Overall Loss 0.222199    Objective Loss 0.222199                                        LR 0.001000    Time 0.076749    
Epoch: [22][  140/  207]    Overall Loss 0.224329    Objective Loss 0.224329                                        LR 0.001000    Time 0.076566    
Epoch: [22][  150/  207]    Overall Loss 0.224568    Objective Loss 0.224568                                        LR 0.001000    Time 0.076379    
Epoch: [22][  160/  207]    Overall Loss 0.224839    Objective Loss 0.224839                                        LR 0.001000    Time 0.075940    
Epoch: [22][  170/  207]    Overall Loss 0.224877    Objective Loss 0.224877                                        LR 0.001000    Time 0.076160    
Epoch: [22][  180/  207]    Overall Loss 0.224430    Objective Loss 0.224430                                        LR 0.001000    Time 0.078836    
Epoch: [22][  190/  207]    Overall Loss 0.224569    Objective Loss 0.224569                                        LR 0.001000    Time 0.079492    
Epoch: [22][  200/  207]    Overall Loss 0.225544    Objective Loss 0.225544                                        LR 0.001000    Time 0.079926    
Epoch: [22][  207/  207]    Overall Loss 0.224488    Objective Loss 0.224488    Top1 91.166478    Top5 99.886750    LR 0.001000    Time 0.079932    
--- validate (epoch=22)-----------
5136 samples (512 per mini-batch)
Epoch: [22][   10/   11]    Loss 0.425537    Top1 79.492188    Top5 99.667969    
Epoch: [22][   11/   11]    Loss 0.419203    Top1 79.458723    Top5 99.669003    
==> Top1: 79.459    Top5: 99.669    Loss: 0.419

==> Confusion:
[[272   5   6   0   0   5   1  11]
 [  5 253  37   0   0   1   0   4]
 [  2  21 271   0   1   2   0   3]
 [  0   6   2 714  81  14   9  11]
 [  5   0   0  33 791  10  21  19]
 [ 22   7  15   7  25 792  12  14]
 [  5   0   0   5  39  20 746  22]
 [ 73  30  26  51  91 240  36 242]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [23][   10/  207]    Overall Loss 0.198285    Objective Loss 0.198285                                        LR 0.001000    Time 0.151575    
Epoch: [23][   20/  207]    Overall Loss 0.216461    Objective Loss 0.216461                                        LR 0.001000    Time 0.114702    
Epoch: [23][   30/  207]    Overall Loss 0.211884    Objective Loss 0.211884                                        LR 0.001000    Time 0.102661    
Epoch: [23][   40/  207]    Overall Loss 0.205376    Objective Loss 0.205376                                        LR 0.001000    Time 0.096004    
Epoch: [23][   50/  207]    Overall Loss 0.203608    Objective Loss 0.203608                                        LR 0.001000    Time 0.091363    
Epoch: [23][   60/  207]    Overall Loss 0.199263    Objective Loss 0.199263                                        LR 0.001000    Time 0.088087    
Epoch: [23][   70/  207]    Overall Loss 0.200283    Objective Loss 0.200283                                        LR 0.001000    Time 0.086152    
Epoch: [23][   80/  207]    Overall Loss 0.204318    Objective Loss 0.204318                                        LR 0.001000    Time 0.084375    
Epoch: [23][   90/  207]    Overall Loss 0.208070    Objective Loss 0.208070                                        LR 0.001000    Time 0.082838    
Epoch: [23][  100/  207]    Overall Loss 0.209383    Objective Loss 0.209383                                        LR 0.001000    Time 0.081834    
Epoch: [23][  110/  207]    Overall Loss 0.208898    Objective Loss 0.208898                                        LR 0.001000    Time 0.081082    
Epoch: [23][  120/  207]    Overall Loss 0.210184    Objective Loss 0.210184                                        LR 0.001000    Time 0.081696    
Epoch: [23][  130/  207]    Overall Loss 0.212338    Objective Loss 0.212338                                        LR 0.001000    Time 0.081335    
Epoch: [23][  140/  207]    Overall Loss 0.209918    Objective Loss 0.209918                                        LR 0.001000    Time 0.080674    
Epoch: [23][  150/  207]    Overall Loss 0.208876    Objective Loss 0.208876                                        LR 0.001000    Time 0.080012    
Epoch: [23][  160/  207]    Overall Loss 0.207498    Objective Loss 0.207498                                        LR 0.001000    Time 0.079545    
Epoch: [23][  170/  207]    Overall Loss 0.208255    Objective Loss 0.208255                                        LR 0.001000    Time 0.079166    
Epoch: [23][  180/  207]    Overall Loss 0.209358    Objective Loss 0.209358                                        LR 0.001000    Time 0.078767    
Epoch: [23][  190/  207]    Overall Loss 0.209486    Objective Loss 0.209486                                        LR 0.001000    Time 0.078405    
Epoch: [23][  200/  207]    Overall Loss 0.210044    Objective Loss 0.210044                                        LR 0.001000    Time 0.078068    
Epoch: [23][  207/  207]    Overall Loss 0.210716    Objective Loss 0.210716    Top1 92.298981    Top5 100.000000    LR 0.001000    Time 0.077728    
--- validate (epoch=23)-----------
5136 samples (512 per mini-batch)
Epoch: [23][   10/   11]    Loss 0.458988    Top1 77.949219    Top5 99.375000    
Epoch: [23][   11/   11]    Loss 0.429378    Top1 77.978972    Top5 99.376947    
==> Top1: 77.979    Top5: 99.377    Loss: 0.429

==> Confusion:
[[266   4  10   0   0  12   0   8]
 [  3 243  48   2   0   4   0   0]
 [  0  19 275   1   0   5   0   0]
 [  0   4   1 719  89  18   5   1]
 [  2   0   1  32 807  23   7   7]
 [  5   7  11   8  22 828   9   4]
 [  2   0   0   6  56  37 726  10]
 [ 47  27  37  67 110 327  33 141]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [24][   10/  207]    Overall Loss 0.200254    Objective Loss 0.200254                                        LR 0.001000    Time 0.129531    
Epoch: [24][   20/  207]    Overall Loss 0.198802    Objective Loss 0.198802                                        LR 0.001000    Time 0.101593    
Epoch: [24][   30/  207]    Overall Loss 0.191094    Objective Loss 0.191094                                        LR 0.001000    Time 0.092105    
Epoch: [24][   40/  207]    Overall Loss 0.183423    Objective Loss 0.183423                                        LR 0.001000    Time 0.087137    
Epoch: [24][   50/  207]    Overall Loss 0.180038    Objective Loss 0.180038                                        LR 0.001000    Time 0.084472    
Epoch: [24][   60/  207]    Overall Loss 0.179304    Objective Loss 0.179304                                        LR 0.001000    Time 0.082485    
Epoch: [24][   70/  207]    Overall Loss 0.181958    Objective Loss 0.181958                                        LR 0.001000    Time 0.081132    
Epoch: [24][   80/  207]    Overall Loss 0.182671    Objective Loss 0.182671                                        LR 0.001000    Time 0.080003    
Epoch: [24][   90/  207]    Overall Loss 0.182262    Objective Loss 0.182262                                        LR 0.001000    Time 0.079339    
Epoch: [24][  100/  207]    Overall Loss 0.186201    Objective Loss 0.186201                                        LR 0.001000    Time 0.078730    
Epoch: [24][  110/  207]    Overall Loss 0.191145    Objective Loss 0.191145                                        LR 0.001000    Time 0.078297    
Epoch: [24][  120/  207]    Overall Loss 0.194076    Objective Loss 0.194076                                        LR 0.001000    Time 0.077817    
Epoch: [24][  130/  207]    Overall Loss 0.195612    Objective Loss 0.195612                                        LR 0.001000    Time 0.077474    
Epoch: [24][  140/  207]    Overall Loss 0.197709    Objective Loss 0.197709                                        LR 0.001000    Time 0.077024    
Epoch: [24][  150/  207]    Overall Loss 0.198956    Objective Loss 0.198956                                        LR 0.001000    Time 0.076591    
Epoch: [24][  160/  207]    Overall Loss 0.201845    Objective Loss 0.201845                                        LR 0.001000    Time 0.076363    
Epoch: [24][  170/  207]    Overall Loss 0.203189    Objective Loss 0.203189                                        LR 0.001000    Time 0.076657    
Epoch: [24][  180/  207]    Overall Loss 0.203459    Objective Loss 0.203459                                        LR 0.001000    Time 0.077333    
Epoch: [24][  190/  207]    Overall Loss 0.204666    Objective Loss 0.204666                                        LR 0.001000    Time 0.077146    
Epoch: [24][  200/  207]    Overall Loss 0.206823    Objective Loss 0.206823                                        LR 0.001000    Time 0.076836    
Epoch: [24][  207/  207]    Overall Loss 0.207128    Objective Loss 0.207128    Top1 92.072480    Top5 100.000000    LR 0.001000    Time 0.076549    
--- validate (epoch=24)-----------
5136 samples (512 per mini-batch)
Epoch: [24][   10/   11]    Loss 0.458914    Top1 78.906250    Top5 99.843750    
Epoch: [24][   11/   11]    Loss 0.434045    Top1 78.874611    Top5 99.844237    
==> Top1: 78.875    Top5: 99.844    Loss: 0.434

==> Confusion:
[[275   1   5   0   1   2   2  14]
 [  5 236  52   0   0   1   0   6]
 [  6  15 273   0   0   1   0   5]
 [  0   4   1 730  62   9  23   8]
 [  2   0   0  58 763   7  26  23]
 [ 29   5  21  15  24 757  21  22]
 [  4   1   0   4  29  18 763  18]
 [ 77  26  38  71  80 179  64 254]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [25][   10/  207]    Overall Loss 0.203687    Objective Loss 0.203687                                        LR 0.001000    Time 0.128237    
Epoch: [25][   20/  207]    Overall Loss 0.211217    Objective Loss 0.211217                                        LR 0.001000    Time 0.099848    
Epoch: [25][   30/  207]    Overall Loss 0.209763    Objective Loss 0.209763                                        LR 0.001000    Time 0.090102    
Epoch: [25][   40/  207]    Overall Loss 0.213729    Objective Loss 0.213729                                        LR 0.001000    Time 0.085365    
Epoch: [25][   50/  207]    Overall Loss 0.215228    Objective Loss 0.215228                                        LR 0.001000    Time 0.082864    
Epoch: [25][   60/  207]    Overall Loss 0.211034    Objective Loss 0.211034                                        LR 0.001000    Time 0.081245    
Epoch: [25][   70/  207]    Overall Loss 0.212838    Objective Loss 0.212838                                        LR 0.001000    Time 0.079754    
Epoch: [25][   80/  207]    Overall Loss 0.213788    Objective Loss 0.213788                                        LR 0.001000    Time 0.079041    
Epoch: [25][   90/  207]    Overall Loss 0.213433    Objective Loss 0.213433                                        LR 0.001000    Time 0.078197    
Epoch: [25][  100/  207]    Overall Loss 0.213220    Objective Loss 0.213220                                        LR 0.001000    Time 0.077370    
Epoch: [25][  110/  207]    Overall Loss 0.213106    Objective Loss 0.213106                                        LR 0.001000    Time 0.076872    
Epoch: [25][  120/  207]    Overall Loss 0.211872    Objective Loss 0.211872                                        LR 0.001000    Time 0.076430    
Epoch: [25][  130/  207]    Overall Loss 0.210049    Objective Loss 0.210049                                        LR 0.001000    Time 0.076169    
Epoch: [25][  140/  207]    Overall Loss 0.210579    Objective Loss 0.210579                                        LR 0.001000    Time 0.075994    
Epoch: [25][  150/  207]    Overall Loss 0.209692    Objective Loss 0.209692                                        LR 0.001000    Time 0.075803    
Epoch: [25][  160/  207]    Overall Loss 0.209079    Objective Loss 0.209079                                        LR 0.001000    Time 0.075723    
Epoch: [25][  170/  207]    Overall Loss 0.209297    Objective Loss 0.209297                                        LR 0.001000    Time 0.075418    
Epoch: [25][  180/  207]    Overall Loss 0.209232    Objective Loss 0.209232                                        LR 0.001000    Time 0.075155    
Epoch: [25][  190/  207]    Overall Loss 0.209723    Objective Loss 0.209723                                        LR 0.001000    Time 0.075123    
Epoch: [25][  200/  207]    Overall Loss 0.208570    Objective Loss 0.208570                                        LR 0.001000    Time 0.074971    
Epoch: [25][  207/  207]    Overall Loss 0.208682    Objective Loss 0.208682    Top1 88.674972    Top5 99.773499    LR 0.001000    Time 0.074757    
--- validate (epoch=25)-----------
5136 samples (512 per mini-batch)
Epoch: [25][   10/   11]    Loss 0.484194    Top1 78.398438    Top5 99.667969    
Epoch: [25][   11/   11]    Loss 0.503791    Top1 78.368380    Top5 99.669003    
==> Top1: 78.368    Top5: 99.669    Loss: 0.504

==> Confusion:
[[266   3   4   3   2   5   4  13]
 [  4 261  25   3   0   1   0   6]
 [  2  25 256   2   1   3   0  11]
 [  0   2   0 752  74   7   2   0]
 [  2   0   0  66 785   4  10  12]
 [  9   5  12  37  35 753  23  20]
 [  2   0   0   7  72  15 726  15]
 [ 45  23  17 128  99 191  60 226]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [26][   10/  207]    Overall Loss 0.220929    Objective Loss 0.220929                                        LR 0.001000    Time 0.127341    
Epoch: [26][   20/  207]    Overall Loss 0.220266    Objective Loss 0.220266                                        LR 0.001000    Time 0.098927    
Epoch: [26][   30/  207]    Overall Loss 0.213934    Objective Loss 0.213934                                        LR 0.001000    Time 0.089892    
Epoch: [26][   40/  207]    Overall Loss 0.210324    Objective Loss 0.210324                                        LR 0.001000    Time 0.085408    
Epoch: [26][   50/  207]    Overall Loss 0.211799    Objective Loss 0.211799                                        LR 0.001000    Time 0.082466    
Epoch: [26][   60/  207]    Overall Loss 0.212058    Objective Loss 0.212058                                        LR 0.001000    Time 0.080281    
Epoch: [26][   70/  207]    Overall Loss 0.209950    Objective Loss 0.209950                                        LR 0.001000    Time 0.079142    
Epoch: [26][   80/  207]    Overall Loss 0.210956    Objective Loss 0.210956                                        LR 0.001000    Time 0.078516    
Epoch: [26][   90/  207]    Overall Loss 0.211492    Objective Loss 0.211492                                        LR 0.001000    Time 0.077763    
Epoch: [26][  100/  207]    Overall Loss 0.208065    Objective Loss 0.208065                                        LR 0.001000    Time 0.076960    
Epoch: [26][  110/  207]    Overall Loss 0.206233    Objective Loss 0.206233                                        LR 0.001000    Time 0.076382    
Epoch: [26][  120/  207]    Overall Loss 0.206345    Objective Loss 0.206345                                        LR 0.001000    Time 0.075994    
Epoch: [26][  130/  207]    Overall Loss 0.205409    Objective Loss 0.205409                                        LR 0.001000    Time 0.075687    
Epoch: [26][  140/  207]    Overall Loss 0.204303    Objective Loss 0.204303                                        LR 0.001000    Time 0.075287    
Epoch: [26][  150/  207]    Overall Loss 0.202096    Objective Loss 0.202096                                        LR 0.001000    Time 0.074999    
Epoch: [26][  160/  207]    Overall Loss 0.200035    Objective Loss 0.200035                                        LR 0.001000    Time 0.074711    
Epoch: [26][  170/  207]    Overall Loss 0.198249    Objective Loss 0.198249                                        LR 0.001000    Time 0.074563    
Epoch: [26][  180/  207]    Overall Loss 0.197521    Objective Loss 0.197521                                        LR 0.001000    Time 0.074402    
Epoch: [26][  190/  207]    Overall Loss 0.195951    Objective Loss 0.195951                                        LR 0.001000    Time 0.074320    
Epoch: [26][  200/  207]    Overall Loss 0.194758    Objective Loss 0.194758                                        LR 0.001000    Time 0.074177    
Epoch: [26][  207/  207]    Overall Loss 0.195799    Objective Loss 0.195799    Top1 86.409966    Top5 100.000000    LR 0.001000    Time 0.073954    
--- validate (epoch=26)-----------
5136 samples (512 per mini-batch)
Epoch: [26][   10/   11]    Loss 0.446914    Top1 77.656250    Top5 99.375000    
Epoch: [26][   11/   11]    Loss 0.438385    Top1 77.667445    Top5 99.376947    
==> Top1: 77.667    Top5: 99.377    Loss: 0.438

==> Confusion:
[[258   9  12   1   0  13   0   7]
 [  1 248  48   0   0   3   0   0]
 [  1  22 274   0   0   3   0   0]
 [  0  11   1 717  55  29  15   9]
 [  2   3   0  48 761  36  21   8]
 [  4   7  19   5  13 835   7   4]
 [  2   2   1   4  30  38 754   6]
 [ 56  40  41  51  79 327  53 142]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [27][   10/  207]    Overall Loss 0.192822    Objective Loss 0.192822                                        LR 0.001000    Time 0.129032    
Epoch: [27][   20/  207]    Overall Loss 0.181300    Objective Loss 0.181300                                        LR 0.001000    Time 0.099868    
Epoch: [27][   30/  207]    Overall Loss 0.178457    Objective Loss 0.178457                                        LR 0.001000    Time 0.090423    
Epoch: [27][   40/  207]    Overall Loss 0.186196    Objective Loss 0.186196                                        LR 0.001000    Time 0.085535    
Epoch: [27][   50/  207]    Overall Loss 0.195752    Objective Loss 0.195752                                        LR 0.001000    Time 0.082936    
Epoch: [27][   60/  207]    Overall Loss 0.199893    Objective Loss 0.199893                                        LR 0.001000    Time 0.080823    
Epoch: [27][   70/  207]    Overall Loss 0.200191    Objective Loss 0.200191                                        LR 0.001000    Time 0.079605    
Epoch: [27][   80/  207]    Overall Loss 0.199975    Objective Loss 0.199975                                        LR 0.001000    Time 0.078714    
Epoch: [27][   90/  207]    Overall Loss 0.196008    Objective Loss 0.196008                                        LR 0.001000    Time 0.078097    
Epoch: [27][  100/  207]    Overall Loss 0.196834    Objective Loss 0.196834                                        LR 0.001000    Time 0.077546    
Epoch: [27][  110/  207]    Overall Loss 0.196749    Objective Loss 0.196749                                        LR 0.001000    Time 0.076941    
Epoch: [27][  120/  207]    Overall Loss 0.195784    Objective Loss 0.195784                                        LR 0.001000    Time 0.076440    
Epoch: [27][  130/  207]    Overall Loss 0.194562    Objective Loss 0.194562                                        LR 0.001000    Time 0.076136    
Epoch: [27][  140/  207]    Overall Loss 0.193583    Objective Loss 0.193583                                        LR 0.001000    Time 0.075975    
Epoch: [27][  150/  207]    Overall Loss 0.192695    Objective Loss 0.192695                                        LR 0.001000    Time 0.075666    
Epoch: [27][  160/  207]    Overall Loss 0.191511    Objective Loss 0.191511                                        LR 0.001000    Time 0.075478    
Epoch: [27][  170/  207]    Overall Loss 0.191873    Objective Loss 0.191873                                        LR 0.001000    Time 0.075390    
Epoch: [27][  180/  207]    Overall Loss 0.191106    Objective Loss 0.191106                                        LR 0.001000    Time 0.075153    
Epoch: [27][  190/  207]    Overall Loss 0.190998    Objective Loss 0.190998                                        LR 0.001000    Time 0.075050    
Epoch: [27][  200/  207]    Overall Loss 0.190612    Objective Loss 0.190612                                        LR 0.001000    Time 0.074849    
Epoch: [27][  207/  207]    Overall Loss 0.191312    Objective Loss 0.191312    Top1 89.694224    Top5 99.546999    LR 0.001000    Time 0.074625    
--- validate (epoch=27)-----------
5136 samples (512 per mini-batch)
Epoch: [27][   10/   11]    Loss 0.462782    Top1 79.375000    Top5 99.687500    
Epoch: [27][   11/   11]    Loss 0.460120    Top1 79.361371    Top5 99.688474    
==> Top1: 79.361    Top5: 99.688    Loss: 0.460

==> Confusion:
[[288   3   1   0   0   0   2   6]
 [ 13 262  21   0   0   1   0   3]
 [ 10  26 255   0   0   2   0   7]
 [  2   7   0 700  71  15  20  22]
 [  6   0   0  35 767   8  36  27]
 [ 47   6  11   8  16 776   9  21]
 [  6   0   0   2  27  27 747  28]
 [102  33  17  46  56 218  36 281]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [28][   10/  207]    Overall Loss 0.174433    Objective Loss 0.174433                                        LR 0.001000    Time 0.132780    
Epoch: [28][   20/  207]    Overall Loss 0.180138    Objective Loss 0.180138                                        LR 0.001000    Time 0.102589    
Epoch: [28][   30/  207]    Overall Loss 0.178391    Objective Loss 0.178391                                        LR 0.001000    Time 0.092738    
Epoch: [28][   40/  207]    Overall Loss 0.175842    Objective Loss 0.175842                                        LR 0.001000    Time 0.087391    
Epoch: [28][   50/  207]    Overall Loss 0.175954    Objective Loss 0.175954                                        LR 0.001000    Time 0.084041    
Epoch: [28][   60/  207]    Overall Loss 0.175019    Objective Loss 0.175019                                        LR 0.001000    Time 0.082117    
Epoch: [28][   70/  207]    Overall Loss 0.174044    Objective Loss 0.174044                                        LR 0.001000    Time 0.080723    
Epoch: [28][   80/  207]    Overall Loss 0.173875    Objective Loss 0.173875                                        LR 0.001000    Time 0.079534    
Epoch: [28][   90/  207]    Overall Loss 0.171418    Objective Loss 0.171418                                        LR 0.001000    Time 0.078753    
Epoch: [28][  100/  207]    Overall Loss 0.170142    Objective Loss 0.170142                                        LR 0.001000    Time 0.078147    
Epoch: [28][  110/  207]    Overall Loss 0.169591    Objective Loss 0.169591                                        LR 0.001000    Time 0.077684    
Epoch: [28][  120/  207]    Overall Loss 0.169765    Objective Loss 0.169765                                        LR 0.001000    Time 0.077271    
Epoch: [28][  130/  207]    Overall Loss 0.173436    Objective Loss 0.173436                                        LR 0.001000    Time 0.076853    
Epoch: [28][  140/  207]    Overall Loss 0.173646    Objective Loss 0.173646                                        LR 0.001000    Time 0.076617    
Epoch: [28][  150/  207]    Overall Loss 0.173479    Objective Loss 0.173479                                        LR 0.001000    Time 0.076331    
Epoch: [28][  160/  207]    Overall Loss 0.173652    Objective Loss 0.173652                                        LR 0.001000    Time 0.076031    
Epoch: [28][  170/  207]    Overall Loss 0.173337    Objective Loss 0.173337                                        LR 0.001000    Time 0.075756    
Epoch: [28][  180/  207]    Overall Loss 0.172571    Objective Loss 0.172571                                        LR 0.001000    Time 0.075560    
Epoch: [28][  190/  207]    Overall Loss 0.172835    Objective Loss 0.172835                                        LR 0.001000    Time 0.075238    
Epoch: [28][  200/  207]    Overall Loss 0.172887    Objective Loss 0.172887                                        LR 0.001000    Time 0.075017    
Epoch: [28][  207/  207]    Overall Loss 0.173118    Objective Loss 0.173118    Top1 88.901472    Top5 99.773499    LR 0.001000    Time 0.074787    
--- validate (epoch=28)-----------
5136 samples (512 per mini-batch)
Epoch: [28][   10/   11]    Loss 0.430874    Top1 79.414062    Top5 99.726562    
Epoch: [28][   11/   11]    Loss 0.417038    Top1 79.419782    Top5 99.727414    
==> Top1: 79.420    Top5: 99.727    Loss: 0.417

==> Confusion:
[[278   5   2   0   1   1   2  11]
 [  5 261  27   2   1   1   0   3]
 [  3  27 255   1   1   6   0   7]
 [  1   4   0 723  72  13  19   5]
 [  4   0   0  39 768   6  46  16]
 [ 22   6   9  16  26 747  43  25]
 [  2   0   0   5  18  11 788  13]
 [ 86  20  17  69  81 171  86 259]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [29][   10/  207]    Overall Loss 0.161548    Objective Loss 0.161548                                        LR 0.001000    Time 0.129808    
Epoch: [29][   20/  207]    Overall Loss 0.150101    Objective Loss 0.150101                                        LR 0.001000    Time 0.101148    
Epoch: [29][   30/  207]    Overall Loss 0.148652    Objective Loss 0.148652                                        LR 0.001000    Time 0.090847    
Epoch: [29][   40/  207]    Overall Loss 0.154428    Objective Loss 0.154428                                        LR 0.001000    Time 0.085438    
Epoch: [29][   50/  207]    Overall Loss 0.163944    Objective Loss 0.163944                                        LR 0.001000    Time 0.082837    
Epoch: [29][   60/  207]    Overall Loss 0.167038    Objective Loss 0.167038                                        LR 0.001000    Time 0.080748    
Epoch: [29][   70/  207]    Overall Loss 0.168328    Objective Loss 0.168328                                        LR 0.001000    Time 0.079586    
Epoch: [29][   80/  207]    Overall Loss 0.170568    Objective Loss 0.170568                                        LR 0.001000    Time 0.078402    
Epoch: [29][   90/  207]    Overall Loss 0.170388    Objective Loss 0.170388                                        LR 0.001000    Time 0.077634    
Epoch: [29][  100/  207]    Overall Loss 0.171923    Objective Loss 0.171923                                        LR 0.001000    Time 0.077206    
Epoch: [29][  110/  207]    Overall Loss 0.172246    Objective Loss 0.172246                                        LR 0.001000    Time 0.076668    
Epoch: [29][  120/  207]    Overall Loss 0.172878    Objective Loss 0.172878                                        LR 0.001000    Time 0.076165    
Epoch: [29][  130/  207]    Overall Loss 0.172808    Objective Loss 0.172808                                        LR 0.001000    Time 0.075750    
Epoch: [29][  140/  207]    Overall Loss 0.174358    Objective Loss 0.174358                                        LR 0.001000    Time 0.075421    
Epoch: [29][  150/  207]    Overall Loss 0.174893    Objective Loss 0.174893                                        LR 0.001000    Time 0.075263    
Epoch: [29][  160/  207]    Overall Loss 0.176109    Objective Loss 0.176109                                        LR 0.001000    Time 0.075094    
Epoch: [29][  170/  207]    Overall Loss 0.177199    Objective Loss 0.177199                                        LR 0.001000    Time 0.074932    
Epoch: [29][  180/  207]    Overall Loss 0.177895    Objective Loss 0.177895                                        LR 0.001000    Time 0.074787    
Epoch: [29][  190/  207]    Overall Loss 0.178020    Objective Loss 0.178020                                        LR 0.001000    Time 0.074498    
Epoch: [29][  200/  207]    Overall Loss 0.177923    Objective Loss 0.177923                                        LR 0.001000    Time 0.074409    
Epoch: [29][  207/  207]    Overall Loss 0.177578    Objective Loss 0.177578    Top1 91.166478    Top5 100.000000    LR 0.001000    Time 0.074165    
--- validate (epoch=29)-----------
5136 samples (512 per mini-batch)
Epoch: [29][   10/   11]    Loss 0.428471    Top1 78.574219    Top5 99.667969    
Epoch: [29][   11/   11]    Loss 0.400069    Top1 78.621495    Top5 99.669003    
==> Top1: 78.621    Top5: 99.669    Loss: 0.400

==> Confusion:
[[276   4   7   1   0   2   3   7]
 [  5 269  18   3   0   1   0   4]
 [  8  28 256   1   2   2   0   3]
 [  0   4   0 723  90   6  10   4]
 [  2   0   0  33 806   4  26   8]
 [ 16   6  15  20  44 753  28  12]
 [  2   0   0   4  50  11 762   8]
 [ 79  24  22  83 109 207  72 193]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [30][   10/  207]    Overall Loss 0.182159    Objective Loss 0.182159                                        LR 0.001000    Time 0.129437    
Epoch: [30][   20/  207]    Overall Loss 0.181364    Objective Loss 0.181364                                        LR 0.001000    Time 0.100075    
Epoch: [30][   30/  207]    Overall Loss 0.171985    Objective Loss 0.171985                                        LR 0.001000    Time 0.090892    
Epoch: [30][   40/  207]    Overall Loss 0.167002    Objective Loss 0.167002                                        LR 0.001000    Time 0.086077    
Epoch: [30][   50/  207]    Overall Loss 0.171153    Objective Loss 0.171153                                        LR 0.001000    Time 0.083349    
Epoch: [30][   60/  207]    Overall Loss 0.171722    Objective Loss 0.171722                                        LR 0.001000    Time 0.083279    
Epoch: [30][   70/  207]    Overall Loss 0.168682    Objective Loss 0.168682                                        LR 0.001000    Time 0.087122    
Epoch: [30][   80/  207]    Overall Loss 0.167737    Objective Loss 0.167737                                        LR 0.001000    Time 0.091313    
Epoch: [30][   90/  207]    Overall Loss 0.166849    Objective Loss 0.166849                                        LR 0.001000    Time 0.089328    
Epoch: [30][  100/  207]    Overall Loss 0.168104    Objective Loss 0.168104                                        LR 0.001000    Time 0.088037    
Epoch: [30][  110/  207]    Overall Loss 0.170826    Objective Loss 0.170826                                        LR 0.001000    Time 0.086859    
Epoch: [30][  120/  207]    Overall Loss 0.174445    Objective Loss 0.174445                                        LR 0.001000    Time 0.086418    
Epoch: [30][  130/  207]    Overall Loss 0.178045    Objective Loss 0.178045                                        LR 0.001000    Time 0.085639    
Epoch: [30][  140/  207]    Overall Loss 0.179969    Objective Loss 0.179969                                        LR 0.001000    Time 0.085050    
Epoch: [30][  150/  207]    Overall Loss 0.180501    Objective Loss 0.180501                                        LR 0.001000    Time 0.084180    
Epoch: [30][  160/  207]    Overall Loss 0.180988    Objective Loss 0.180988                                        LR 0.001000    Time 0.083383    
Epoch: [30][  170/  207]    Overall Loss 0.182034    Objective Loss 0.182034                                        LR 0.001000    Time 0.082740    
Epoch: [30][  180/  207]    Overall Loss 0.182886    Objective Loss 0.182886                                        LR 0.001000    Time 0.082325    
Epoch: [30][  190/  207]    Overall Loss 0.183929    Objective Loss 0.183929                                        LR 0.001000    Time 0.082175    
Epoch: [30][  200/  207]    Overall Loss 0.183313    Objective Loss 0.183313                                        LR 0.001000    Time 0.081985    
Epoch: [30][  207/  207]    Overall Loss 0.183887    Objective Loss 0.183887    Top1 87.315968    Top5 100.000000    LR 0.001000    Time 0.081624    
--- validate (epoch=30)-----------
5136 samples (512 per mini-batch)
Epoch: [30][   10/   11]    Loss 0.550221    Top1 77.246094    Top5 99.492188    
Epoch: [30][   11/   11]    Loss 0.583019    Top1 77.278037    Top5 99.493769    
==> Top1: 77.278    Top5: 99.494    Loss: 0.583

==> Confusion:
[[250   6  15   0   3  12   2  12]
 [  1 202  92   0   0   3   0   2]
 [  2  10 280   0   1   7   0   0]
 [  0   2   2 706 109  11   5   2]
 [  1   0   0  25 834   5   7   7]
 [  9   4  16  13  49 778  16   9]
 [  2   0   0   5  80  16 722  12]
 [ 40  22  47  73 135 248  27 197]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [31][   10/  207]    Overall Loss 0.200119    Objective Loss 0.200119                                        LR 0.001000    Time 0.132576    
Epoch: [31][   20/  207]    Overall Loss 0.179394    Objective Loss 0.179394                                        LR 0.001000    Time 0.104426    
Epoch: [31][   30/  207]    Overall Loss 0.170031    Objective Loss 0.170031                                        LR 0.001000    Time 0.095110    
Epoch: [31][   40/  207]    Overall Loss 0.162053    Objective Loss 0.162053                                        LR 0.001000    Time 0.089326    
Epoch: [31][   50/  207]    Overall Loss 0.157979    Objective Loss 0.157979                                        LR 0.001000    Time 0.087451    
Epoch: [31][   60/  207]    Overall Loss 0.153692    Objective Loss 0.153692                                        LR 0.001000    Time 0.086199    
Epoch: [31][   70/  207]    Overall Loss 0.153528    Objective Loss 0.153528                                        LR 0.001000    Time 0.084640    
Epoch: [31][   80/  207]    Overall Loss 0.152430    Objective Loss 0.152430                                        LR 0.001000    Time 0.084226    
Epoch: [31][   90/  207]    Overall Loss 0.150328    Objective Loss 0.150328                                        LR 0.001000    Time 0.083605    
Epoch: [31][  100/  207]    Overall Loss 0.152498    Objective Loss 0.152498                                        LR 0.001000    Time 0.083146    
Epoch: [31][  110/  207]    Overall Loss 0.154262    Objective Loss 0.154262                                        LR 0.001000    Time 0.082610    
Epoch: [31][  120/  207]    Overall Loss 0.154267    Objective Loss 0.154267                                        LR 0.001000    Time 0.081918    
Epoch: [31][  130/  207]    Overall Loss 0.154341    Objective Loss 0.154341                                        LR 0.001000    Time 0.081426    
Epoch: [31][  140/  207]    Overall Loss 0.155027    Objective Loss 0.155027                                        LR 0.001000    Time 0.081054    
Epoch: [31][  150/  207]    Overall Loss 0.154661    Objective Loss 0.154661                                        LR 0.001000    Time 0.080595    
Epoch: [31][  160/  207]    Overall Loss 0.155718    Objective Loss 0.155718                                        LR 0.001000    Time 0.080417    
Epoch: [31][  170/  207]    Overall Loss 0.156460    Objective Loss 0.156460                                        LR 0.001000    Time 0.080084    
Epoch: [31][  180/  207]    Overall Loss 0.156094    Objective Loss 0.156094                                        LR 0.001000    Time 0.079969    
Epoch: [31][  190/  207]    Overall Loss 0.156522    Objective Loss 0.156522                                        LR 0.001000    Time 0.079658    
Epoch: [31][  200/  207]    Overall Loss 0.157446    Objective Loss 0.157446                                        LR 0.001000    Time 0.079362    
Epoch: [31][  207/  207]    Overall Loss 0.157597    Objective Loss 0.157597    Top1 90.939977    Top5 100.000000    LR 0.001000    Time 0.079037    
--- validate (epoch=31)-----------
5136 samples (512 per mini-batch)
Epoch: [31][   10/   11]    Loss 0.470806    Top1 79.394531    Top5 99.453125    
Epoch: [31][   11/   11]    Loss 0.443118    Top1 79.419782    Top5 99.454829    
==> Top1: 79.420    Top5: 99.455    Loss: 0.443

==> Confusion:
[[270   4   5   0   1   6   3  11]
 [  2 248  41   2   0   3   0   4]
 [  3  20 265   0   0   8   0   4]
 [  0   5   0 747  61   9  14   1]
 [  3   0   0  62 773   3  31   7]
 [  8   5   8  15  21 798  28  11]
 [  2   0   0   6  41  14 769   5]
 [ 49  15  25  97  83 232  79 209]]

==> Best [Top1: 79.459   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 22]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [32][   10/  207]    Overall Loss 0.143191    Objective Loss 0.143191                                        LR 0.001000    Time 0.129793    
Epoch: [32][   20/  207]    Overall Loss 0.163627    Objective Loss 0.163627                                        LR 0.001000    Time 0.099817    
Epoch: [32][   30/  207]    Overall Loss 0.166802    Objective Loss 0.166802                                        LR 0.001000    Time 0.091873    
Epoch: [32][   40/  207]    Overall Loss 0.173746    Objective Loss 0.173746                                        LR 0.001000    Time 0.086957    
Epoch: [32][   50/  207]    Overall Loss 0.179037    Objective Loss 0.179037                                        LR 0.001000    Time 0.085505    
Epoch: [32][   60/  207]    Overall Loss 0.177947    Objective Loss 0.177947                                        LR 0.001000    Time 0.084551    
Epoch: [32][   70/  207]    Overall Loss 0.177088    Objective Loss 0.177088                                        LR 0.001000    Time 0.083792    
Epoch: [32][   80/  207]    Overall Loss 0.176874    Objective Loss 0.176874                                        LR 0.001000    Time 0.082959    
Epoch: [32][   90/  207]    Overall Loss 0.175146    Objective Loss 0.175146                                        LR 0.001000    Time 0.081779    
Epoch: [32][  100/  207]    Overall Loss 0.172918    Objective Loss 0.172918                                        LR 0.001000    Time 0.080971    
Epoch: [32][  110/  207]    Overall Loss 0.172595    Objective Loss 0.172595                                        LR 0.001000    Time 0.080712    
Epoch: [32][  120/  207]    Overall Loss 0.171501    Objective Loss 0.171501                                        LR 0.001000    Time 0.080565    
Epoch: [32][  130/  207]    Overall Loss 0.169917    Objective Loss 0.169917                                        LR 0.001000    Time 0.080271    
Epoch: [32][  140/  207]    Overall Loss 0.169325    Objective Loss 0.169325                                        LR 0.001000    Time 0.080103    
Epoch: [32][  150/  207]    Overall Loss 0.168708    Objective Loss 0.168708                                        LR 0.001000    Time 0.080065    
Epoch: [32][  160/  207]    Overall Loss 0.169217    Objective Loss 0.169217                                        LR 0.001000    Time 0.079976    
Epoch: [32][  170/  207]    Overall Loss 0.170146    Objective Loss 0.170146                                        LR 0.001000    Time 0.079916    
Epoch: [32][  180/  207]    Overall Loss 0.171322    Objective Loss 0.171322                                        LR 0.001000    Time 0.079744    
Epoch: [32][  190/  207]    Overall Loss 0.171266    Objective Loss 0.171266                                        LR 0.001000    Time 0.079507    
Epoch: [32][  200/  207]    Overall Loss 0.171220    Objective Loss 0.171220                                        LR 0.001000    Time 0.079182    
Epoch: [32][  207/  207]    Overall Loss 0.170455    Objective Loss 0.170455    Top1 91.279728    Top5 100.000000    LR 0.001000    Time 0.078937    
--- validate (epoch=32)-----------
5136 samples (512 per mini-batch)
Epoch: [32][   10/   11]    Loss 0.474426    Top1 80.117188    Top5 99.648438    
Epoch: [32][   11/   11]    Loss 0.524177    Top1 80.081776    Top5 99.649533    
==> Top1: 80.082    Top5: 99.650    Loss: 0.524

==> Confusion:
[[256   4   7   0   1   7   3  22]
 [  4 246  43   2   0   0   0   5]
 [  1  10 276   0   0   3   0  10]
 [  0   4   2 754  57   7   5   8]
 [  1   0   0  67 771  13  14  13]
 [  6   3  15  26  20 784  17  23]
 [  2   0   0   8  41  18 748  20]
 [ 37  25  26 107  76 207  33 278]]

==> Best [Top1: 80.082   Top5: 99.650   Sparsity:0.00   Params: 117200 on epoch: 32]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [33][   10/  207]    Overall Loss 0.123766    Objective Loss 0.123766                                        LR 0.001000    Time 0.138043    
Epoch: [33][   20/  207]    Overall Loss 0.132385    Objective Loss 0.132385                                        LR 0.001000    Time 0.107116    
Epoch: [33][   30/  207]    Overall Loss 0.133403    Objective Loss 0.133403                                        LR 0.001000    Time 0.096468    
Epoch: [33][   40/  207]    Overall Loss 0.132568    Objective Loss 0.132568                                        LR 0.001000    Time 0.091147    
Epoch: [33][   50/  207]    Overall Loss 0.129582    Objective Loss 0.129582                                        LR 0.001000    Time 0.088015    
Epoch: [33][   60/  207]    Overall Loss 0.127173    Objective Loss 0.127173                                        LR 0.001000    Time 0.085998    
Epoch: [33][   70/  207]    Overall Loss 0.126016    Objective Loss 0.126016                                        LR 0.001000    Time 0.083889    
Epoch: [33][   80/  207]    Overall Loss 0.128322    Objective Loss 0.128322                                        LR 0.001000    Time 0.082416    
Epoch: [33][   90/  207]    Overall Loss 0.129690    Objective Loss 0.129690                                        LR 0.001000    Time 0.081294    
Epoch: [33][  100/  207]    Overall Loss 0.134129    Objective Loss 0.134129                                        LR 0.001000    Time 0.080582    
Epoch: [33][  110/  207]    Overall Loss 0.142629    Objective Loss 0.142629                                        LR 0.001000    Time 0.080038    
Epoch: [33][  120/  207]    Overall Loss 0.147417    Objective Loss 0.147417                                        LR 0.001000    Time 0.079313    
Epoch: [33][  130/  207]    Overall Loss 0.148485    Objective Loss 0.148485                                        LR 0.001000    Time 0.078819    
Epoch: [33][  140/  207]    Overall Loss 0.148988    Objective Loss 0.148988                                        LR 0.001000    Time 0.078301    
Epoch: [33][  150/  207]    Overall Loss 0.148471    Objective Loss 0.148471                                        LR 0.001000    Time 0.078044    
Epoch: [33][  160/  207]    Overall Loss 0.147602    Objective Loss 0.147602                                        LR 0.001000    Time 0.077726    
Epoch: [33][  170/  207]    Overall Loss 0.147607    Objective Loss 0.147607                                        LR 0.001000    Time 0.077380    
Epoch: [33][  180/  207]    Overall Loss 0.150302    Objective Loss 0.150302                                        LR 0.001000    Time 0.077192    
Epoch: [33][  190/  207]    Overall Loss 0.151172    Objective Loss 0.151172                                        LR 0.001000    Time 0.076999    
Epoch: [33][  200/  207]    Overall Loss 0.151563    Objective Loss 0.151563                                        LR 0.001000    Time 0.076807    
Epoch: [33][  207/  207]    Overall Loss 0.151758    Objective Loss 0.151758    Top1 93.771234    Top5 99.773499    LR 0.001000    Time 0.076514    
--- validate (epoch=33)-----------
5136 samples (512 per mini-batch)
Epoch: [33][   10/   11]    Loss 0.442147    Top1 80.917969    Top5 99.726562    
Epoch: [33][   11/   11]    Loss 0.613007    Top1 80.899533    Top5 99.727414    
==> Top1: 80.900    Top5: 99.727    Loss: 0.613

==> Confusion:
[[277   5   2   3   0   3   2   8]
 [  4 281  10   1   0   1   0   3]
 [  3  43 253   0   0   0   0   1]
 [  0   5   2 745  46  11  13  15]
 [  3   0   0  53 757   8  34  24]
 [ 16  15  18  13  14 780  10  28]
 [  3   0   0   4  28  22 758  22]
 [ 59  33  23  72  62 190  46 304]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [34][   10/  207]    Overall Loss 0.147299    Objective Loss 0.147299                                        LR 0.001000    Time 0.110918    
Epoch: [34][   20/  207]    Overall Loss 0.144028    Objective Loss 0.144028                                        LR 0.001000    Time 0.091053    
Epoch: [34][   30/  207]    Overall Loss 0.143485    Objective Loss 0.143485                                        LR 0.001000    Time 0.084882    
Epoch: [34][   40/  207]    Overall Loss 0.141154    Objective Loss 0.141154                                        LR 0.001000    Time 0.081552    
Epoch: [34][   50/  207]    Overall Loss 0.143965    Objective Loss 0.143965                                        LR 0.001000    Time 0.079497    
Epoch: [34][   60/  207]    Overall Loss 0.145244    Objective Loss 0.145244                                        LR 0.001000    Time 0.078291    
Epoch: [34][   70/  207]    Overall Loss 0.144763    Objective Loss 0.144763                                        LR 0.001000    Time 0.077209    
Epoch: [34][   80/  207]    Overall Loss 0.145882    Objective Loss 0.145882                                        LR 0.001000    Time 0.076552    
Epoch: [34][   90/  207]    Overall Loss 0.144501    Objective Loss 0.144501                                        LR 0.001000    Time 0.076016    
Epoch: [34][  100/  207]    Overall Loss 0.144261    Objective Loss 0.144261                                        LR 0.001000    Time 0.075540    
Epoch: [34][  110/  207]    Overall Loss 0.144145    Objective Loss 0.144145                                        LR 0.001000    Time 0.075150    
Epoch: [34][  120/  207]    Overall Loss 0.145627    Objective Loss 0.145627                                        LR 0.001000    Time 0.074838    
Epoch: [34][  130/  207]    Overall Loss 0.146069    Objective Loss 0.146069                                        LR 0.001000    Time 0.074629    
Epoch: [34][  140/  207]    Overall Loss 0.148175    Objective Loss 0.148175                                        LR 0.001000    Time 0.074575    
Epoch: [34][  150/  207]    Overall Loss 0.148207    Objective Loss 0.148207                                        LR 0.001000    Time 0.074511    
Epoch: [34][  160/  207]    Overall Loss 0.148714    Objective Loss 0.148714                                        LR 0.001000    Time 0.074392    
Epoch: [34][  170/  207]    Overall Loss 0.147284    Objective Loss 0.147284                                        LR 0.001000    Time 0.074316    
Epoch: [34][  180/  207]    Overall Loss 0.145739    Objective Loss 0.145739                                        LR 0.001000    Time 0.074090    
Epoch: [34][  190/  207]    Overall Loss 0.144806    Objective Loss 0.144806                                        LR 0.001000    Time 0.073883    
Epoch: [34][  200/  207]    Overall Loss 0.145117    Objective Loss 0.145117                                        LR 0.001000    Time 0.073738    
Epoch: [34][  207/  207]    Overall Loss 0.146300    Objective Loss 0.146300    Top1 92.638732    Top5 99.886750    LR 0.001000    Time 0.073542    
--- validate (epoch=34)-----------
5136 samples (512 per mini-batch)
Epoch: [34][   10/   11]    Loss 0.419807    Top1 80.351562    Top5 99.609375    
Epoch: [34][   11/   11]    Loss 0.390852    Top1 80.334891    Top5 99.610592    
==> Top1: 80.335    Top5: 99.611    Loss: 0.391

==> Confusion:
[[269   8   2   2   0   3   4  12]
 [  2 266  28   1   0   1   0   2]
 [  2  21 274   1   0   1   0   1]
 [  0   7   0 755  46   9  15   5]
 [  2   0   0  67 757   9  31  13]
 [ 14   8  29  21  26 763  19  14]
 [  3   0   0   6  21  17 773  17]
 [ 61  29  39  89  76 176  50 269]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [35][   10/  207]    Overall Loss 0.124178    Objective Loss 0.124178                                        LR 0.001000    Time 0.129568    
Epoch: [35][   20/  207]    Overall Loss 0.121033    Objective Loss 0.121033                                        LR 0.001000    Time 0.101132    
Epoch: [35][   30/  207]    Overall Loss 0.120050    Objective Loss 0.120050                                        LR 0.001000    Time 0.091285    
Epoch: [35][   40/  207]    Overall Loss 0.117912    Objective Loss 0.117912                                        LR 0.001000    Time 0.086835    
Epoch: [35][   50/  207]    Overall Loss 0.118823    Objective Loss 0.118823                                        LR 0.001000    Time 0.083460    
Epoch: [35][   60/  207]    Overall Loss 0.118278    Objective Loss 0.118278                                        LR 0.001000    Time 0.081417    
Epoch: [35][   70/  207]    Overall Loss 0.117614    Objective Loss 0.117614                                        LR 0.001000    Time 0.080019    
Epoch: [35][   80/  207]    Overall Loss 0.119740    Objective Loss 0.119740                                        LR 0.001000    Time 0.079016    
Epoch: [35][   90/  207]    Overall Loss 0.121644    Objective Loss 0.121644                                        LR 0.001000    Time 0.078461    
Epoch: [35][  100/  207]    Overall Loss 0.122596    Objective Loss 0.122596                                        LR 0.001000    Time 0.077787    
Epoch: [35][  110/  207]    Overall Loss 0.123418    Objective Loss 0.123418                                        LR 0.001000    Time 0.077351    
Epoch: [35][  120/  207]    Overall Loss 0.123534    Objective Loss 0.123534                                        LR 0.001000    Time 0.076825    
Epoch: [35][  130/  207]    Overall Loss 0.124513    Objective Loss 0.124513                                        LR 0.001000    Time 0.076466    
Epoch: [35][  140/  207]    Overall Loss 0.126094    Objective Loss 0.126094                                        LR 0.001000    Time 0.076036    
Epoch: [35][  150/  207]    Overall Loss 0.127774    Objective Loss 0.127774                                        LR 0.001000    Time 0.075847    
Epoch: [35][  160/  207]    Overall Loss 0.128401    Objective Loss 0.128401                                        LR 0.001000    Time 0.075706    
Epoch: [35][  170/  207]    Overall Loss 0.130135    Objective Loss 0.130135                                        LR 0.001000    Time 0.075567    
Epoch: [35][  180/  207]    Overall Loss 0.131767    Objective Loss 0.131767                                        LR 0.001000    Time 0.075380    
Epoch: [35][  190/  207]    Overall Loss 0.134539    Objective Loss 0.134539                                        LR 0.001000    Time 0.075261    
Epoch: [35][  200/  207]    Overall Loss 0.136956    Objective Loss 0.136956                                        LR 0.001000    Time 0.075091    
Epoch: [35][  207/  207]    Overall Loss 0.138231    Objective Loss 0.138231    Top1 91.732729    Top5 99.660249    LR 0.001000    Time 0.074830    
--- validate (epoch=35)-----------
5136 samples (512 per mini-batch)
Epoch: [35][   10/   11]    Loss 0.530208    Top1 80.839844    Top5 99.687500    
Epoch: [35][   11/   11]    Loss 0.511572    Top1 80.821651    Top5 99.688474    
==> Top1: 80.822    Top5: 99.688    Loss: 0.512

==> Confusion:
[[263   4   3   0   0   5   1  24]
 [  1 254  25   2   0   3   0  15]
 [  2  21 244   1   1  11   0  20]
 [  1   2   1 766  37  10   9  11]
 [  0   0   0  69 749  11  26  24]
 [  2   8   4  20  26 776  24  34]
 [  2   0   0   7  33  12 755  28]
 [ 25  13  17  85  79 187  39 344]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [36][   10/  207]    Overall Loss 0.164752    Objective Loss 0.164752                                        LR 0.001000    Time 0.130400    
Epoch: [36][   20/  207]    Overall Loss 0.175644    Objective Loss 0.175644                                        LR 0.001000    Time 0.100987    
Epoch: [36][   30/  207]    Overall Loss 0.168330    Objective Loss 0.168330                                        LR 0.001000    Time 0.091938    
Epoch: [36][   40/  207]    Overall Loss 0.164854    Objective Loss 0.164854                                        LR 0.001000    Time 0.087164    
Epoch: [36][   50/  207]    Overall Loss 0.165603    Objective Loss 0.165603                                        LR 0.001000    Time 0.084212    
Epoch: [36][   60/  207]    Overall Loss 0.165687    Objective Loss 0.165687                                        LR 0.001000    Time 0.081973    
Epoch: [36][   70/  207]    Overall Loss 0.165577    Objective Loss 0.165577                                        LR 0.001000    Time 0.080232    
Epoch: [36][   80/  207]    Overall Loss 0.166711    Objective Loss 0.166711                                        LR 0.001000    Time 0.079051    
Epoch: [36][   90/  207]    Overall Loss 0.167397    Objective Loss 0.167397                                        LR 0.001000    Time 0.078309    
Epoch: [36][  100/  207]    Overall Loss 0.169120    Objective Loss 0.169120                                        LR 0.001000    Time 0.077809    
Epoch: [36][  110/  207]    Overall Loss 0.169142    Objective Loss 0.169142                                        LR 0.001000    Time 0.077316    
Epoch: [36][  120/  207]    Overall Loss 0.167288    Objective Loss 0.167288                                        LR 0.001000    Time 0.077089    
Epoch: [36][  130/  207]    Overall Loss 0.169278    Objective Loss 0.169278                                        LR 0.001000    Time 0.076780    
Epoch: [36][  140/  207]    Overall Loss 0.169940    Objective Loss 0.169940                                        LR 0.001000    Time 0.076548    
Epoch: [36][  150/  207]    Overall Loss 0.170226    Objective Loss 0.170226                                        LR 0.001000    Time 0.076205    
Epoch: [36][  160/  207]    Overall Loss 0.172229    Objective Loss 0.172229                                        LR 0.001000    Time 0.075956    
Epoch: [36][  170/  207]    Overall Loss 0.172547    Objective Loss 0.172547                                        LR 0.001000    Time 0.075680    
Epoch: [36][  180/  207]    Overall Loss 0.172418    Objective Loss 0.172418                                        LR 0.001000    Time 0.075426    
Epoch: [36][  190/  207]    Overall Loss 0.171810    Objective Loss 0.171810                                        LR 0.001000    Time 0.075252    
Epoch: [36][  200/  207]    Overall Loss 0.173067    Objective Loss 0.173067                                        LR 0.001000    Time 0.075058    
Epoch: [36][  207/  207]    Overall Loss 0.174041    Objective Loss 0.174041    Top1 90.147225    Top5 100.000000    LR 0.001000    Time 0.074828    
--- validate (epoch=36)-----------
5136 samples (512 per mini-batch)
Epoch: [36][   10/   11]    Loss 0.500471    Top1 80.019531    Top5 99.589844    
Epoch: [36][   11/   11]    Loss 0.455474    Top1 80.081776    Top5 99.591121    
==> Top1: 80.082    Top5: 99.591    Loss: 0.455

==> Confusion:
[[259   5   9   1   0   8   4  14]
 [  3 252  38   2   0   1   0   4]
 [  3  24 269   0   0   3   0   1]
 [  0   6   2 723  79  12   5  10]
 [  2   0   3  36 789   8  22  19]
 [  6   5  16  12  29 782  28  16]
 [  2   1   0   5  37  14 765  13]
 [ 33  36  27  63  91 196  69 274]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [37][   10/  207]    Overall Loss 0.165439    Objective Loss 0.165439                                        LR 0.001000    Time 0.128956    
Epoch: [37][   20/  207]    Overall Loss 0.164766    Objective Loss 0.164766                                        LR 0.001000    Time 0.099721    
Epoch: [37][   30/  207]    Overall Loss 0.160570    Objective Loss 0.160570                                        LR 0.001000    Time 0.094247    
Epoch: [37][   40/  207]    Overall Loss 0.161935    Objective Loss 0.161935                                        LR 0.001000    Time 0.089380    
Epoch: [37][   50/  207]    Overall Loss 0.165627    Objective Loss 0.165627                                        LR 0.001000    Time 0.085981    
Epoch: [37][   60/  207]    Overall Loss 0.165435    Objective Loss 0.165435                                        LR 0.001000    Time 0.083796    
Epoch: [37][   70/  207]    Overall Loss 0.164529    Objective Loss 0.164529                                        LR 0.001000    Time 0.082065    
Epoch: [37][   80/  207]    Overall Loss 0.162208    Objective Loss 0.162208                                        LR 0.001000    Time 0.081131    
Epoch: [37][   90/  207]    Overall Loss 0.163624    Objective Loss 0.163624                                        LR 0.001000    Time 0.080214    
Epoch: [37][  100/  207]    Overall Loss 0.161705    Objective Loss 0.161705                                        LR 0.001000    Time 0.079664    
Epoch: [37][  110/  207]    Overall Loss 0.161956    Objective Loss 0.161956                                        LR 0.001000    Time 0.079005    
Epoch: [37][  120/  207]    Overall Loss 0.159914    Objective Loss 0.159914                                        LR 0.001000    Time 0.078558    
Epoch: [37][  130/  207]    Overall Loss 0.158938    Objective Loss 0.158938                                        LR 0.001000    Time 0.078196    
Epoch: [37][  140/  207]    Overall Loss 0.157221    Objective Loss 0.157221                                        LR 0.001000    Time 0.077915    
Epoch: [37][  150/  207]    Overall Loss 0.156921    Objective Loss 0.156921                                        LR 0.001000    Time 0.077658    
Epoch: [37][  160/  207]    Overall Loss 0.155300    Objective Loss 0.155300                                        LR 0.001000    Time 0.077419    
Epoch: [37][  170/  207]    Overall Loss 0.153652    Objective Loss 0.153652                                        LR 0.001000    Time 0.077128    
Epoch: [37][  180/  207]    Overall Loss 0.151728    Objective Loss 0.151728                                        LR 0.001000    Time 0.076923    
Epoch: [37][  190/  207]    Overall Loss 0.151663    Objective Loss 0.151663                                        LR 0.001000    Time 0.076603    
Epoch: [37][  200/  207]    Overall Loss 0.151989    Objective Loss 0.151989                                        LR 0.001000    Time 0.076454    
Epoch: [37][  207/  207]    Overall Loss 0.152489    Objective Loss 0.152489    Top1 91.845980    Top5 100.000000    LR 0.001000    Time 0.076160    
--- validate (epoch=37)-----------
5136 samples (512 per mini-batch)
Epoch: [37][   10/   11]    Loss 0.535132    Top1 79.589844    Top5 99.628906    
Epoch: [37][   11/   11]    Loss 0.511829    Top1 79.614486    Top5 99.630062    
==> Top1: 79.614    Top5: 99.630    Loss: 0.512

==> Confusion:
[[248   8  13   0   0  11   4  16]
 [  1 237  56   1   0   2   0   3]
 [  0  19 275   0   0   6   0   0]
 [  0   4   0 732  60  19  11  11]
 [  0   0   0  40 792   9  27  11]
 [  1   3  12  16  22 808  24   8]
 [  2   0   0   5  39  11 765  15]
 [ 37  39  35  63  98 235  50 232]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [38][   10/  207]    Overall Loss 0.131367    Objective Loss 0.131367                                        LR 0.001000    Time 0.112644    
Epoch: [38][   20/  207]    Overall Loss 0.131558    Objective Loss 0.131558                                        LR 0.001000    Time 0.091866    
Epoch: [38][   30/  207]    Overall Loss 0.129406    Objective Loss 0.129406                                        LR 0.001000    Time 0.084604    
Epoch: [38][   40/  207]    Overall Loss 0.122098    Objective Loss 0.122098                                        LR 0.001000    Time 0.081347    
Epoch: [38][   50/  207]    Overall Loss 0.121184    Objective Loss 0.121184                                        LR 0.001000    Time 0.078972    
Epoch: [38][   60/  207]    Overall Loss 0.121920    Objective Loss 0.121920                                        LR 0.001000    Time 0.077836    
Epoch: [38][   70/  207]    Overall Loss 0.125075    Objective Loss 0.125075                                        LR 0.001000    Time 0.077073    
Epoch: [38][   80/  207]    Overall Loss 0.125649    Objective Loss 0.125649                                        LR 0.001000    Time 0.076505    
Epoch: [38][   90/  207]    Overall Loss 0.125397    Objective Loss 0.125397                                        LR 0.001000    Time 0.076138    
Epoch: [38][  100/  207]    Overall Loss 0.125965    Objective Loss 0.125965                                        LR 0.001000    Time 0.075707    
Epoch: [38][  110/  207]    Overall Loss 0.126350    Objective Loss 0.126350                                        LR 0.001000    Time 0.075222    
Epoch: [38][  120/  207]    Overall Loss 0.126864    Objective Loss 0.126864                                        LR 0.001000    Time 0.074940    
Epoch: [38][  130/  207]    Overall Loss 0.127053    Objective Loss 0.127053                                        LR 0.001000    Time 0.074687    
Epoch: [38][  140/  207]    Overall Loss 0.128494    Objective Loss 0.128494                                        LR 0.001000    Time 0.074558    
Epoch: [38][  150/  207]    Overall Loss 0.131081    Objective Loss 0.131081                                        LR 0.001000    Time 0.074431    
Epoch: [38][  160/  207]    Overall Loss 0.131706    Objective Loss 0.131706                                        LR 0.001000    Time 0.074189    
Epoch: [38][  170/  207]    Overall Loss 0.133332    Objective Loss 0.133332                                        LR 0.001000    Time 0.073961    
Epoch: [38][  180/  207]    Overall Loss 0.135719    Objective Loss 0.135719                                        LR 0.001000    Time 0.073886    
Epoch: [38][  190/  207]    Overall Loss 0.137750    Objective Loss 0.137750                                        LR 0.001000    Time 0.073800    
Epoch: [38][  200/  207]    Overall Loss 0.138935    Objective Loss 0.138935                                        LR 0.001000    Time 0.073643    
Epoch: [38][  207/  207]    Overall Loss 0.140515    Objective Loss 0.140515    Top1 92.072480    Top5 99.886750    LR 0.001000    Time 0.073427    
--- validate (epoch=38)-----------
5136 samples (512 per mini-batch)
Epoch: [38][   10/   11]    Loss 0.439195    Top1 80.585938    Top5 99.667969    
Epoch: [38][   11/   11]    Loss 0.427010    Top1 80.529595    Top5 99.669003    
==> Top1: 80.530    Top5: 99.669    Loss: 0.427

==> Confusion:
[[269   5   2   1   0   7   4  12]
 [  2 274  16   2   0   2   1   3]
 [  2  38 244   1   0   7   0   8]
 [  0   5   1 749  51  12  12   7]
 [  2   2   0  74 752   5  27  17]
 [ 15   3   7  16  15 802  15  21]
 [  4   0   0   8  30  18 760  17]
 [ 62  23  14  95  60 203  46 286]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [39][   10/  207]    Overall Loss 0.128901    Objective Loss 0.128901                                        LR 0.001000    Time 0.129091    
Epoch: [39][   20/  207]    Overall Loss 0.129422    Objective Loss 0.129422                                        LR 0.001000    Time 0.100356    
Epoch: [39][   30/  207]    Overall Loss 0.131220    Objective Loss 0.131220                                        LR 0.001000    Time 0.091601    
Epoch: [39][   40/  207]    Overall Loss 0.135045    Objective Loss 0.135045                                        LR 0.001000    Time 0.086748    
Epoch: [39][   50/  207]    Overall Loss 0.134108    Objective Loss 0.134108                                        LR 0.001000    Time 0.083647    
Epoch: [39][   60/  207]    Overall Loss 0.132101    Objective Loss 0.132101                                        LR 0.001000    Time 0.081837    
Epoch: [39][   70/  207]    Overall Loss 0.131980    Objective Loss 0.131980                                        LR 0.001000    Time 0.080328    
Epoch: [39][   80/  207]    Overall Loss 0.130695    Objective Loss 0.130695                                        LR 0.001000    Time 0.079672    
Epoch: [39][   90/  207]    Overall Loss 0.130158    Objective Loss 0.130158                                        LR 0.001000    Time 0.078967    
Epoch: [39][  100/  207]    Overall Loss 0.129444    Objective Loss 0.129444                                        LR 0.001000    Time 0.077976    
Epoch: [39][  110/  207]    Overall Loss 0.130023    Objective Loss 0.130023                                        LR 0.001000    Time 0.077597    
Epoch: [39][  120/  207]    Overall Loss 0.131368    Objective Loss 0.131368                                        LR 0.001000    Time 0.077667    
Epoch: [39][  130/  207]    Overall Loss 0.132622    Objective Loss 0.132622                                        LR 0.001000    Time 0.077727    
Epoch: [39][  140/  207]    Overall Loss 0.133303    Objective Loss 0.133303                                        LR 0.001000    Time 0.077794    
Epoch: [39][  150/  207]    Overall Loss 0.134131    Objective Loss 0.134131                                        LR 0.001000    Time 0.077725    
Epoch: [39][  160/  207]    Overall Loss 0.133303    Objective Loss 0.133303                                        LR 0.001000    Time 0.078295    
Epoch: [39][  170/  207]    Overall Loss 0.132894    Objective Loss 0.132894                                        LR 0.001000    Time 0.078386    
Epoch: [39][  180/  207]    Overall Loss 0.132135    Objective Loss 0.132135                                        LR 0.001000    Time 0.078105    
Epoch: [39][  190/  207]    Overall Loss 0.131129    Objective Loss 0.131129                                        LR 0.001000    Time 0.078309    
Epoch: [39][  200/  207]    Overall Loss 0.130837    Objective Loss 0.130837                                        LR 0.001000    Time 0.078301    
Epoch: [39][  207/  207]    Overall Loss 0.131322    Objective Loss 0.131322    Top1 91.053228    Top5 100.000000    LR 0.001000    Time 0.078044    
--- validate (epoch=39)-----------
5136 samples (512 per mini-batch)
Epoch: [39][   10/   11]    Loss 0.434241    Top1 80.839844    Top5 99.687500    
Epoch: [39][   11/   11]    Loss 0.426534    Top1 80.821651    Top5 99.688474    
==> Top1: 80.822    Top5: 99.688    Loss: 0.427

==> Confusion:
[[276   6   1   0   0   2   4  11]
 [  3 265  25   1   0   1   0   5]
 [  7  23 258   1   0   7   0   4]
 [  0   5   0 755  50  15   5   7]
 [  3   0   0  59 778  11  16  12]
 [  9   5   8  13  13 828   8  10]
 [  3   1   0   6  42  30 741  14]
 [ 64  25  16  75  90 237  32 250]]

==> Best [Top1: 80.900   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 33]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [40][   10/  207]    Overall Loss 0.121080    Objective Loss 0.121080                                        LR 0.001000    Time 0.129914    
Epoch: [40][   20/  207]    Overall Loss 0.115262    Objective Loss 0.115262                                        LR 0.001000    Time 0.101247    
Epoch: [40][   30/  207]    Overall Loss 0.113645    Objective Loss 0.113645                                        LR 0.001000    Time 0.091518    
Epoch: [40][   40/  207]    Overall Loss 0.111601    Objective Loss 0.111601                                        LR 0.001000    Time 0.086648    
Epoch: [40][   50/  207]    Overall Loss 0.110690    Objective Loss 0.110690                                        LR 0.001000    Time 0.083448    
Epoch: [40][   60/  207]    Overall Loss 0.110023    Objective Loss 0.110023                                        LR 0.001000    Time 0.081670    
Epoch: [40][   70/  207]    Overall Loss 0.109029    Objective Loss 0.109029                                        LR 0.001000    Time 0.082078    
Epoch: [40][   80/  207]    Overall Loss 0.108475    Objective Loss 0.108475                                        LR 0.001000    Time 0.083647    
Epoch: [40][   90/  207]    Overall Loss 0.107499    Objective Loss 0.107499                                        LR 0.001000    Time 0.083778    
Epoch: [40][  100/  207]    Overall Loss 0.106473    Objective Loss 0.106473                                        LR 0.001000    Time 0.083751    
Epoch: [40][  110/  207]    Overall Loss 0.106556    Objective Loss 0.106556                                        LR 0.001000    Time 0.084407    
Epoch: [40][  120/  207]    Overall Loss 0.106691    Objective Loss 0.106691                                        LR 0.001000    Time 0.084540    
Epoch: [40][  130/  207]    Overall Loss 0.106529    Objective Loss 0.106529                                        LR 0.001000    Time 0.084034    
Epoch: [40][  140/  207]    Overall Loss 0.107144    Objective Loss 0.107144                                        LR 0.001000    Time 0.083525    
Epoch: [40][  150/  207]    Overall Loss 0.107531    Objective Loss 0.107531                                        LR 0.001000    Time 0.083492    
Epoch: [40][  160/  207]    Overall Loss 0.107470    Objective Loss 0.107470                                        LR 0.001000    Time 0.083476    
Epoch: [40][  170/  207]    Overall Loss 0.107958    Objective Loss 0.107958                                        LR 0.001000    Time 0.083472    
Epoch: [40][  180/  207]    Overall Loss 0.108140    Objective Loss 0.108140                                        LR 0.001000    Time 0.083230    
Epoch: [40][  190/  207]    Overall Loss 0.108086    Objective Loss 0.108086                                        LR 0.001000    Time 0.082810    
Epoch: [40][  200/  207]    Overall Loss 0.108333    Objective Loss 0.108333                                        LR 0.001000    Time 0.082408    
Epoch: [40][  207/  207]    Overall Loss 0.108416    Objective Loss 0.108416    Top1 93.771234    Top5 100.000000    LR 0.001000    Time 0.081918    
--- validate (epoch=40)-----------
5136 samples (512 per mini-batch)
Epoch: [40][   10/   11]    Loss 0.513330    Top1 81.113281    Top5 99.589844    
Epoch: [40][   11/   11]    Loss 0.498211    Top1 81.094237    Top5 99.591121    
==> Top1: 81.094    Top5: 99.591    Loss: 0.498

==> Confusion:
[[260   4   6   0   1   6   3  20]
 [  2 260  27   1   0   3   1   6]
 [  3  20 268   0   0   5   0   4]
 [  0   4   0 724  59  18  14  18]
 [  1   0   0  42 782  14  21  19]
 [  6   2  10   8  15 826  13  14]
 [  1   0   0   5  34  25 754  18]
 [ 26  19  25  46  71 263  48 291]]

==> Best [Top1: 81.094   Top5: 99.591   Sparsity:0.00   Params: 117200 on epoch: 40]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [41][   10/  207]    Overall Loss 0.089364    Objective Loss 0.089364                                        LR 0.001000    Time 0.146309    
Epoch: [41][   20/  207]    Overall Loss 0.093013    Objective Loss 0.093013                                        LR 0.001000    Time 0.111923    
Epoch: [41][   30/  207]    Overall Loss 0.095336    Objective Loss 0.095336                                        LR 0.001000    Time 0.101304    
Epoch: [41][   40/  207]    Overall Loss 0.098120    Objective Loss 0.098120                                        LR 0.001000    Time 0.096810    
Epoch: [41][   50/  207]    Overall Loss 0.101779    Objective Loss 0.101779                                        LR 0.001000    Time 0.095826    
Epoch: [41][   60/  207]    Overall Loss 0.101897    Objective Loss 0.101897                                        LR 0.001000    Time 0.095397    
Epoch: [41][   70/  207]    Overall Loss 0.101742    Objective Loss 0.101742                                        LR 0.001000    Time 0.095396    
Epoch: [41][   80/  207]    Overall Loss 0.102649    Objective Loss 0.102649                                        LR 0.001000    Time 0.095199    
Epoch: [41][   90/  207]    Overall Loss 0.103977    Objective Loss 0.103977                                        LR 0.001000    Time 0.095033    
Epoch: [41][  100/  207]    Overall Loss 0.105768    Objective Loss 0.105768                                        LR 0.001000    Time 0.093411    
Epoch: [41][  110/  207]    Overall Loss 0.108835    Objective Loss 0.108835                                        LR 0.001000    Time 0.091763    
Epoch: [41][  120/  207]    Overall Loss 0.109375    Objective Loss 0.109375                                        LR 0.001000    Time 0.090420    
Epoch: [41][  130/  207]    Overall Loss 0.110414    Objective Loss 0.110414                                        LR 0.001000    Time 0.089198    
Epoch: [41][  140/  207]    Overall Loss 0.111278    Objective Loss 0.111278                                        LR 0.001000    Time 0.088081    
Epoch: [41][  150/  207]    Overall Loss 0.111254    Objective Loss 0.111254                                        LR 0.001000    Time 0.087114    
Epoch: [41][  160/  207]    Overall Loss 0.111124    Objective Loss 0.111124                                        LR 0.001000    Time 0.086359    
Epoch: [41][  170/  207]    Overall Loss 0.110887    Objective Loss 0.110887                                        LR 0.001000    Time 0.085706    
Epoch: [41][  180/  207]    Overall Loss 0.110789    Objective Loss 0.110789                                        LR 0.001000    Time 0.085488    
Epoch: [41][  190/  207]    Overall Loss 0.111197    Objective Loss 0.111197                                        LR 0.001000    Time 0.085071    
Epoch: [41][  200/  207]    Overall Loss 0.111587    Objective Loss 0.111587                                        LR 0.001000    Time 0.084712    
Epoch: [41][  207/  207]    Overall Loss 0.111725    Objective Loss 0.111725    Top1 93.884485    Top5 100.000000    LR 0.001000    Time 0.084355    
--- validate (epoch=41)-----------
5136 samples (512 per mini-batch)
Epoch: [41][   10/   11]    Loss 0.519671    Top1 81.875000    Top5 99.667969    
Epoch: [41][   11/   11]    Loss 0.574896    Top1 81.814642    Top5 99.669003    
==> Top1: 81.815    Top5: 99.669    Loss: 0.575

==> Confusion:
[[259   4   3   1   1   5   2  25]
 [  2 253  36   2   0   1   0   6]
 [  1  10 276   1   0   6   0   6]
 [  0   4   0 739  61   8  11  14]
 [  1   0   0  41 795   3  18  21]
 [  5   4  10  18  30 777  13  37]
 [  1   0   0   4  40  13 757  22]
 [ 28  20  22  62  94 169  48 346]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [42][   10/  207]    Overall Loss 0.113886    Objective Loss 0.113886                                        LR 0.001000    Time 0.147477    
Epoch: [42][   20/  207]    Overall Loss 0.103240    Objective Loss 0.103240                                        LR 0.001000    Time 0.110723    
Epoch: [42][   30/  207]    Overall Loss 0.103182    Objective Loss 0.103182                                        LR 0.001000    Time 0.098703    
Epoch: [42][   40/  207]    Overall Loss 0.109009    Objective Loss 0.109009                                        LR 0.001000    Time 0.092633    
Epoch: [42][   50/  207]    Overall Loss 0.115583    Objective Loss 0.115583                                        LR 0.001000    Time 0.089287    
Epoch: [42][   60/  207]    Overall Loss 0.118426    Objective Loss 0.118426                                        LR 0.001000    Time 0.086519    
Epoch: [42][   70/  207]    Overall Loss 0.121715    Objective Loss 0.121715                                        LR 0.001000    Time 0.084869    
Epoch: [42][   80/  207]    Overall Loss 0.123826    Objective Loss 0.123826                                        LR 0.001000    Time 0.084296    
Epoch: [42][   90/  207]    Overall Loss 0.125557    Objective Loss 0.125557                                        LR 0.001000    Time 0.084028    
Epoch: [42][  100/  207]    Overall Loss 0.128369    Objective Loss 0.128369                                        LR 0.001000    Time 0.083540    
Epoch: [42][  110/  207]    Overall Loss 0.129575    Objective Loss 0.129575                                        LR 0.001000    Time 0.083253    
Epoch: [42][  120/  207]    Overall Loss 0.128541    Objective Loss 0.128541                                        LR 0.001000    Time 0.083058    
Epoch: [42][  130/  207]    Overall Loss 0.129990    Objective Loss 0.129990                                        LR 0.001000    Time 0.082627    
Epoch: [42][  140/  207]    Overall Loss 0.133321    Objective Loss 0.133321                                        LR 0.001000    Time 0.082161    
Epoch: [42][  150/  207]    Overall Loss 0.135517    Objective Loss 0.135517                                        LR 0.001000    Time 0.081504    
Epoch: [42][  160/  207]    Overall Loss 0.138349    Objective Loss 0.138349                                        LR 0.001000    Time 0.081170    
Epoch: [42][  170/  207]    Overall Loss 0.137989    Objective Loss 0.137989                                        LR 0.001000    Time 0.080859    
Epoch: [42][  180/  207]    Overall Loss 0.139271    Objective Loss 0.139271                                        LR 0.001000    Time 0.080624    
Epoch: [42][  190/  207]    Overall Loss 0.142697    Objective Loss 0.142697                                        LR 0.001000    Time 0.080204    
Epoch: [42][  200/  207]    Overall Loss 0.144817    Objective Loss 0.144817                                        LR 0.001000    Time 0.079856    
Epoch: [42][  207/  207]    Overall Loss 0.145479    Objective Loss 0.145479    Top1 91.619479    Top5 99.886750    LR 0.001000    Time 0.079502    
--- validate (epoch=42)-----------
5136 samples (512 per mini-batch)
Epoch: [42][   10/   11]    Loss 0.498971    Top1 80.527344    Top5 99.726562    
Epoch: [42][   11/   11]    Loss 0.462806    Top1 80.549065    Top5 99.727414    
==> Top1: 80.549    Top5: 99.727    Loss: 0.463

==> Confusion:
[[268   4   3   0   1   3   4  17]
 [  3 258  28   0   0   1   1   9]
 [  3  21 263   2   0   7   0   4]
 [  1   6   0 729  61  11  13  16]
 [  2   0   0  40 786   7  26  18]
 [  7   6  12  12  24 813   5  15]
 [  3   0   0   3  42  34 732  23]
 [ 48  18  25  65  94 227  24 288]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [43][   10/  207]    Overall Loss 0.122041    Objective Loss 0.122041                                        LR 0.001000    Time 0.143807    
Epoch: [43][   20/  207]    Overall Loss 0.135084    Objective Loss 0.135084                                        LR 0.001000    Time 0.110883    
Epoch: [43][   30/  207]    Overall Loss 0.137134    Objective Loss 0.137134                                        LR 0.001000    Time 0.099654    
Epoch: [43][   40/  207]    Overall Loss 0.133597    Objective Loss 0.133597                                        LR 0.001000    Time 0.094268    
Epoch: [43][   50/  207]    Overall Loss 0.133820    Objective Loss 0.133820                                        LR 0.001000    Time 0.091135    
Epoch: [43][   60/  207]    Overall Loss 0.133199    Objective Loss 0.133199                                        LR 0.001000    Time 0.088489    
Epoch: [43][   70/  207]    Overall Loss 0.135849    Objective Loss 0.135849                                        LR 0.001000    Time 0.086162    
Epoch: [43][   80/  207]    Overall Loss 0.140346    Objective Loss 0.140346                                        LR 0.001000    Time 0.084664    
Epoch: [43][   90/  207]    Overall Loss 0.142824    Objective Loss 0.142824                                        LR 0.001000    Time 0.083267    
Epoch: [43][  100/  207]    Overall Loss 0.143004    Objective Loss 0.143004                                        LR 0.001000    Time 0.082364    
Epoch: [43][  110/  207]    Overall Loss 0.142984    Objective Loss 0.142984                                        LR 0.001000    Time 0.081745    
Epoch: [43][  120/  207]    Overall Loss 0.142492    Objective Loss 0.142492                                        LR 0.001000    Time 0.081401    
Epoch: [43][  130/  207]    Overall Loss 0.142353    Objective Loss 0.142353                                        LR 0.001000    Time 0.082022    
Epoch: [43][  140/  207]    Overall Loss 0.141676    Objective Loss 0.141676                                        LR 0.001000    Time 0.082553    
Epoch: [43][  150/  207]    Overall Loss 0.140468    Objective Loss 0.140468                                        LR 0.001000    Time 0.083171    
Epoch: [43][  160/  207]    Overall Loss 0.139714    Objective Loss 0.139714                                        LR 0.001000    Time 0.083484    
Epoch: [43][  170/  207]    Overall Loss 0.137985    Objective Loss 0.137985                                        LR 0.001000    Time 0.083852    
Epoch: [43][  180/  207]    Overall Loss 0.136799    Objective Loss 0.136799                                        LR 0.001000    Time 0.083252    
Epoch: [43][  190/  207]    Overall Loss 0.137010    Objective Loss 0.137010                                        LR 0.001000    Time 0.082799    
Epoch: [43][  200/  207]    Overall Loss 0.137100    Objective Loss 0.137100                                        LR 0.001000    Time 0.082225    
Epoch: [43][  207/  207]    Overall Loss 0.136895    Objective Loss 0.136895    Top1 92.525481    Top5 100.000000    LR 0.001000    Time 0.081733    
--- validate (epoch=43)-----------
5136 samples (512 per mini-batch)
Epoch: [43][   10/   11]    Loss 0.648635    Top1 79.042969    Top5 99.609375    
Epoch: [43][   11/   11]    Loss 0.608071    Top1 79.088785    Top5 99.610592    
==> Top1: 79.089    Top5: 99.611    Loss: 0.608

==> Confusion:
[[235   5   7   1   0   9  10  33]
 [  1 252  32   1   0   1   2  11]
 [  0  28 256   1   0   4   1  10]
 [  0   4   1 784  26   4   6  12]
 [  1   0   0 142 694   4  20  18]
 [  2   5  14  40  26 737  30  40]
 [  1   0   0  11  42  11 757  15]
 [ 20  18  16 100  70 166  52 347]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [44][   10/  207]    Overall Loss 0.141842    Objective Loss 0.141842                                        LR 0.001000    Time 0.131416    
Epoch: [44][   20/  207]    Overall Loss 0.140286    Objective Loss 0.140286                                        LR 0.001000    Time 0.102395    
Epoch: [44][   30/  207]    Overall Loss 0.148489    Objective Loss 0.148489                                        LR 0.001000    Time 0.092279    
Epoch: [44][   40/  207]    Overall Loss 0.147421    Objective Loss 0.147421                                        LR 0.001000    Time 0.087287    
Epoch: [44][   50/  207]    Overall Loss 0.147540    Objective Loss 0.147540                                        LR 0.001000    Time 0.084266    
Epoch: [44][   60/  207]    Overall Loss 0.141888    Objective Loss 0.141888                                        LR 0.001000    Time 0.081962    
Epoch: [44][   70/  207]    Overall Loss 0.140043    Objective Loss 0.140043                                        LR 0.001000    Time 0.080445    
Epoch: [44][   80/  207]    Overall Loss 0.137717    Objective Loss 0.137717                                        LR 0.001000    Time 0.079469    
Epoch: [44][   90/  207]    Overall Loss 0.136078    Objective Loss 0.136078                                        LR 0.001000    Time 0.078645    
Epoch: [44][  100/  207]    Overall Loss 0.134497    Objective Loss 0.134497                                        LR 0.001000    Time 0.078110    
Epoch: [44][  110/  207]    Overall Loss 0.132946    Objective Loss 0.132946                                        LR 0.001000    Time 0.077587    
Epoch: [44][  120/  207]    Overall Loss 0.131387    Objective Loss 0.131387                                        LR 0.001000    Time 0.077224    
Epoch: [44][  130/  207]    Overall Loss 0.129792    Objective Loss 0.129792                                        LR 0.001000    Time 0.076873    
Epoch: [44][  140/  207]    Overall Loss 0.127742    Objective Loss 0.127742                                        LR 0.001000    Time 0.076480    
Epoch: [44][  150/  207]    Overall Loss 0.126089    Objective Loss 0.126089                                        LR 0.001000    Time 0.076138    
Epoch: [44][  160/  207]    Overall Loss 0.124542    Objective Loss 0.124542                                        LR 0.001000    Time 0.075778    
Epoch: [44][  170/  207]    Overall Loss 0.123293    Objective Loss 0.123293                                        LR 0.001000    Time 0.075554    
Epoch: [44][  180/  207]    Overall Loss 0.122722    Objective Loss 0.122722                                        LR 0.001000    Time 0.075511    
Epoch: [44][  190/  207]    Overall Loss 0.120882    Objective Loss 0.120882                                        LR 0.001000    Time 0.075340    
Epoch: [44][  200/  207]    Overall Loss 0.120501    Objective Loss 0.120501                                        LR 0.001000    Time 0.075199    
Epoch: [44][  207/  207]    Overall Loss 0.120930    Objective Loss 0.120930    Top1 93.318233    Top5 100.000000    LR 0.001000    Time 0.075040    
--- validate (epoch=44)-----------
5136 samples (512 per mini-batch)
Epoch: [44][   10/   11]    Loss 0.561179    Top1 79.335938    Top5 99.472656    
Epoch: [44][   11/   11]    Loss 0.553890    Top1 79.322430    Top5 99.474299    
==> Top1: 79.322    Top5: 99.474    Loss: 0.554

==> Confusion:
[[260   1   2   4   2  11   5  15]
 [  4 242  35   7   0   3   1   8]
 [  2  12 268   1   1  10   0   6]
 [  1   0   0 793  26   8   9   0]
 [  1   0   0 123 722   2  18  13]
 [  6   3   8  45  32 776  15   9]
 [  2   0   0  14  46  13 745  17]
 [ 29  15  15 138  80 208  36 268]]

==> Best [Top1: 81.815   Top5: 99.669   Sparsity:0.00   Params: 117200 on epoch: 41]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [45][   10/  207]    Overall Loss 0.113734    Objective Loss 0.113734                                        LR 0.000500    Time 0.129995    
Epoch: [45][   20/  207]    Overall Loss 0.103953    Objective Loss 0.103953                                        LR 0.000500    Time 0.100315    
Epoch: [45][   30/  207]    Overall Loss 0.096061    Objective Loss 0.096061                                        LR 0.000500    Time 0.091395    
Epoch: [45][   40/  207]    Overall Loss 0.092135    Objective Loss 0.092135                                        LR 0.000500    Time 0.087016    
Epoch: [45][   50/  207]    Overall Loss 0.089604    Objective Loss 0.089604                                        LR 0.000500    Time 0.084422    
Epoch: [45][   60/  207]    Overall Loss 0.086446    Objective Loss 0.086446                                        LR 0.000500    Time 0.082686    
Epoch: [45][   70/  207]    Overall Loss 0.085132    Objective Loss 0.085132                                        LR 0.000500    Time 0.081339    
Epoch: [45][   80/  207]    Overall Loss 0.083017    Objective Loss 0.083017                                        LR 0.000500    Time 0.080066    
Epoch: [45][   90/  207]    Overall Loss 0.081296    Objective Loss 0.081296                                        LR 0.000500    Time 0.079317    
Epoch: [45][  100/  207]    Overall Loss 0.081693    Objective Loss 0.081693                                        LR 0.000500    Time 0.078852    
Epoch: [45][  110/  207]    Overall Loss 0.080502    Objective Loss 0.080502                                        LR 0.000500    Time 0.078156    
Epoch: [45][  120/  207]    Overall Loss 0.080109    Objective Loss 0.080109                                        LR 0.000500    Time 0.077564    
Epoch: [45][  130/  207]    Overall Loss 0.079818    Objective Loss 0.079818                                        LR 0.000500    Time 0.077196    
Epoch: [45][  140/  207]    Overall Loss 0.079271    Objective Loss 0.079271                                        LR 0.000500    Time 0.076879    
Epoch: [45][  150/  207]    Overall Loss 0.079343    Objective Loss 0.079343                                        LR 0.000500    Time 0.076544    
Epoch: [45][  160/  207]    Overall Loss 0.079026    Objective Loss 0.079026                                        LR 0.000500    Time 0.076205    
Epoch: [45][  170/  207]    Overall Loss 0.079047    Objective Loss 0.079047                                        LR 0.000500    Time 0.075996    
Epoch: [45][  180/  207]    Overall Loss 0.078501    Objective Loss 0.078501                                        LR 0.000500    Time 0.075738    
Epoch: [45][  190/  207]    Overall Loss 0.078228    Objective Loss 0.078228                                        LR 0.000500    Time 0.075586    
Epoch: [45][  200/  207]    Overall Loss 0.078316    Objective Loss 0.078316                                        LR 0.000500    Time 0.075469    
Epoch: [45][  207/  207]    Overall Loss 0.078291    Objective Loss 0.078291    Top1 93.771234    Top5 99.886750    LR 0.000500    Time 0.075193    
--- validate (epoch=45)-----------
5136 samples (512 per mini-batch)
Epoch: [45][   10/   11]    Loss 0.486998    Top1 82.246094    Top5 99.609375    
Epoch: [45][   11/   11]    Loss 0.446035    Top1 82.301402    Top5 99.610592    
==> Top1: 82.301    Top5: 99.611    Loss: 0.446

==> Confusion:
[[271   3   1   2   1   2   5  15]
 [  2 267  19   3   0   1   1   7]
 [  4  22 252   1   0   7   0  14]
 [  0   1   1 747  46  17  15  10]
 [  1   0   0  46 784   9  24  15]
 [ 10   5   7  13  24 787  24  24]
 [  2   0   0   5  25  13 777  15]
 [ 33  18  18  69  79 171  59 342]]

==> Best [Top1: 82.301   Top5: 99.611   Sparsity:0.00   Params: 117200 on epoch: 45]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [46][   10/  207]    Overall Loss 0.065926    Objective Loss 0.065926                                        LR 0.000500    Time 0.144580    
Epoch: [46][   20/  207]    Overall Loss 0.065250    Objective Loss 0.065250                                        LR 0.000500    Time 0.111000    
Epoch: [46][   30/  207]    Overall Loss 0.065546    Objective Loss 0.065546                                        LR 0.000500    Time 0.100411    
Epoch: [46][   40/  207]    Overall Loss 0.064749    Objective Loss 0.064749                                        LR 0.000500    Time 0.095009    
Epoch: [46][   50/  207]    Overall Loss 0.064108    Objective Loss 0.064108                                        LR 0.000500    Time 0.091268    
Epoch: [46][   60/  207]    Overall Loss 0.064914    Objective Loss 0.064914                                        LR 0.000500    Time 0.089056    
Epoch: [46][   70/  207]    Overall Loss 0.064119    Objective Loss 0.064119                                        LR 0.000500    Time 0.087372    
Epoch: [46][   80/  207]    Overall Loss 0.064445    Objective Loss 0.064445                                        LR 0.000500    Time 0.086339    
Epoch: [46][   90/  207]    Overall Loss 0.065051    Objective Loss 0.065051                                        LR 0.000500    Time 0.085849    
Epoch: [46][  100/  207]    Overall Loss 0.064785    Objective Loss 0.064785                                        LR 0.000500    Time 0.084595    
Epoch: [46][  110/  207]    Overall Loss 0.064747    Objective Loss 0.064747                                        LR 0.000500    Time 0.083666    
Epoch: [46][  120/  207]    Overall Loss 0.064974    Objective Loss 0.064974                                        LR 0.000500    Time 0.082982    
Epoch: [46][  130/  207]    Overall Loss 0.065530    Objective Loss 0.065530                                        LR 0.000500    Time 0.082232    
Epoch: [46][  140/  207]    Overall Loss 0.066043    Objective Loss 0.066043                                        LR 0.000500    Time 0.081629    
Epoch: [46][  150/  207]    Overall Loss 0.066561    Objective Loss 0.066561                                        LR 0.000500    Time 0.081074    
Epoch: [46][  160/  207]    Overall Loss 0.066478    Objective Loss 0.066478                                        LR 0.000500    Time 0.080622    
Epoch: [46][  170/  207]    Overall Loss 0.066895    Objective Loss 0.066895                                        LR 0.000500    Time 0.080572    
Epoch: [46][  180/  207]    Overall Loss 0.067072    Objective Loss 0.067072                                        LR 0.000500    Time 0.080521    
Epoch: [46][  190/  207]    Overall Loss 0.067252    Objective Loss 0.067252                                        LR 0.000500    Time 0.080609    
Epoch: [46][  200/  207]    Overall Loss 0.067145    Objective Loss 0.067145                                        LR 0.000500    Time 0.080473    
Epoch: [46][  207/  207]    Overall Loss 0.067053    Objective Loss 0.067053    Top1 96.375991    Top5 100.000000    LR 0.000500    Time 0.080309    
--- validate (epoch=46)-----------
5136 samples (512 per mini-batch)
Epoch: [46][   10/   11]    Loss 0.451315    Top1 82.675781    Top5 99.746094    
Epoch: [46][   11/   11]    Loss 0.503498    Top1 82.612928    Top5 99.727414    
==> Top1: 82.613    Top5: 99.727    Loss: 0.503

==> Confusion:
[[270   3   3   2   1   3   4  14]
 [  2 268  21   3   0   1   1   4]
 [  2  22 264   2   0   7   0   3]
 [  0   5   0 750  46  12  16   8]
 [  1   0   0  50 781  10  23  14]
 [  8   5   9  15  23 780  23  31]
 [  2   0   0   5  31  10 777  12]
 [ 33  22  17  71  75 162  56 353]]

==> Best [Top1: 82.613   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 46]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [47][   10/  207]    Overall Loss 0.056289    Objective Loss 0.056289                                        LR 0.000500    Time 0.135822    
Epoch: [47][   20/  207]    Overall Loss 0.063219    Objective Loss 0.063219                                        LR 0.000500    Time 0.104979    
Epoch: [47][   30/  207]    Overall Loss 0.065726    Objective Loss 0.065726                                        LR 0.000500    Time 0.094636    
Epoch: [47][   40/  207]    Overall Loss 0.063737    Objective Loss 0.063737                                        LR 0.000500    Time 0.089148    
Epoch: [47][   50/  207]    Overall Loss 0.064930    Objective Loss 0.064930                                        LR 0.000500    Time 0.085813    
Epoch: [47][   60/  207]    Overall Loss 0.063243    Objective Loss 0.063243                                        LR 0.000500    Time 0.083626    
Epoch: [47][   70/  207]    Overall Loss 0.063275    Objective Loss 0.063275                                        LR 0.000500    Time 0.082354    
Epoch: [47][   80/  207]    Overall Loss 0.063964    Objective Loss 0.063964                                        LR 0.000500    Time 0.082005    
Epoch: [47][   90/  207]    Overall Loss 0.063453    Objective Loss 0.063453                                        LR 0.000500    Time 0.081552    
Epoch: [47][  100/  207]    Overall Loss 0.063135    Objective Loss 0.063135                                        LR 0.000500    Time 0.081321    
Epoch: [47][  110/  207]    Overall Loss 0.062703    Objective Loss 0.062703                                        LR 0.000500    Time 0.081227    
Epoch: [47][  120/  207]    Overall Loss 0.063030    Objective Loss 0.063030                                        LR 0.000500    Time 0.080922    
Epoch: [47][  130/  207]    Overall Loss 0.063348    Objective Loss 0.063348                                        LR 0.000500    Time 0.080967    
Epoch: [47][  140/  207]    Overall Loss 0.063086    Objective Loss 0.063086                                        LR 0.000500    Time 0.080467    
Epoch: [47][  150/  207]    Overall Loss 0.063226    Objective Loss 0.063226                                        LR 0.000500    Time 0.080002    
Epoch: [47][  160/  207]    Overall Loss 0.063740    Objective Loss 0.063740                                        LR 0.000500    Time 0.079754    
Epoch: [47][  170/  207]    Overall Loss 0.063869    Objective Loss 0.063869                                        LR 0.000500    Time 0.079564    
Epoch: [47][  180/  207]    Overall Loss 0.063965    Objective Loss 0.063965                                        LR 0.000500    Time 0.079322    
Epoch: [47][  190/  207]    Overall Loss 0.064035    Objective Loss 0.064035                                        LR 0.000500    Time 0.079084    
Epoch: [47][  200/  207]    Overall Loss 0.064287    Objective Loss 0.064287                                        LR 0.000500    Time 0.078834    
Epoch: [47][  207/  207]    Overall Loss 0.064240    Objective Loss 0.064240    Top1 94.224236    Top5 99.886750    LR 0.000500    Time 0.078743    
--- validate (epoch=47)-----------
5136 samples (512 per mini-batch)
Epoch: [47][   10/   11]    Loss 0.455553    Top1 81.992188    Top5 99.785156    
Epoch: [47][   11/   11]    Loss 0.414702    Top1 82.048287    Top5 99.785826    
==> Top1: 82.048    Top5: 99.786    Loss: 0.415

==> Confusion:
[[277   2   2   1   1   2   4  11]
 [  3 264  25   1   0   2   1   4]
 [  7  23 261   0   0   7   0   2]
 [  0   4   0 735  54  22   8  14]
 [  1   0   0  44 786  11  22  15]
 [ 12   8   8  11  21 806   9  19]
 [  2   0   0   5  36  19 757  18]
 [ 55  22  20  52  84 192  36 328]]

==> Best [Top1: 82.613   Top5: 99.727   Sparsity:0.00   Params: 117200 on epoch: 46]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [48][   10/  207]    Overall Loss 0.057898    Objective Loss 0.057898                                        LR 0.000500    Time 0.142021    
Epoch: [48][   20/  207]    Overall Loss 0.054965    Objective Loss 0.054965                                        LR 0.000500    Time 0.110071    
Epoch: [48][   30/  207]    Overall Loss 0.056548    Objective Loss 0.056548                                        LR 0.000500    Time 0.099482    
Epoch: [48][   40/  207]    Overall Loss 0.058732    Objective Loss 0.058732                                        LR 0.000500    Time 0.093887    
Epoch: [48][   50/  207]    Overall Loss 0.059108    Objective Loss 0.059108                                        LR 0.000500    Time 0.089546    
Epoch: [48][   60/  207]    Overall Loss 0.059262    Objective Loss 0.059262                                        LR 0.000500    Time 0.087041    
Epoch: [48][   70/  207]    Overall Loss 0.059584    Objective Loss 0.059584                                        LR 0.000500    Time 0.085344    
Epoch: [48][   80/  207]    Overall Loss 0.060094    Objective Loss 0.060094                                        LR 0.000500    Time 0.083887    
Epoch: [48][   90/  207]    Overall Loss 0.059977    Objective Loss 0.059977                                        LR 0.000500    Time 0.082872    
Epoch: [48][  100/  207]    Overall Loss 0.059781    Objective Loss 0.059781                                        LR 0.000500    Time 0.082013    
Epoch: [48][  110/  207]    Overall Loss 0.060072    Objective Loss 0.060072                                        LR 0.000500    Time 0.081461    
Epoch: [48][  120/  207]    Overall Loss 0.059834    Objective Loss 0.059834                                        LR 0.000500    Time 0.081588    
Epoch: [48][  130/  207]    Overall Loss 0.059491    Objective Loss 0.059491                                        LR 0.000500    Time 0.081216    
Epoch: [48][  140/  207]    Overall Loss 0.059909    Objective Loss 0.059909                                        LR 0.000500    Time 0.081000    
Epoch: [48][  150/  207]    Overall Loss 0.059692    Objective Loss 0.059692                                        LR 0.000500    Time 0.080920    
Epoch: [48][  160/  207]    Overall Loss 0.060232    Objective Loss 0.060232                                        LR 0.000500    Time 0.080971    
Epoch: [48][  170/  207]    Overall Loss 0.060285    Objective Loss 0.060285                                        LR 0.000500    Time 0.080851    
Epoch: [48][  180/  207]    Overall Loss 0.060734    Objective Loss 0.060734                                        LR 0.000500    Time 0.080452    
Epoch: [48][  190/  207]    Overall Loss 0.060801    Objective Loss 0.060801                                        LR 0.000500    Time 0.080111    
Epoch: [48][  200/  207]    Overall Loss 0.060972    Objective Loss 0.060972                                        LR 0.000500    Time 0.080014    
Epoch: [48][  207/  207]    Overall Loss 0.061074    Objective Loss 0.061074    Top1 94.903737    Top5 100.000000    LR 0.000500    Time 0.079648    
--- validate (epoch=48)-----------
5136 samples (512 per mini-batch)
Epoch: [48][   10/   11]    Loss 0.465145    Top1 83.125000    Top5 99.785156    
Epoch: [48][   11/   11]    Loss 0.460964    Top1 83.119159    Top5 99.785826    
==> Top1: 83.119    Top5: 99.786    Loss: 0.461

==> Confusion:
[[272   3   1   1   0   2   3  18]
 [  2 270  20   1   0   1   0   6]
 [  3  21 265   1   0   4   0   6]
 [  0   5   0 750  49  11   9  13]
 [  2   0   0  51 773  13  21  19]
 [ 11   5  10  16  15 800  10  27]
 [  2   0   0   6  25  16 770  18]
 [ 38  23  22  70  65 163  39 369]]

==> Best [Top1: 83.119   Top5: 99.786   Sparsity:0.00   Params: 117200 on epoch: 48]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [49][   10/  207]    Overall Loss 0.053204    Objective Loss 0.053204                                        LR 0.000500    Time 0.133194    
Epoch: [49][   20/  207]    Overall Loss 0.055213    Objective Loss 0.055213                                        LR 0.000500    Time 0.102795    
Epoch: [49][   30/  207]    Overall Loss 0.056396    Objective Loss 0.056396                                        LR 0.000500    Time 0.092785    
Epoch: [49][   40/  207]    Overall Loss 0.057237    Objective Loss 0.057237                                        LR 0.000500    Time 0.088058    
Epoch: [49][   50/  207]    Overall Loss 0.057330    Objective Loss 0.057330                                        LR 0.000500    Time 0.085153    
Epoch: [49][   60/  207]    Overall Loss 0.055991    Objective Loss 0.055991                                        LR 0.000500    Time 0.083267    
Epoch: [49][   70/  207]    Overall Loss 0.056129    Objective Loss 0.056129                                        LR 0.000500    Time 0.084304    
Epoch: [49][   80/  207]    Overall Loss 0.056316    Objective Loss 0.056316                                        LR 0.000500    Time 0.083669    
Epoch: [49][   90/  207]    Overall Loss 0.057004    Objective Loss 0.057004                                        LR 0.000500    Time 0.084198    
Epoch: [49][  100/  207]    Overall Loss 0.057263    Objective Loss 0.057263                                        LR 0.000500    Time 0.085088    
Epoch: [49][  110/  207]    Overall Loss 0.057070    Objective Loss 0.057070                                        LR 0.000500    Time 0.084577    
Epoch: [49][  120/  207]    Overall Loss 0.056400    Objective Loss 0.056400                                        LR 0.000500    Time 0.083818    
Epoch: [49][  130/  207]    Overall Loss 0.056534    Objective Loss 0.056534                                        LR 0.000500    Time 0.083321    
Epoch: [49][  140/  207]    Overall Loss 0.056480    Objective Loss 0.056480                                        LR 0.000500    Time 0.083023    
Epoch: [49][  150/  207]    Overall Loss 0.056217    Objective Loss 0.056217                                        LR 0.000500    Time 0.082417    
Epoch: [49][  160/  207]    Overall Loss 0.056450    Objective Loss 0.056450                                        LR 0.000500    Time 0.081947    
Epoch: [49][  170/  207]    Overall Loss 0.056907    Objective Loss 0.056907                                        LR 0.000500    Time 0.081567    
Epoch: [49][  180/  207]    Overall Loss 0.057255    Objective Loss 0.057255                                        LR 0.000500    Time 0.081085    
Epoch: [49][  190/  207]    Overall Loss 0.057509    Objective Loss 0.057509                                        LR 0.000500    Time 0.080674    
Epoch: [49][  200/  207]    Overall Loss 0.057904    Objective Loss 0.057904                                        LR 0.000500    Time 0.080301    
Epoch: [49][  207/  207]    Overall Loss 0.058327    Objective Loss 0.058327    Top1 96.602492    Top5 99.886750    LR 0.000500    Time 0.079923    
--- validate (epoch=49)-----------
5136 samples (512 per mini-batch)
Epoch: [49][   10/   11]    Loss 0.510462    Top1 83.300781    Top5 99.746094    
Epoch: [49][   11/   11]    Loss 0.481636    Top1 83.294393    Top5 99.746885    
==> Top1: 83.294    Top5: 99.747    Loss: 0.482

==> Confusion:
[[268   1   1   1   1   3   4  21]
 [  2 257  29   1   0   2   0   9]
 [  2  16 266   1   0   5   0  10]
 [  0   3   0 742  48  10  15  19]
 [  1   0   0  42 778   6  30  22]
 [  7   4  10  12  23 785  17  36]
 [  2   0   0   5  26  12 769  23]
 [ 23  16  18  50  73 156  40 413]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [50][   10/  207]    Overall Loss 0.054373    Objective Loss 0.054373                                        LR 0.000500    Time 0.131172    
Epoch: [50][   20/  207]    Overall Loss 0.055796    Objective Loss 0.055796                                        LR 0.000500    Time 0.102009    
Epoch: [50][   30/  207]    Overall Loss 0.055237    Objective Loss 0.055237                                        LR 0.000500    Time 0.092592    
Epoch: [50][   40/  207]    Overall Loss 0.055812    Objective Loss 0.055812                                        LR 0.000500    Time 0.088169    
Epoch: [50][   50/  207]    Overall Loss 0.056766    Objective Loss 0.056766                                        LR 0.000500    Time 0.085927    
Epoch: [50][   60/  207]    Overall Loss 0.057364    Objective Loss 0.057364                                        LR 0.000500    Time 0.084597    
Epoch: [50][   70/  207]    Overall Loss 0.057263    Objective Loss 0.057263                                        LR 0.000500    Time 0.083620    
Epoch: [50][   80/  207]    Overall Loss 0.057487    Objective Loss 0.057487                                        LR 0.000500    Time 0.082863    
Epoch: [50][   90/  207]    Overall Loss 0.057627    Objective Loss 0.057627                                        LR 0.000500    Time 0.082351    
Epoch: [50][  100/  207]    Overall Loss 0.057507    Objective Loss 0.057507                                        LR 0.000500    Time 0.082199    
Epoch: [50][  110/  207]    Overall Loss 0.057143    Objective Loss 0.057143                                        LR 0.000500    Time 0.081899    
Epoch: [50][  120/  207]    Overall Loss 0.057533    Objective Loss 0.057533                                        LR 0.000500    Time 0.081452    
Epoch: [50][  130/  207]    Overall Loss 0.058085    Objective Loss 0.058085                                        LR 0.000500    Time 0.080855    
Epoch: [50][  140/  207]    Overall Loss 0.058390    Objective Loss 0.058390                                        LR 0.000500    Time 0.080326    
Epoch: [50][  150/  207]    Overall Loss 0.058527    Objective Loss 0.058527                                        LR 0.000500    Time 0.079911    
Epoch: [50][  160/  207]    Overall Loss 0.058754    Objective Loss 0.058754                                        LR 0.000500    Time 0.079626    
Epoch: [50][  170/  207]    Overall Loss 0.058715    Objective Loss 0.058715                                        LR 0.000500    Time 0.079371    
Epoch: [50][  180/  207]    Overall Loss 0.058802    Objective Loss 0.058802                                        LR 0.000500    Time 0.079269    
Epoch: [50][  190/  207]    Overall Loss 0.058769    Objective Loss 0.058769                                        LR 0.000500    Time 0.079339    
Epoch: [50][  200/  207]    Overall Loss 0.059165    Objective Loss 0.059165                                        LR 0.000500    Time 0.079336    
Epoch: [50][  207/  207]    Overall Loss 0.059368    Objective Loss 0.059368    Top1 93.657984    Top5 99.886750    LR 0.000500    Time 0.079212    
--- validate (epoch=50)-----------
5136 samples (512 per mini-batch)
Epoch: [50][   10/   11]    Loss 0.458552    Top1 82.500000    Top5 99.746094    
Epoch: [50][   11/   11]    Loss 0.423091    Top1 82.515576    Top5 99.746885    
==> Top1: 82.516    Top5: 99.747    Loss: 0.423

==> Confusion:
[[274   3   4   2   1   2   4  10]
 [  4 261  29   2   0   1   1   2]
 [  4  23 267   1   0   5   0   0]
 [  0   5   0 734  63  10  17   8]
 [  1   0   0  36 793   6  30  13]
 [ 11   5  12  13  21 785  23  24]
 [  2   0   0   3  31  13 779   9]
 [ 38  22  23  55  88 163  55 345]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [51][   10/  207]    Overall Loss 0.050954    Objective Loss 0.050954                                        LR 0.000500    Time 0.145053    
Epoch: [51][   20/  207]    Overall Loss 0.053954    Objective Loss 0.053954                                        LR 0.000500    Time 0.110178    
Epoch: [51][   30/  207]    Overall Loss 0.054845    Objective Loss 0.054845                                        LR 0.000500    Time 0.097964    
Epoch: [51][   40/  207]    Overall Loss 0.056118    Objective Loss 0.056118                                        LR 0.000500    Time 0.091736    
Epoch: [51][   50/  207]    Overall Loss 0.055055    Objective Loss 0.055055                                        LR 0.000500    Time 0.087769    
Epoch: [51][   60/  207]    Overall Loss 0.055752    Objective Loss 0.055752                                        LR 0.000500    Time 0.085615    
Epoch: [51][   70/  207]    Overall Loss 0.055929    Objective Loss 0.055929                                        LR 0.000500    Time 0.083692    
Epoch: [51][   80/  207]    Overall Loss 0.055746    Objective Loss 0.055746                                        LR 0.000500    Time 0.082503    
Epoch: [51][   90/  207]    Overall Loss 0.055264    Objective Loss 0.055264                                        LR 0.000500    Time 0.081755    
Epoch: [51][  100/  207]    Overall Loss 0.055555    Objective Loss 0.055555                                        LR 0.000500    Time 0.081779    
Epoch: [51][  110/  207]    Overall Loss 0.055553    Objective Loss 0.055553                                        LR 0.000500    Time 0.082117    
Epoch: [51][  120/  207]    Overall Loss 0.055220    Objective Loss 0.055220                                        LR 0.000500    Time 0.081977    
Epoch: [51][  130/  207]    Overall Loss 0.055076    Objective Loss 0.055076                                        LR 0.000500    Time 0.081742    
Epoch: [51][  140/  207]    Overall Loss 0.055357    Objective Loss 0.055357                                        LR 0.000500    Time 0.081725    
Epoch: [51][  150/  207]    Overall Loss 0.055631    Objective Loss 0.055631                                        LR 0.000500    Time 0.081392    
Epoch: [51][  160/  207]    Overall Loss 0.056545    Objective Loss 0.056545                                        LR 0.000500    Time 0.080898    
Epoch: [51][  170/  207]    Overall Loss 0.057353    Objective Loss 0.057353                                        LR 0.000500    Time 0.080360    
Epoch: [51][  180/  207]    Overall Loss 0.057803    Objective Loss 0.057803                                        LR 0.000500    Time 0.079916    
Epoch: [51][  190/  207]    Overall Loss 0.058299    Objective Loss 0.058299                                        LR 0.000500    Time 0.079598    
Epoch: [51][  200/  207]    Overall Loss 0.059284    Objective Loss 0.059284                                        LR 0.000500    Time 0.079415    
Epoch: [51][  207/  207]    Overall Loss 0.059488    Objective Loss 0.059488    Top1 95.130238    Top5 100.000000    LR 0.000500    Time 0.079067    
--- validate (epoch=51)-----------
5136 samples (512 per mini-batch)
Epoch: [51][   10/   11]    Loss 0.491299    Top1 82.578125    Top5 99.824219    
Epoch: [51][   11/   11]    Loss 0.490238    Top1 82.554517    Top5 99.824766    
==> Top1: 82.555    Top5: 99.825    Loss: 0.490

==> Confusion:
[[276   2   1   1   0   3   4  13]
 [  5 262  22   1   0   2   1   7]
 [  5  16 267   1   0   5   0   6]
 [  0   3   0 721  70  13  14  16]
 [  2   0   0  27 800  10  21  19]
 [  9   3   9  13  25 800  15  20]
 [  2   0   0   4  39  13 765  14]
 [ 36  19  18  49  89 174  55 349]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [52][   10/  207]    Overall Loss 0.059558    Objective Loss 0.059558                                        LR 0.000500    Time 0.143843    
Epoch: [52][   20/  207]    Overall Loss 0.060960    Objective Loss 0.060960                                        LR 0.000500    Time 0.111054    
Epoch: [52][   30/  207]    Overall Loss 0.059717    Objective Loss 0.059717                                        LR 0.000500    Time 0.099916    
Epoch: [52][   40/  207]    Overall Loss 0.060348    Objective Loss 0.060348                                        LR 0.000500    Time 0.094557    
Epoch: [52][   50/  207]    Overall Loss 0.059725    Objective Loss 0.059725                                        LR 0.000500    Time 0.091681    
Epoch: [52][   60/  207]    Overall Loss 0.058851    Objective Loss 0.058851                                        LR 0.000500    Time 0.089512    
Epoch: [52][   70/  207]    Overall Loss 0.058209    Objective Loss 0.058209                                        LR 0.000500    Time 0.087267    
Epoch: [52][   80/  207]    Overall Loss 0.057823    Objective Loss 0.057823                                        LR 0.000500    Time 0.085591    
Epoch: [52][   90/  207]    Overall Loss 0.057438    Objective Loss 0.057438                                        LR 0.000500    Time 0.084128    
Epoch: [52][  100/  207]    Overall Loss 0.058398    Objective Loss 0.058398                                        LR 0.000500    Time 0.082993    
Epoch: [52][  110/  207]    Overall Loss 0.058827    Objective Loss 0.058827                                        LR 0.000500    Time 0.082198    
Epoch: [52][  120/  207]    Overall Loss 0.059640    Objective Loss 0.059640                                        LR 0.000500    Time 0.081608    
Epoch: [52][  130/  207]    Overall Loss 0.060496    Objective Loss 0.060496                                        LR 0.000500    Time 0.080883    
Epoch: [52][  140/  207]    Overall Loss 0.060755    Objective Loss 0.060755                                        LR 0.000500    Time 0.080349    
Epoch: [52][  150/  207]    Overall Loss 0.060731    Objective Loss 0.060731                                        LR 0.000500    Time 0.079842    
Epoch: [52][  160/  207]    Overall Loss 0.061128    Objective Loss 0.061128                                        LR 0.000500    Time 0.079510    
Epoch: [52][  170/  207]    Overall Loss 0.061379    Objective Loss 0.061379                                        LR 0.000500    Time 0.079243    
Epoch: [52][  180/  207]    Overall Loss 0.061546    Objective Loss 0.061546                                        LR 0.000500    Time 0.078905    
Epoch: [52][  190/  207]    Overall Loss 0.061405    Objective Loss 0.061405                                        LR 0.000500    Time 0.078569    
Epoch: [52][  200/  207]    Overall Loss 0.061331    Objective Loss 0.061331                                        LR 0.000500    Time 0.078382    
Epoch: [52][  207/  207]    Overall Loss 0.061358    Objective Loss 0.061358    Top1 95.243488    Top5 100.000000    LR 0.000500    Time 0.078080    
--- validate (epoch=52)-----------
5136 samples (512 per mini-batch)
Epoch: [52][   10/   11]    Loss 0.536345    Top1 83.105469    Top5 99.667969    
Epoch: [52][   11/   11]    Loss 0.491062    Top1 83.138629    Top5 99.669003    
==> Top1: 83.139    Top5: 99.669    Loss: 0.491

==> Confusion:
[[267   3   1   2   1   6   4  16]
 [  3 263  21   3   0   2   0   8]
 [  1  21 254   1   0   8   0  15]
 [  0   3   0 746  52  13  12  11]
 [  1   0   0  43 781  11  20  23]
 [  4   2   8  13  22 797  15  33]
 [  1   0   0   5  32  15 767  17]
 [ 21  17  12  62  77 162  43 395]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [53][   10/  207]    Overall Loss 0.045809    Objective Loss 0.045809                                        LR 0.000500    Time 0.132014    
Epoch: [53][   20/  207]    Overall Loss 0.055972    Objective Loss 0.055972                                        LR 0.000500    Time 0.102673    
Epoch: [53][   30/  207]    Overall Loss 0.060242    Objective Loss 0.060242                                        LR 0.000500    Time 0.092220    
Epoch: [53][   40/  207]    Overall Loss 0.065857    Objective Loss 0.065857                                        LR 0.000500    Time 0.087439    
Epoch: [53][   50/  207]    Overall Loss 0.067083    Objective Loss 0.067083                                        LR 0.000500    Time 0.085072    
Epoch: [53][   60/  207]    Overall Loss 0.069317    Objective Loss 0.069317                                        LR 0.000500    Time 0.082844    
Epoch: [53][   70/  207]    Overall Loss 0.069344    Objective Loss 0.069344                                        LR 0.000500    Time 0.081444    
Epoch: [53][   80/  207]    Overall Loss 0.070143    Objective Loss 0.070143                                        LR 0.000500    Time 0.080350    
Epoch: [53][   90/  207]    Overall Loss 0.070250    Objective Loss 0.070250                                        LR 0.000500    Time 0.079909    
Epoch: [53][  100/  207]    Overall Loss 0.069602    Objective Loss 0.069602                                        LR 0.000500    Time 0.079476    
Epoch: [53][  110/  207]    Overall Loss 0.071126    Objective Loss 0.071126                                        LR 0.000500    Time 0.079037    
Epoch: [53][  120/  207]    Overall Loss 0.073242    Objective Loss 0.073242                                        LR 0.000500    Time 0.078714    
Epoch: [53][  130/  207]    Overall Loss 0.074295    Objective Loss 0.074295                                        LR 0.000500    Time 0.078609    
Epoch: [53][  140/  207]    Overall Loss 0.074195    Objective Loss 0.074195                                        LR 0.000500    Time 0.078223    
Epoch: [53][  150/  207]    Overall Loss 0.074292    Objective Loss 0.074292                                        LR 0.000500    Time 0.077866    
Epoch: [53][  160/  207]    Overall Loss 0.077321    Objective Loss 0.077321                                        LR 0.000500    Time 0.077554    
Epoch: [53][  170/  207]    Overall Loss 0.079563    Objective Loss 0.079563                                        LR 0.000500    Time 0.077372    
Epoch: [53][  180/  207]    Overall Loss 0.081992    Objective Loss 0.081992                                        LR 0.000500    Time 0.077269    
Epoch: [53][  190/  207]    Overall Loss 0.083533    Objective Loss 0.083533                                        LR 0.000500    Time 0.077243    
Epoch: [53][  200/  207]    Overall Loss 0.084384    Objective Loss 0.084384                                        LR 0.000500    Time 0.077114    
Epoch: [53][  207/  207]    Overall Loss 0.085154    Objective Loss 0.085154    Top1 93.431484    Top5 100.000000    LR 0.000500    Time 0.076849    
--- validate (epoch=53)-----------
5136 samples (512 per mini-batch)
Epoch: [53][   10/   11]    Loss 0.499671    Top1 82.382812    Top5 99.765625    
Epoch: [53][   11/   11]    Loss 0.560162    Top1 82.359813    Top5 99.766355    
==> Top1: 82.360    Top5: 99.766    Loss: 0.560

==> Confusion:
[[270   2   4   1   0   9   4  10]
 [  4 249  42   1   0   2   0   2]
 [  6  18 262   0   0  10   0   4]
 [  1   2   1 746  46  16  14  11]
 [  1   0   0  48 775  12  26  17]
 [  9   6   6   9  18 823  10  13]
 [  3   0   0   4  27  20 768  15]
 [ 39  22  12  53  69 208  49 337]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [54][   10/  207]    Overall Loss 0.088261    Objective Loss 0.088261                                        LR 0.000500    Time 0.132832    
Epoch: [54][   20/  207]    Overall Loss 0.087219    Objective Loss 0.087219                                        LR 0.000500    Time 0.102923    
Epoch: [54][   30/  207]    Overall Loss 0.089745    Objective Loss 0.089745                                        LR 0.000500    Time 0.092763    
Epoch: [54][   40/  207]    Overall Loss 0.087845    Objective Loss 0.087845                                        LR 0.000500    Time 0.088071    
Epoch: [54][   50/  207]    Overall Loss 0.085229    Objective Loss 0.085229                                        LR 0.000500    Time 0.085007    
Epoch: [54][   60/  207]    Overall Loss 0.088807    Objective Loss 0.088807                                        LR 0.000500    Time 0.083205    
Epoch: [54][   70/  207]    Overall Loss 0.087983    Objective Loss 0.087983                                        LR 0.000500    Time 0.082176    
Epoch: [54][   80/  207]    Overall Loss 0.086588    Objective Loss 0.086588                                        LR 0.000500    Time 0.081103    
Epoch: [54][   90/  207]    Overall Loss 0.087186    Objective Loss 0.087186                                        LR 0.000500    Time 0.080305    
Epoch: [54][  100/  207]    Overall Loss 0.088416    Objective Loss 0.088416                                        LR 0.000500    Time 0.079716    
Epoch: [54][  110/  207]    Overall Loss 0.090087    Objective Loss 0.090087                                        LR 0.000500    Time 0.079206    
Epoch: [54][  120/  207]    Overall Loss 0.090157    Objective Loss 0.090157                                        LR 0.000500    Time 0.078731    
Epoch: [54][  130/  207]    Overall Loss 0.089808    Objective Loss 0.089808                                        LR 0.000500    Time 0.078310    
Epoch: [54][  140/  207]    Overall Loss 0.088606    Objective Loss 0.088606                                        LR 0.000500    Time 0.077878    
Epoch: [54][  150/  207]    Overall Loss 0.087532    Objective Loss 0.087532                                        LR 0.000500    Time 0.077666    
Epoch: [54][  160/  207]    Overall Loss 0.086463    Objective Loss 0.086463                                        LR 0.000500    Time 0.077380    
Epoch: [54][  170/  207]    Overall Loss 0.085175    Objective Loss 0.085175                                        LR 0.000500    Time 0.077276    
Epoch: [54][  180/  207]    Overall Loss 0.083817    Objective Loss 0.083817                                        LR 0.000500    Time 0.077169    
Epoch: [54][  190/  207]    Overall Loss 0.082866    Objective Loss 0.082866                                        LR 0.000500    Time 0.077108    
Epoch: [54][  200/  207]    Overall Loss 0.081777    Objective Loss 0.081777                                        LR 0.000500    Time 0.076898    
Epoch: [54][  207/  207]    Overall Loss 0.081240    Objective Loss 0.081240    Top1 95.469989    Top5 100.000000    LR 0.000500    Time 0.076631    
--- validate (epoch=54)-----------
5136 samples (512 per mini-batch)
Epoch: [54][   10/   11]    Loss 0.502136    Top1 82.910156    Top5 99.726562    
Epoch: [54][   11/   11]    Loss 0.738012    Top1 82.846573    Top5 99.707944    
==> Top1: 82.847    Top5: 99.708    Loss: 0.738

==> Confusion:
[[268   2   1   0   2   1   3  23]
 [  4 266  20   1   0   1   0   8]
 [  5  23 254   1   0   4   0  13]
 [  1   1   0 736  63   8   8  20]
 [  2   0   0  39 791   4  23  20]
 [  8   3  13  16  24 757  28  45]
 [  2   0   0   6  26   8 776  19]
 [ 32  19  16  52  80 128  55 407]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [55][   10/  207]    Overall Loss 0.060041    Objective Loss 0.060041                                        LR 0.000250    Time 0.133440    
Epoch: [55][   20/  207]    Overall Loss 0.055938    Objective Loss 0.055938                                        LR 0.000250    Time 0.103773    
Epoch: [55][   30/  207]    Overall Loss 0.054685    Objective Loss 0.054685                                        LR 0.000250    Time 0.094067    
Epoch: [55][   40/  207]    Overall Loss 0.053472    Objective Loss 0.053472                                        LR 0.000250    Time 0.090373    
Epoch: [55][   50/  207]    Overall Loss 0.053445    Objective Loss 0.053445                                        LR 0.000250    Time 0.087520    
Epoch: [55][   60/  207]    Overall Loss 0.052788    Objective Loss 0.052788                                        LR 0.000250    Time 0.085219    
Epoch: [55][   70/  207]    Overall Loss 0.052813    Objective Loss 0.052813                                        LR 0.000250    Time 0.083906    
Epoch: [55][   80/  207]    Overall Loss 0.052591    Objective Loss 0.052591                                        LR 0.000250    Time 0.082567    
Epoch: [55][   90/  207]    Overall Loss 0.052377    Objective Loss 0.052377                                        LR 0.000250    Time 0.081589    
Epoch: [55][  100/  207]    Overall Loss 0.052626    Objective Loss 0.052626                                        LR 0.000250    Time 0.080882    
Epoch: [55][  110/  207]    Overall Loss 0.052010    Objective Loss 0.052010                                        LR 0.000250    Time 0.080399    
Epoch: [55][  120/  207]    Overall Loss 0.051544    Objective Loss 0.051544                                        LR 0.000250    Time 0.079950    
Epoch: [55][  130/  207]    Overall Loss 0.051110    Objective Loss 0.051110                                        LR 0.000250    Time 0.079482    
Epoch: [55][  140/  207]    Overall Loss 0.050997    Objective Loss 0.050997                                        LR 0.000250    Time 0.079015    
Epoch: [55][  150/  207]    Overall Loss 0.050401    Objective Loss 0.050401                                        LR 0.000250    Time 0.078609    
Epoch: [55][  160/  207]    Overall Loss 0.050725    Objective Loss 0.050725                                        LR 0.000250    Time 0.078256    
Epoch: [55][  170/  207]    Overall Loss 0.050337    Objective Loss 0.050337                                        LR 0.000250    Time 0.078002    
Epoch: [55][  180/  207]    Overall Loss 0.050407    Objective Loss 0.050407                                        LR 0.000250    Time 0.077735    
Epoch: [55][  190/  207]    Overall Loss 0.050202    Objective Loss 0.050202                                        LR 0.000250    Time 0.077523    
Epoch: [55][  200/  207]    Overall Loss 0.050124    Objective Loss 0.050124                                        LR 0.000250    Time 0.077276    
Epoch: [55][  207/  207]    Overall Loss 0.050158    Objective Loss 0.050158    Top1 96.149490    Top5 100.000000    LR 0.000250    Time 0.077009    
--- validate (epoch=55)-----------
5136 samples (512 per mini-batch)
Epoch: [55][   10/   11]    Loss 0.505600    Top1 83.203125    Top5 99.707031    
Epoch: [55][   11/   11]    Loss 0.513234    Top1 83.177570    Top5 99.707944    
==> Top1: 83.178    Top5: 99.708    Loss: 0.513

==> Confusion:
[[272   2   2   1   0   1   3  19]
 [  3 265  23   1   0   1   0   7]
 [  2  19 265   1   0   3   0  10]
 [  0   4   0 738  54  11  11  19]
 [  2   0   0  42 777   8  27  23]
 [ 11   4  12  11  22 768  21  45]
 [  2   0   0   6  26  12 774  17]
 [ 25  20  19  53  71 142  46 413]]

==> Best [Top1: 83.294   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 49]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [56][   10/  207]    Overall Loss 0.043469    Objective Loss 0.043469                                        LR 0.000250    Time 0.131980    
Epoch: [56][   20/  207]    Overall Loss 0.043671    Objective Loss 0.043671                                        LR 0.000250    Time 0.102648    
Epoch: [56][   30/  207]    Overall Loss 0.045572    Objective Loss 0.045572                                        LR 0.000250    Time 0.092894    
Epoch: [56][   40/  207]    Overall Loss 0.044665    Objective Loss 0.044665                                        LR 0.000250    Time 0.088271    
Epoch: [56][   50/  207]    Overall Loss 0.044244    Objective Loss 0.044244                                        LR 0.000250    Time 0.085452    
Epoch: [56][   60/  207]    Overall Loss 0.043870    Objective Loss 0.043870                                        LR 0.000250    Time 0.083447    
Epoch: [56][   70/  207]    Overall Loss 0.044159    Objective Loss 0.044159                                        LR 0.000250    Time 0.082258    
Epoch: [56][   80/  207]    Overall Loss 0.045204    Objective Loss 0.045204                                        LR 0.000250    Time 0.081019    
Epoch: [56][   90/  207]    Overall Loss 0.045117    Objective Loss 0.045117                                        LR 0.000250    Time 0.080135    
Epoch: [56][  100/  207]    Overall Loss 0.044893    Objective Loss 0.044893                                        LR 0.000250    Time 0.079602    
Epoch: [56][  110/  207]    Overall Loss 0.044603    Objective Loss 0.044603                                        LR 0.000250    Time 0.079083    
Epoch: [56][  120/  207]    Overall Loss 0.044617    Objective Loss 0.044617                                        LR 0.000250    Time 0.078612    
Epoch: [56][  130/  207]    Overall Loss 0.044549    Objective Loss 0.044549                                        LR 0.000250    Time 0.078260    
Epoch: [56][  140/  207]    Overall Loss 0.044738    Objective Loss 0.044738                                        LR 0.000250    Time 0.077994    
Epoch: [56][  150/  207]    Overall Loss 0.045098    Objective Loss 0.045098                                        LR 0.000250    Time 0.077786    
Epoch: [56][  160/  207]    Overall Loss 0.045142    Objective Loss 0.045142                                        LR 0.000250    Time 0.077429    
Epoch: [56][  170/  207]    Overall Loss 0.045269    Objective Loss 0.045269                                        LR 0.000250    Time 0.077307    
Epoch: [56][  180/  207]    Overall Loss 0.045403    Objective Loss 0.045403                                        LR 0.000250    Time 0.077035    
Epoch: [56][  190/  207]    Overall Loss 0.045664    Objective Loss 0.045664                                        LR 0.000250    Time 0.076854    
Epoch: [56][  200/  207]    Overall Loss 0.045488    Objective Loss 0.045488                                        LR 0.000250    Time 0.076684    
Epoch: [56][  207/  207]    Overall Loss 0.045349    Objective Loss 0.045349    Top1 96.828992    Top5 100.000000    LR 0.000250    Time 0.076450    
--- validate (epoch=56)-----------
5136 samples (512 per mini-batch)
Epoch: [56][   10/   11]    Loss 0.506257    Top1 83.554688    Top5 99.687500    
Epoch: [56][   11/   11]    Loss 0.462788    Top1 83.605919    Top5 99.688474    
==> Top1: 83.606    Top5: 99.688    Loss: 0.463

==> Confusion:
[[266   3   3   1   1   3   3  20]
 [  3 262  25   1   0   1   0   8]
 [  2  17 267   1   0   4   0   9]
 [  0   3   0 746  51  13   9  15]
 [  2   0   0  38 785   8  23  23]
 [  4   4  11  14  20 797  10  34]
 [  1   0   0   5  28  14 764  25]
 [ 25  18  16  63  70 157  33 407]]

==> Best [Top1: 83.606   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [57][   10/  207]    Overall Loss 0.041195    Objective Loss 0.041195                                        LR 0.000250    Time 0.135067    
Epoch: [57][   20/  207]    Overall Loss 0.041231    Objective Loss 0.041231                                        LR 0.000250    Time 0.103489    
Epoch: [57][   30/  207]    Overall Loss 0.041118    Objective Loss 0.041118                                        LR 0.000250    Time 0.093240    
Epoch: [57][   40/  207]    Overall Loss 0.043641    Objective Loss 0.043641                                        LR 0.000250    Time 0.088205    
Epoch: [57][   50/  207]    Overall Loss 0.043963    Objective Loss 0.043963                                        LR 0.000250    Time 0.085390    
Epoch: [57][   60/  207]    Overall Loss 0.043987    Objective Loss 0.043987                                        LR 0.000250    Time 0.083262    
Epoch: [57][   70/  207]    Overall Loss 0.043804    Objective Loss 0.043804                                        LR 0.000250    Time 0.082150    
Epoch: [57][   80/  207]    Overall Loss 0.043804    Objective Loss 0.043804                                        LR 0.000250    Time 0.082043    
Epoch: [57][   90/  207]    Overall Loss 0.044053    Objective Loss 0.044053                                        LR 0.000250    Time 0.081695    
Epoch: [57][  100/  207]    Overall Loss 0.043812    Objective Loss 0.043812                                        LR 0.000250    Time 0.081571    
Epoch: [57][  110/  207]    Overall Loss 0.043663    Objective Loss 0.043663                                        LR 0.000250    Time 0.081624    
Epoch: [57][  120/  207]    Overall Loss 0.043933    Objective Loss 0.043933                                        LR 0.000250    Time 0.081348    
Epoch: [57][  130/  207]    Overall Loss 0.043753    Objective Loss 0.043753                                        LR 0.000250    Time 0.081126    
Epoch: [57][  140/  207]    Overall Loss 0.043925    Objective Loss 0.043925                                        LR 0.000250    Time 0.080834    
Epoch: [57][  150/  207]    Overall Loss 0.043872    Objective Loss 0.043872                                        LR 0.000250    Time 0.080270    
Epoch: [57][  160/  207]    Overall Loss 0.043911    Objective Loss 0.043911                                        LR 0.000250    Time 0.079868    
Epoch: [57][  170/  207]    Overall Loss 0.043799    Objective Loss 0.043799                                        LR 0.000250    Time 0.079452    
Epoch: [57][  180/  207]    Overall Loss 0.043834    Objective Loss 0.043834                                        LR 0.000250    Time 0.079157    
Epoch: [57][  190/  207]    Overall Loss 0.043825    Objective Loss 0.043825                                        LR 0.000250    Time 0.079030    
Epoch: [57][  200/  207]    Overall Loss 0.043681    Objective Loss 0.043681                                        LR 0.000250    Time 0.078775    
Epoch: [57][  207/  207]    Overall Loss 0.043828    Objective Loss 0.043828    Top1 96.942242    Top5 100.000000    LR 0.000250    Time 0.078621    
--- validate (epoch=57)-----------
5136 samples (512 per mini-batch)
Epoch: [57][   10/   11]    Loss 0.516810    Top1 83.222656    Top5 99.648438    
Epoch: [57][   11/   11]    Loss 0.473493    Top1 83.255452    Top5 99.649533    
==> Top1: 83.255    Top5: 99.650    Loss: 0.473

==> Confusion:
[[267   3   1   1   2   2   3  21]
 [  1 270  20   1   0   1   0   7]
 [  1  23 257   1   0   5   0  13]
 [  0   4   0 749  50   6  14  14]
 [  2   0   0  46 779   5  27  20]
 [  7   6  10  13  21 760  34  43]
 [  2   0   0   7  28   8 778  14]
 [ 24  17  15  64  67 133  53 416]]

==> Best [Top1: 83.606   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [58][   10/  207]    Overall Loss 0.038166    Objective Loss 0.038166                                        LR 0.000250    Time 0.145532    
Epoch: [58][   20/  207]    Overall Loss 0.039808    Objective Loss 0.039808                                        LR 0.000250    Time 0.111309    
Epoch: [58][   30/  207]    Overall Loss 0.039885    Objective Loss 0.039885                                        LR 0.000250    Time 0.100836    
Epoch: [58][   40/  207]    Overall Loss 0.039938    Objective Loss 0.039938                                        LR 0.000250    Time 0.095144    
Epoch: [58][   50/  207]    Overall Loss 0.040600    Objective Loss 0.040600                                        LR 0.000250    Time 0.092737    
Epoch: [58][   60/  207]    Overall Loss 0.041738    Objective Loss 0.041738                                        LR 0.000250    Time 0.091746    
Epoch: [58][   70/  207]    Overall Loss 0.041432    Objective Loss 0.041432                                        LR 0.000250    Time 0.090211    
Epoch: [58][   80/  207]    Overall Loss 0.041782    Objective Loss 0.041782                                        LR 0.000250    Time 0.088016    
Epoch: [58][   90/  207]    Overall Loss 0.041486    Objective Loss 0.041486                                        LR 0.000250    Time 0.086223    
Epoch: [58][  100/  207]    Overall Loss 0.041395    Objective Loss 0.041395                                        LR 0.000250    Time 0.084993    
Epoch: [58][  110/  207]    Overall Loss 0.041326    Objective Loss 0.041326                                        LR 0.000250    Time 0.083952    
Epoch: [58][  120/  207]    Overall Loss 0.041230    Objective Loss 0.041230                                        LR 0.000250    Time 0.083064    
Epoch: [58][  130/  207]    Overall Loss 0.041437    Objective Loss 0.041437                                        LR 0.000250    Time 0.082246    
Epoch: [58][  140/  207]    Overall Loss 0.041520    Objective Loss 0.041520                                        LR 0.000250    Time 0.081560    
Epoch: [58][  150/  207]    Overall Loss 0.041594    Objective Loss 0.041594                                        LR 0.000250    Time 0.081093    
Epoch: [58][  160/  207]    Overall Loss 0.042015    Objective Loss 0.042015                                        LR 0.000250    Time 0.080617    
Epoch: [58][  170/  207]    Overall Loss 0.042253    Objective Loss 0.042253                                        LR 0.000250    Time 0.080128    
Epoch: [58][  180/  207]    Overall Loss 0.042372    Objective Loss 0.042372                                        LR 0.000250    Time 0.079834    
Epoch: [58][  190/  207]    Overall Loss 0.042493    Objective Loss 0.042493                                        LR 0.000250    Time 0.079366    
Epoch: [58][  200/  207]    Overall Loss 0.042549    Objective Loss 0.042549                                        LR 0.000250    Time 0.079135    
Epoch: [58][  207/  207]    Overall Loss 0.042632    Objective Loss 0.042632    Top1 97.168743    Top5 100.000000    LR 0.000250    Time 0.078888    
--- validate (epoch=58)-----------
5136 samples (512 per mini-batch)
Epoch: [58][   10/   11]    Loss 0.508277    Top1 83.457031    Top5 99.687500    
Epoch: [58][   11/   11]    Loss 0.512093    Top1 83.430685    Top5 99.688474    
==> Top1: 83.431    Top5: 99.688    Loss: 0.512

==> Confusion:
[[270   3   1   2   1   3   3  17]
 [  4 264  22   1   0   1   1   7]
 [  2  19 265   1   0   5   0   8]
 [  0   2   0 757  36  14   9  19]
 [  2   0   0  53 765   8  25  26]
 [  6   6  10  13  19 792  15  33]
 [  2   0   0   5  27  12 767  24]
 [ 27  20  18  60  66 155  38 405]]

==> Best [Top1: 83.606   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 56]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [59][   10/  207]    Overall Loss 0.041534    Objective Loss 0.041534                                        LR 0.000250    Time 0.145599    
Epoch: [59][   20/  207]    Overall Loss 0.040575    Objective Loss 0.040575                                        LR 0.000250    Time 0.116745    
Epoch: [59][   30/  207]    Overall Loss 0.038702    Objective Loss 0.038702                                        LR 0.000250    Time 0.108906    
Epoch: [59][   40/  207]    Overall Loss 0.039964    Objective Loss 0.039964                                        LR 0.000250    Time 0.101878    
Epoch: [59][   50/  207]    Overall Loss 0.039572    Objective Loss 0.039572                                        LR 0.000250    Time 0.101615    
Epoch: [59][   60/  207]    Overall Loss 0.039653    Objective Loss 0.039653                                        LR 0.000250    Time 0.100305    
Epoch: [59][   70/  207]    Overall Loss 0.039420    Objective Loss 0.039420                                        LR 0.000250    Time 0.098664    
Epoch: [59][   80/  207]    Overall Loss 0.039974    Objective Loss 0.039974                                        LR 0.000250    Time 0.096126    
Epoch: [59][   90/  207]    Overall Loss 0.039744    Objective Loss 0.039744                                        LR 0.000250    Time 0.093710    
Epoch: [59][  100/  207]    Overall Loss 0.039909    Objective Loss 0.039909                                        LR 0.000250    Time 0.092024    
Epoch: [59][  110/  207]    Overall Loss 0.040406    Objective Loss 0.040406                                        LR 0.000250    Time 0.090596    
Epoch: [59][  120/  207]    Overall Loss 0.040743    Objective Loss 0.040743                                        LR 0.000250    Time 0.089182    
Epoch: [59][  130/  207]    Overall Loss 0.040809    Objective Loss 0.040809                                        LR 0.000250    Time 0.087927    
Epoch: [59][  140/  207]    Overall Loss 0.041204    Objective Loss 0.041204                                        LR 0.000250    Time 0.087032    
Epoch: [59][  150/  207]    Overall Loss 0.041473    Objective Loss 0.041473                                        LR 0.000250    Time 0.086496    
Epoch: [59][  160/  207]    Overall Loss 0.041720    Objective Loss 0.041720                                        LR 0.000250    Time 0.086197    
Epoch: [59][  170/  207]    Overall Loss 0.041714    Objective Loss 0.041714                                        LR 0.000250    Time 0.085956    
Epoch: [59][  180/  207]    Overall Loss 0.041810    Objective Loss 0.041810                                        LR 0.000250    Time 0.085542    
Epoch: [59][  190/  207]    Overall Loss 0.041672    Objective Loss 0.041672                                        LR 0.000250    Time 0.085078    
Epoch: [59][  200/  207]    Overall Loss 0.041642    Objective Loss 0.041642                                        LR 0.000250    Time 0.084768    
Epoch: [59][  207/  207]    Overall Loss 0.041900    Objective Loss 0.041900    Top1 96.375991    Top5 100.000000    LR 0.000250    Time 0.084225    
--- validate (epoch=59)-----------
5136 samples (512 per mini-batch)
Epoch: [59][   10/   11]    Loss 0.559787    Top1 83.632812    Top5 99.707031    
Epoch: [59][   11/   11]    Loss 0.561357    Top1 83.644860    Top5 99.707944    
==> Top1: 83.645    Top5: 99.708    Loss: 0.561

==> Confusion:
[[263   3   2   2   2   3   3  22]
 [  3 263  19   2   0   1   0  12]
 [  1  15 261   1   0   6   0  16]
 [  0   2   0 750  44  12  10  19]
 [  2   0   0  42 780   8  23  24]
 [  5   3  11  15  19 781  19  41]
 [  1   0   0   3  29  14 766  24]
 [ 21  14  13  62  70 143  34 432]]

==> Best [Top1: 83.645   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 59]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [60][   10/  207]    Overall Loss 0.037837    Objective Loss 0.037837                                        LR 0.000250    Time 0.134017    
Epoch: [60][   20/  207]    Overall Loss 0.037178    Objective Loss 0.037178                                        LR 0.000250    Time 0.104745    
Epoch: [60][   30/  207]    Overall Loss 0.037220    Objective Loss 0.037220                                        LR 0.000250    Time 0.099384    
Epoch: [60][   40/  207]    Overall Loss 0.038112    Objective Loss 0.038112                                        LR 0.000250    Time 0.093888    
Epoch: [60][   50/  207]    Overall Loss 0.039685    Objective Loss 0.039685                                        LR 0.000250    Time 0.093494    
Epoch: [60][   60/  207]    Overall Loss 0.039553    Objective Loss 0.039553                                        LR 0.000250    Time 0.093486    
Epoch: [60][   70/  207]    Overall Loss 0.039194    Objective Loss 0.039194                                        LR 0.000250    Time 0.093686    
Epoch: [60][   80/  207]    Overall Loss 0.039671    Objective Loss 0.039671                                        LR 0.000250    Time 0.093736    
Epoch: [60][   90/  207]    Overall Loss 0.039441    Objective Loss 0.039441                                        LR 0.000250    Time 0.093894    
Epoch: [60][  100/  207]    Overall Loss 0.039072    Objective Loss 0.039072                                        LR 0.000250    Time 0.093791    
Epoch: [60][  110/  207]    Overall Loss 0.039608    Objective Loss 0.039608                                        LR 0.000250    Time 0.092122    
Epoch: [60][  120/  207]    Overall Loss 0.039549    Objective Loss 0.039549                                        LR 0.000250    Time 0.091123    
Epoch: [60][  130/  207]    Overall Loss 0.039790    Objective Loss 0.039790                                        LR 0.000250    Time 0.089967    
Epoch: [60][  140/  207]    Overall Loss 0.040208    Objective Loss 0.040208                                        LR 0.000250    Time 0.089070    
Epoch: [60][  150/  207]    Overall Loss 0.040402    Objective Loss 0.040402                                        LR 0.000250    Time 0.088398    
Epoch: [60][  160/  207]    Overall Loss 0.040639    Objective Loss 0.040639                                        LR 0.000250    Time 0.087424    
Epoch: [60][  170/  207]    Overall Loss 0.040441    Objective Loss 0.040441                                        LR 0.000250    Time 0.086960    
Epoch: [60][  180/  207]    Overall Loss 0.040573    Objective Loss 0.040573                                        LR 0.000250    Time 0.086566    
Epoch: [60][  190/  207]    Overall Loss 0.040811    Objective Loss 0.040811                                        LR 0.000250    Time 0.086361    
Epoch: [60][  200/  207]    Overall Loss 0.041049    Objective Loss 0.041049                                        LR 0.000250    Time 0.086002    
Epoch: [60][  207/  207]    Overall Loss 0.041136    Objective Loss 0.041136    Top1 95.130238    Top5 100.000000    LR 0.000250    Time 0.085618    
--- validate (epoch=60)-----------
5136 samples (512 per mini-batch)
Epoch: [60][   10/   11]    Loss 0.514099    Top1 83.144531    Top5 99.687500    
Epoch: [60][   11/   11]    Loss 0.516860    Top1 83.119159    Top5 99.688474    
==> Top1: 83.119    Top5: 99.688    Loss: 0.517

==> Confusion:
[[278   2   2   2   1   1   3  11]
 [  5 262  23   2   0   1   1   6]
 [  2  18 266   1   0   6   0   7]
 [  1   2   0 735  65   6  14  14]
 [  2   0   0  38 789   6  26  18]
 [ 12   5  11  17  24 776  15  34]
 [  2   0   0   4  32  13 769  17]
 [ 33  20  15  59  91 142  35 394]]

==> Best [Top1: 83.645   Top5: 99.708   Sparsity:0.00   Params: 117200 on epoch: 59]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [61][   10/  207]    Overall Loss 0.038318    Objective Loss 0.038318                                        LR 0.000250    Time 0.141261    
Epoch: [61][   20/  207]    Overall Loss 0.036536    Objective Loss 0.036536                                        LR 0.000250    Time 0.106938    
Epoch: [61][   30/  207]    Overall Loss 0.036203    Objective Loss 0.036203                                        LR 0.000250    Time 0.095675    
Epoch: [61][   40/  207]    Overall Loss 0.037407    Objective Loss 0.037407                                        LR 0.000250    Time 0.090433    
Epoch: [61][   50/  207]    Overall Loss 0.037053    Objective Loss 0.037053                                        LR 0.000250    Time 0.087312    
Epoch: [61][   60/  207]    Overall Loss 0.037691    Objective Loss 0.037691                                        LR 0.000250    Time 0.085171    
Epoch: [61][   70/  207]    Overall Loss 0.038127    Objective Loss 0.038127                                        LR 0.000250    Time 0.083765    
Epoch: [61][   80/  207]    Overall Loss 0.038497    Objective Loss 0.038497                                        LR 0.000250    Time 0.083046    
Epoch: [61][   90/  207]    Overall Loss 0.038354    Objective Loss 0.038354                                        LR 0.000250    Time 0.082706    
Epoch: [61][  100/  207]    Overall Loss 0.038359    Objective Loss 0.038359                                        LR 0.000250    Time 0.082483    
Epoch: [61][  110/  207]    Overall Loss 0.038749    Objective Loss 0.038749                                        LR 0.000250    Time 0.082192    
Epoch: [61][  120/  207]    Overall Loss 0.038631    Objective Loss 0.038631                                        LR 0.000250    Time 0.081869    
Epoch: [61][  130/  207]    Overall Loss 0.038460    Objective Loss 0.038460                                        LR 0.000250    Time 0.081771    
Epoch: [61][  140/  207]    Overall Loss 0.038282    Objective Loss 0.038282                                        LR 0.000250    Time 0.081115    
Epoch: [61][  150/  207]    Overall Loss 0.038669    Objective Loss 0.038669                                        LR 0.000250    Time 0.080624    
Epoch: [61][  160/  207]    Overall Loss 0.038702    Objective Loss 0.038702                                        LR 0.000250    Time 0.080200    
Epoch: [61][  170/  207]    Overall Loss 0.039160    Objective Loss 0.039160                                        LR 0.000250    Time 0.079737    
Epoch: [61][  180/  207]    Overall Loss 0.039415    Objective Loss 0.039415                                        LR 0.000250    Time 0.079397    
Epoch: [61][  190/  207]    Overall Loss 0.039597    Objective Loss 0.039597                                        LR 0.000250    Time 0.079057    
Epoch: [61][  200/  207]    Overall Loss 0.039751    Objective Loss 0.039751                                        LR 0.000250    Time 0.078696    
Epoch: [61][  207/  207]    Overall Loss 0.039891    Objective Loss 0.039891    Top1 96.375991    Top5 100.000000    LR 0.000250    Time 0.078519    
--- validate (epoch=61)-----------
5136 samples (512 per mini-batch)
Epoch: [61][   10/   11]    Loss 0.551561    Top1 84.023438    Top5 99.746094    
Epoch: [61][   11/   11]    Loss 0.519540    Top1 84.034268    Top5 99.746885    
==> Top1: 84.034    Top5: 99.747    Loss: 0.520

==> Confusion:
[[266   4   2   1   2   2   3  20]
 [  3 259  28   1   0   1   0   8]
 [  2  16 272   1   0   3   0   6]
 [  0   5   0 754  41   4  11  22]
 [  1   0   0  47 781   5  18  27]
 [  9   4  13  15  25 750  14  64]
 [  2   0   0   7  33   8 754  33]
 [ 24  22  19  49  72  96  27 480]]

==> Best [Top1: 84.034   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 61]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [62][   10/  207]    Overall Loss 0.040005    Objective Loss 0.040005                                        LR 0.000250    Time 0.142455    
Epoch: [62][   20/  207]    Overall Loss 0.038839    Objective Loss 0.038839                                        LR 0.000250    Time 0.110596    
Epoch: [62][   30/  207]    Overall Loss 0.038168    Objective Loss 0.038168                                        LR 0.000250    Time 0.100257    
Epoch: [62][   40/  207]    Overall Loss 0.036909    Objective Loss 0.036909                                        LR 0.000250    Time 0.094755    
Epoch: [62][   50/  207]    Overall Loss 0.037665    Objective Loss 0.037665                                        LR 0.000250    Time 0.090436    
Epoch: [62][   60/  207]    Overall Loss 0.037708    Objective Loss 0.037708                                        LR 0.000250    Time 0.087539    
Epoch: [62][   70/  207]    Overall Loss 0.037459    Objective Loss 0.037459                                        LR 0.000250    Time 0.085381    
Epoch: [62][   80/  207]    Overall Loss 0.037945    Objective Loss 0.037945                                        LR 0.000250    Time 0.084373    
Epoch: [62][   90/  207]    Overall Loss 0.038150    Objective Loss 0.038150                                        LR 0.000250    Time 0.083149    
Epoch: [62][  100/  207]    Overall Loss 0.037767    Objective Loss 0.037767                                        LR 0.000250    Time 0.082185    
Epoch: [62][  110/  207]    Overall Loss 0.037924    Objective Loss 0.037924                                        LR 0.000250    Time 0.082078    
Epoch: [62][  120/  207]    Overall Loss 0.038183    Objective Loss 0.038183                                        LR 0.000250    Time 0.083258    
Epoch: [62][  130/  207]    Overall Loss 0.038122    Objective Loss 0.038122                                        LR 0.000250    Time 0.084636    
Epoch: [62][  140/  207]    Overall Loss 0.038640    Objective Loss 0.038640                                        LR 0.000250    Time 0.085536    
Epoch: [62][  150/  207]    Overall Loss 0.038837    Objective Loss 0.038837                                        LR 0.000250    Time 0.086143    
Epoch: [62][  160/  207]    Overall Loss 0.039120    Objective Loss 0.039120                                        LR 0.000250    Time 0.086710    
Epoch: [62][  170/  207]    Overall Loss 0.039242    Objective Loss 0.039242                                        LR 0.000250    Time 0.086304    
Epoch: [62][  180/  207]    Overall Loss 0.039479    Objective Loss 0.039479                                        LR 0.000250    Time 0.085816    
Epoch: [62][  190/  207]    Overall Loss 0.039713    Objective Loss 0.039713                                        LR 0.000250    Time 0.085311    
Epoch: [62][  200/  207]    Overall Loss 0.039690    Objective Loss 0.039690                                        LR 0.000250    Time 0.084837    
Epoch: [62][  207/  207]    Overall Loss 0.039712    Objective Loss 0.039712    Top1 97.508494    Top5 100.000000    LR 0.000250    Time 0.084446    
--- validate (epoch=62)-----------
5136 samples (512 per mini-batch)
Epoch: [62][   10/   11]    Loss 0.554762    Top1 83.730469    Top5 99.785156    
Epoch: [62][   11/   11]    Loss 0.541756    Top1 83.742212    Top5 99.785826    
==> Top1: 83.742    Top5: 99.786    Loss: 0.542

==> Confusion:
[[266   3   2   1   2   4   3  19]
 [  6 257  26   1   0   2   0   8]
 [  2  16 264   1   0   6   0  11]
 [  0   4   0 744  46  11  12  20]
 [  1   0   0  44 778   6  23  27]
 [  7   3   9  13  21 779  17  45]
 [  2   0   0   5  31  11 759  29]
 [ 23  16  16  46  72 134  28 454]]

==> Best [Top1: 84.034   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 61]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [63][   10/  207]    Overall Loss 0.039550    Objective Loss 0.039550                                        LR 0.000250    Time 0.152361    
Epoch: [63][   20/  207]    Overall Loss 0.039133    Objective Loss 0.039133                                        LR 0.000250    Time 0.123610    
Epoch: [63][   30/  207]    Overall Loss 0.039023    Objective Loss 0.039023                                        LR 0.000250    Time 0.114532    
Epoch: [63][   40/  207]    Overall Loss 0.037350    Objective Loss 0.037350                                        LR 0.000250    Time 0.110118    
Epoch: [63][   50/  207]    Overall Loss 0.037227    Objective Loss 0.037227                                        LR 0.000250    Time 0.107180    
Epoch: [63][   60/  207]    Overall Loss 0.037478    Objective Loss 0.037478                                        LR 0.000250    Time 0.103692    
Epoch: [63][   70/  207]    Overall Loss 0.037861    Objective Loss 0.037861                                        LR 0.000250    Time 0.099740    
Epoch: [63][   80/  207]    Overall Loss 0.038648    Objective Loss 0.038648                                        LR 0.000250    Time 0.096875    
Epoch: [63][   90/  207]    Overall Loss 0.038860    Objective Loss 0.038860                                        LR 0.000250    Time 0.096004    
Epoch: [63][  100/  207]    Overall Loss 0.038672    Objective Loss 0.038672                                        LR 0.000250    Time 0.094712    
Epoch: [63][  110/  207]    Overall Loss 0.038439    Objective Loss 0.038439                                        LR 0.000250    Time 0.093853    
Epoch: [63][  120/  207]    Overall Loss 0.038219    Objective Loss 0.038219                                        LR 0.000250    Time 0.092294    
Epoch: [63][  130/  207]    Overall Loss 0.037873    Objective Loss 0.037873                                        LR 0.000250    Time 0.090868    
Epoch: [63][  140/  207]    Overall Loss 0.037686    Objective Loss 0.037686                                        LR 0.000250    Time 0.089707    
Epoch: [63][  150/  207]    Overall Loss 0.037609    Objective Loss 0.037609                                        LR 0.000250    Time 0.088859    
Epoch: [63][  160/  207]    Overall Loss 0.037846    Objective Loss 0.037846                                        LR 0.000250    Time 0.087928    
Epoch: [63][  170/  207]    Overall Loss 0.038113    Objective Loss 0.038113                                        LR 0.000250    Time 0.087190    
Epoch: [63][  180/  207]    Overall Loss 0.038047    Objective Loss 0.038047                                        LR 0.000250    Time 0.086585    
Epoch: [63][  190/  207]    Overall Loss 0.038249    Objective Loss 0.038249                                        LR 0.000250    Time 0.085931    
Epoch: [63][  200/  207]    Overall Loss 0.038379    Objective Loss 0.038379                                        LR 0.000250    Time 0.085348    
Epoch: [63][  207/  207]    Overall Loss 0.038290    Objective Loss 0.038290    Top1 98.074745    Top5 99.886750    LR 0.000250    Time 0.084812    
--- validate (epoch=63)-----------
5136 samples (512 per mini-batch)
Epoch: [63][   10/   11]    Loss 0.553106    Top1 84.277344    Top5 99.746094    
Epoch: [63][   11/   11]    Loss 0.513728    Top1 84.248442    Top5 99.746885    
==> Top1: 84.248    Top5: 99.747    Loss: 0.514

==> Confusion:
[[273   2   2   1   1   1   3  17]
 [  4 265  22   1   0   1   0   7]
 [  1  19 264   1   0   4   0  11]
 [  0   3   0 744  44  11  10  25]
 [  2   0   0  54 767   8  22  26]
 [  9   6  10  14  15 783  11  46]
 [  2   0   0   5  22  14 762  32]
 [ 25  21  17  49  56 128  24 469]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [64][   10/  207]    Overall Loss 0.033043    Objective Loss 0.033043                                        LR 0.000250    Time 0.136449    
Epoch: [64][   20/  207]    Overall Loss 0.034708    Objective Loss 0.034708                                        LR 0.000250    Time 0.104862    
Epoch: [64][   30/  207]    Overall Loss 0.034442    Objective Loss 0.034442                                        LR 0.000250    Time 0.094017    
Epoch: [64][   40/  207]    Overall Loss 0.035792    Objective Loss 0.035792                                        LR 0.000250    Time 0.089240    
Epoch: [64][   50/  207]    Overall Loss 0.037012    Objective Loss 0.037012                                        LR 0.000250    Time 0.086289    
Epoch: [64][   60/  207]    Overall Loss 0.037593    Objective Loss 0.037593                                        LR 0.000250    Time 0.083939    
Epoch: [64][   70/  207]    Overall Loss 0.038588    Objective Loss 0.038588                                        LR 0.000250    Time 0.082432    
Epoch: [64][   80/  207]    Overall Loss 0.039169    Objective Loss 0.039169                                        LR 0.000250    Time 0.081463    
Epoch: [64][   90/  207]    Overall Loss 0.038718    Objective Loss 0.038718                                        LR 0.000250    Time 0.080611    
Epoch: [64][  100/  207]    Overall Loss 0.039097    Objective Loss 0.039097                                        LR 0.000250    Time 0.080066    
Epoch: [64][  110/  207]    Overall Loss 0.039205    Objective Loss 0.039205                                        LR 0.000250    Time 0.079623    
Epoch: [64][  120/  207]    Overall Loss 0.039161    Objective Loss 0.039161                                        LR 0.000250    Time 0.079245    
Epoch: [64][  130/  207]    Overall Loss 0.039115    Objective Loss 0.039115                                        LR 0.000250    Time 0.079053    
Epoch: [64][  140/  207]    Overall Loss 0.038944    Objective Loss 0.038944                                        LR 0.000250    Time 0.078858    
Epoch: [64][  150/  207]    Overall Loss 0.038920    Objective Loss 0.038920                                        LR 0.000250    Time 0.078775    
Epoch: [64][  160/  207]    Overall Loss 0.038918    Objective Loss 0.038918                                        LR 0.000250    Time 0.078557    
Epoch: [64][  170/  207]    Overall Loss 0.039230    Objective Loss 0.039230                                        LR 0.000250    Time 0.078442    
Epoch: [64][  180/  207]    Overall Loss 0.039326    Objective Loss 0.039326                                        LR 0.000250    Time 0.078251    
Epoch: [64][  190/  207]    Overall Loss 0.039706    Objective Loss 0.039706                                        LR 0.000250    Time 0.078089    
Epoch: [64][  200/  207]    Overall Loss 0.039668    Objective Loss 0.039668                                        LR 0.000250    Time 0.078000    
Epoch: [64][  207/  207]    Overall Loss 0.039641    Objective Loss 0.039641    Top1 97.055493    Top5 100.000000    LR 0.000250    Time 0.077718    
--- validate (epoch=64)-----------
5136 samples (512 per mini-batch)
Epoch: [64][   10/   11]    Loss 0.510738    Top1 84.023438    Top5 99.707031    
Epoch: [64][   11/   11]    Loss 0.666304    Top1 83.975857    Top5 99.707944    
==> Top1: 83.976    Top5: 99.708    Loss: 0.666

==> Confusion:
[[275   3   2   1   0   2   3  14]
 [  7 264  23   1   0   1   0   4]
 [  3  22 266   1   0   4   0   4]
 [  0   4   0 763  38  12   4  16]
 [  2   0   0  53 773   8  21  22]
 [ 11   5  11   9  16 798  13  31]
 [  2   0   0   8  29  16 758  24]
 [ 29  24  18  61  68 147  26 416]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [65][   10/  207]    Overall Loss 0.032061    Objective Loss 0.032061                                        LR 0.000125    Time 0.139634    
Epoch: [65][   20/  207]    Overall Loss 0.034156    Objective Loss 0.034156                                        LR 0.000125    Time 0.106781    
Epoch: [65][   30/  207]    Overall Loss 0.035159    Objective Loss 0.035159                                        LR 0.000125    Time 0.095226    
Epoch: [65][   40/  207]    Overall Loss 0.034920    Objective Loss 0.034920                                        LR 0.000125    Time 0.090288    
Epoch: [65][   50/  207]    Overall Loss 0.034297    Objective Loss 0.034297                                        LR 0.000125    Time 0.087130    
Epoch: [65][   60/  207]    Overall Loss 0.033999    Objective Loss 0.033999                                        LR 0.000125    Time 0.084899    
Epoch: [65][   70/  207]    Overall Loss 0.034119    Objective Loss 0.034119                                        LR 0.000125    Time 0.083503    
Epoch: [65][   80/  207]    Overall Loss 0.033822    Objective Loss 0.033822                                        LR 0.000125    Time 0.082764    
Epoch: [65][   90/  207]    Overall Loss 0.033435    Objective Loss 0.033435                                        LR 0.000125    Time 0.082128    
Epoch: [65][  100/  207]    Overall Loss 0.033824    Objective Loss 0.033824                                        LR 0.000125    Time 0.081437    
Epoch: [65][  110/  207]    Overall Loss 0.033762    Objective Loss 0.033762                                        LR 0.000125    Time 0.081027    
Epoch: [65][  120/  207]    Overall Loss 0.034017    Objective Loss 0.034017                                        LR 0.000125    Time 0.080621    
Epoch: [65][  130/  207]    Overall Loss 0.033779    Objective Loss 0.033779                                        LR 0.000125    Time 0.080326    
Epoch: [65][  140/  207]    Overall Loss 0.034022    Objective Loss 0.034022                                        LR 0.000125    Time 0.079838    
Epoch: [65][  150/  207]    Overall Loss 0.033806    Objective Loss 0.033806                                        LR 0.000125    Time 0.079411    
Epoch: [65][  160/  207]    Overall Loss 0.034115    Objective Loss 0.034115                                        LR 0.000125    Time 0.079067    
Epoch: [65][  170/  207]    Overall Loss 0.034281    Objective Loss 0.034281                                        LR 0.000125    Time 0.078763    
Epoch: [65][  180/  207]    Overall Loss 0.034403    Objective Loss 0.034403                                        LR 0.000125    Time 0.078638    
Epoch: [65][  190/  207]    Overall Loss 0.034569    Objective Loss 0.034569                                        LR 0.000125    Time 0.078366    
Epoch: [65][  200/  207]    Overall Loss 0.034687    Objective Loss 0.034687                                        LR 0.000125    Time 0.078160    
Epoch: [65][  207/  207]    Overall Loss 0.034819    Objective Loss 0.034819    Top1 96.942242    Top5 100.000000    LR 0.000125    Time 0.077950    
--- validate (epoch=65)-----------
5136 samples (512 per mini-batch)
Epoch: [65][   10/   11]    Loss 0.560431    Top1 83.984375    Top5 99.687500    
Epoch: [65][   11/   11]    Loss 0.526576    Top1 83.995327    Top5 99.688474    
==> Top1: 83.995    Top5: 99.688    Loss: 0.527

==> Confusion:
[[268   2   2   1   2   2   3  20]
 [  3 263  21   1   0   1   0  11]
 [  1  15 263   1   0   5   0  15]
 [  0   3   0 737  55  13   9  20]
 [  2   0   0  40 782   6  23  26]
 [  7   4   8  10  20 795  10  40]
 [  2   0   0   4  33  16 756  26]
 [ 19  13  16  55  71 139  26 450]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [66][   10/  207]    Overall Loss 0.033182    Objective Loss 0.033182                                        LR 0.000125    Time 0.118147    
Epoch: [66][   20/  207]    Overall Loss 0.031884    Objective Loss 0.031884                                        LR 0.000125    Time 0.096917    
Epoch: [66][   30/  207]    Overall Loss 0.031920    Objective Loss 0.031920                                        LR 0.000125    Time 0.089358    
Epoch: [66][   40/  207]    Overall Loss 0.032920    Objective Loss 0.032920                                        LR 0.000125    Time 0.085591    
Epoch: [66][   50/  207]    Overall Loss 0.033271    Objective Loss 0.033271                                        LR 0.000125    Time 0.083159    
Epoch: [66][   60/  207]    Overall Loss 0.034054    Objective Loss 0.034054                                        LR 0.000125    Time 0.081599    
Epoch: [66][   70/  207]    Overall Loss 0.033781    Objective Loss 0.033781                                        LR 0.000125    Time 0.080450    
Epoch: [66][   80/  207]    Overall Loss 0.033134    Objective Loss 0.033134                                        LR 0.000125    Time 0.079475    
Epoch: [66][   90/  207]    Overall Loss 0.033295    Objective Loss 0.033295                                        LR 0.000125    Time 0.079169    
Epoch: [66][  100/  207]    Overall Loss 0.033130    Objective Loss 0.033130                                        LR 0.000125    Time 0.078552    
Epoch: [66][  110/  207]    Overall Loss 0.033596    Objective Loss 0.033596                                        LR 0.000125    Time 0.078085    
Epoch: [66][  120/  207]    Overall Loss 0.034021    Objective Loss 0.034021                                        LR 0.000125    Time 0.078011    
Epoch: [66][  130/  207]    Overall Loss 0.033984    Objective Loss 0.033984                                        LR 0.000125    Time 0.077671    
Epoch: [66][  140/  207]    Overall Loss 0.034088    Objective Loss 0.034088                                        LR 0.000125    Time 0.077402    
Epoch: [66][  150/  207]    Overall Loss 0.033866    Objective Loss 0.033866                                        LR 0.000125    Time 0.077246    
Epoch: [66][  160/  207]    Overall Loss 0.033885    Objective Loss 0.033885                                        LR 0.000125    Time 0.076981    
Epoch: [66][  170/  207]    Overall Loss 0.033899    Objective Loss 0.033899                                        LR 0.000125    Time 0.076812    
Epoch: [66][  180/  207]    Overall Loss 0.033996    Objective Loss 0.033996                                        LR 0.000125    Time 0.076612    
Epoch: [66][  190/  207]    Overall Loss 0.033904    Objective Loss 0.033904                                        LR 0.000125    Time 0.076505    
Epoch: [66][  200/  207]    Overall Loss 0.033889    Objective Loss 0.033889                                        LR 0.000125    Time 0.076362    
Epoch: [66][  207/  207]    Overall Loss 0.033803    Objective Loss 0.033803    Top1 98.074745    Top5 100.000000    LR 0.000125    Time 0.076158    
--- validate (epoch=66)-----------
5136 samples (512 per mini-batch)
Epoch: [66][   10/   11]    Loss 0.542507    Top1 84.062500    Top5 99.726562    
Epoch: [66][   11/   11]    Loss 0.544587    Top1 84.053738    Top5 99.727414    
==> Top1: 84.054    Top5: 99.727    Loss: 0.545

==> Confusion:
[[273   3   2   1   1   2   3  15]
 [  4 261  25   1   0   1   0   8]
 [  1  16 272   1   0   4   0   6]
 [  0   4   0 728  57  14  11  23]
 [  2   0   0  37 783   8  27  22]
 [ 10   3  11  10  20 791  12  37]
 [  2   0   0   4  27  13 767  24]
 [ 22  18  17  43  73 143  31 442]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [67][   10/  207]    Overall Loss 0.032493    Objective Loss 0.032493                                        LR 0.000125    Time 0.133479    
Epoch: [67][   20/  207]    Overall Loss 0.031444    Objective Loss 0.031444                                        LR 0.000125    Time 0.103616    
Epoch: [67][   30/  207]    Overall Loss 0.032069    Objective Loss 0.032069                                        LR 0.000125    Time 0.094353    
Epoch: [67][   40/  207]    Overall Loss 0.031695    Objective Loss 0.031695                                        LR 0.000125    Time 0.089148    
Epoch: [67][   50/  207]    Overall Loss 0.032337    Objective Loss 0.032337                                        LR 0.000125    Time 0.086287    
Epoch: [67][   60/  207]    Overall Loss 0.031930    Objective Loss 0.031930                                        LR 0.000125    Time 0.084104    
Epoch: [67][   70/  207]    Overall Loss 0.031916    Objective Loss 0.031916                                        LR 0.000125    Time 0.082824    
Epoch: [67][   80/  207]    Overall Loss 0.032528    Objective Loss 0.032528                                        LR 0.000125    Time 0.081549    
Epoch: [67][   90/  207]    Overall Loss 0.032974    Objective Loss 0.032974                                        LR 0.000125    Time 0.080693    
Epoch: [67][  100/  207]    Overall Loss 0.033473    Objective Loss 0.033473                                        LR 0.000125    Time 0.079937    
Epoch: [67][  110/  207]    Overall Loss 0.033522    Objective Loss 0.033522                                        LR 0.000125    Time 0.079394    
Epoch: [67][  120/  207]    Overall Loss 0.033433    Objective Loss 0.033433                                        LR 0.000125    Time 0.079149    
Epoch: [67][  130/  207]    Overall Loss 0.033259    Objective Loss 0.033259                                        LR 0.000125    Time 0.079025    
Epoch: [67][  140/  207]    Overall Loss 0.033087    Objective Loss 0.033087                                        LR 0.000125    Time 0.078733    
Epoch: [67][  150/  207]    Overall Loss 0.033135    Objective Loss 0.033135                                        LR 0.000125    Time 0.078465    
Epoch: [67][  160/  207]    Overall Loss 0.033434    Objective Loss 0.033434                                        LR 0.000125    Time 0.078150    
Epoch: [67][  170/  207]    Overall Loss 0.033388    Objective Loss 0.033388                                        LR 0.000125    Time 0.077812    
Epoch: [67][  180/  207]    Overall Loss 0.033309    Objective Loss 0.033309                                        LR 0.000125    Time 0.077626    
Epoch: [67][  190/  207]    Overall Loss 0.033291    Objective Loss 0.033291                                        LR 0.000125    Time 0.077435    
Epoch: [67][  200/  207]    Overall Loss 0.033410    Objective Loss 0.033410                                        LR 0.000125    Time 0.077329    
Epoch: [67][  207/  207]    Overall Loss 0.033354    Objective Loss 0.033354    Top1 97.508494    Top5 99.886750    LR 0.000125    Time 0.077045    
--- validate (epoch=67)-----------
5136 samples (512 per mini-batch)
Epoch: [67][   10/   11]    Loss 0.562545    Top1 84.121094    Top5 99.667969    
Epoch: [67][   11/   11]    Loss 0.525543    Top1 84.112150    Top5 99.669003    
==> Top1: 84.112    Top5: 99.669    Loss: 0.526

==> Confusion:
[[273   3   2   1   0   1   3  17]
 [  2 264  22   2   0   1   0   9]
 [  1  20 262   1   0   6   0  10]
 [  0   4   0 737  55   8  12  21]
 [  2   0   0  38 784   7  24  24]
 [  9   4  10  12  23 784  13  39]
 [  2   0   0   4  31  14 758  28]
 [ 22  20  15  48  70 131  25 458]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [68][   10/  207]    Overall Loss 0.033229    Objective Loss 0.033229                                        LR 0.000125    Time 0.137470    
Epoch: [68][   20/  207]    Overall Loss 0.032492    Objective Loss 0.032492                                        LR 0.000125    Time 0.105878    
Epoch: [68][   30/  207]    Overall Loss 0.032617    Objective Loss 0.032617                                        LR 0.000125    Time 0.094964    
Epoch: [68][   40/  207]    Overall Loss 0.032379    Objective Loss 0.032379                                        LR 0.000125    Time 0.089936    
Epoch: [68][   50/  207]    Overall Loss 0.031776    Objective Loss 0.031776                                        LR 0.000125    Time 0.087208    
Epoch: [68][   60/  207]    Overall Loss 0.032472    Objective Loss 0.032472                                        LR 0.000125    Time 0.086547    
Epoch: [68][   70/  207]    Overall Loss 0.032330    Objective Loss 0.032330                                        LR 0.000125    Time 0.085625    
Epoch: [68][   80/  207]    Overall Loss 0.032653    Objective Loss 0.032653                                        LR 0.000125    Time 0.084945    
Epoch: [68][   90/  207]    Overall Loss 0.032597    Objective Loss 0.032597                                        LR 0.000125    Time 0.084561    
Epoch: [68][  100/  207]    Overall Loss 0.032441    Objective Loss 0.032441                                        LR 0.000125    Time 0.083947    
Epoch: [68][  110/  207]    Overall Loss 0.032331    Objective Loss 0.032331                                        LR 0.000125    Time 0.083476    
Epoch: [68][  120/  207]    Overall Loss 0.032379    Objective Loss 0.032379                                        LR 0.000125    Time 0.082638    
Epoch: [68][  130/  207]    Overall Loss 0.032203    Objective Loss 0.032203                                        LR 0.000125    Time 0.082029    
Epoch: [68][  140/  207]    Overall Loss 0.032134    Objective Loss 0.032134                                        LR 0.000125    Time 0.081476    
Epoch: [68][  150/  207]    Overall Loss 0.032150    Objective Loss 0.032150                                        LR 0.000125    Time 0.081212    
Epoch: [68][  160/  207]    Overall Loss 0.032367    Objective Loss 0.032367                                        LR 0.000125    Time 0.080784    
Epoch: [68][  170/  207]    Overall Loss 0.032443    Objective Loss 0.032443                                        LR 0.000125    Time 0.080513    
Epoch: [68][  180/  207]    Overall Loss 0.032850    Objective Loss 0.032850                                        LR 0.000125    Time 0.080374    
Epoch: [68][  190/  207]    Overall Loss 0.032970    Objective Loss 0.032970                                        LR 0.000125    Time 0.080117    
Epoch: [68][  200/  207]    Overall Loss 0.033087    Objective Loss 0.033087                                        LR 0.000125    Time 0.079892    
Epoch: [68][  207/  207]    Overall Loss 0.033200    Objective Loss 0.033200    Top1 96.828992    Top5 99.773499    LR 0.000125    Time 0.079598    
--- validate (epoch=68)-----------
5136 samples (512 per mini-batch)
Epoch: [68][   10/   11]    Loss 0.548448    Top1 84.179688    Top5 99.746094    
Epoch: [68][   11/   11]    Loss 0.500815    Top1 84.209502    Top5 99.746885    
==> Top1: 84.210    Top5: 99.747    Loss: 0.501

==> Confusion:
[[272   3   2   1   0   2   3  17]
 [  3 264  23   1   0   1   0   8]
 [  1  18 267   1   0   6   0   7]
 [  0   4   0 734  52  15  13  19]
 [  2   0   0  40 778   7  27  25]
 [  7   4  10  10  19 790  12  42]
 [  2   0   0   4  27  13 768  23]
 [ 25  18  18  46  63 135  32 452]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [69][   10/  207]    Overall Loss 0.032051    Objective Loss 0.032051                                        LR 0.000125    Time 0.136655    
Epoch: [69][   20/  207]    Overall Loss 0.033920    Objective Loss 0.033920                                        LR 0.000125    Time 0.105827    
Epoch: [69][   30/  207]    Overall Loss 0.032593    Objective Loss 0.032593                                        LR 0.000125    Time 0.095628    
Epoch: [69][   40/  207]    Overall Loss 0.031550    Objective Loss 0.031550                                        LR 0.000125    Time 0.090246    
Epoch: [69][   50/  207]    Overall Loss 0.031227    Objective Loss 0.031227                                        LR 0.000125    Time 0.087113    
Epoch: [69][   60/  207]    Overall Loss 0.031768    Objective Loss 0.031768                                        LR 0.000125    Time 0.084892    
Epoch: [69][   70/  207]    Overall Loss 0.032479    Objective Loss 0.032479                                        LR 0.000125    Time 0.083366    
Epoch: [69][   80/  207]    Overall Loss 0.032141    Objective Loss 0.032141                                        LR 0.000125    Time 0.082380    
Epoch: [69][   90/  207]    Overall Loss 0.032362    Objective Loss 0.032362                                        LR 0.000125    Time 0.082004    
Epoch: [69][  100/  207]    Overall Loss 0.032324    Objective Loss 0.032324                                        LR 0.000125    Time 0.081956    
Epoch: [69][  110/  207]    Overall Loss 0.032509    Objective Loss 0.032509                                        LR 0.000125    Time 0.081730    
Epoch: [69][  120/  207]    Overall Loss 0.032589    Objective Loss 0.032589                                        LR 0.000125    Time 0.081389    
Epoch: [69][  130/  207]    Overall Loss 0.032458    Objective Loss 0.032458                                        LR 0.000125    Time 0.081196    
Epoch: [69][  140/  207]    Overall Loss 0.032362    Objective Loss 0.032362                                        LR 0.000125    Time 0.081013    
Epoch: [69][  150/  207]    Overall Loss 0.032567    Objective Loss 0.032567                                        LR 0.000125    Time 0.080855    
Epoch: [69][  160/  207]    Overall Loss 0.032498    Objective Loss 0.032498                                        LR 0.000125    Time 0.080468    
Epoch: [69][  170/  207]    Overall Loss 0.032487    Objective Loss 0.032487                                        LR 0.000125    Time 0.080072    
Epoch: [69][  180/  207]    Overall Loss 0.032608    Objective Loss 0.032608                                        LR 0.000125    Time 0.079945    
Epoch: [69][  190/  207]    Overall Loss 0.032398    Objective Loss 0.032398                                        LR 0.000125    Time 0.079654    
Epoch: [69][  200/  207]    Overall Loss 0.032353    Objective Loss 0.032353                                        LR 0.000125    Time 0.079315    
Epoch: [69][  207/  207]    Overall Loss 0.032540    Objective Loss 0.032540    Top1 97.055493    Top5 100.000000    LR 0.000125    Time 0.079000    
--- validate (epoch=69)-----------
5136 samples (512 per mini-batch)
Epoch: [69][   10/   11]    Loss 0.565616    Top1 84.179688    Top5 99.726562    
Epoch: [69][   11/   11]    Loss 0.527944    Top1 84.190031    Top5 99.727414    
==> Top1: 84.190    Top5: 99.727    Loss: 0.528

==> Confusion:
[[271   2   2   1   0   2   3  19]
 [  2 264  23   1   0   1   0   9]
 [  1  20 263   1   0   5   0  10]
 [  0   4   0 748  41  12   9  23]
 [  2   0   0  44 774   6  25  28]
 [  8   6  10  11  17 782  12  48]
 [  2   0   0   5  29  13 758  30]
 [ 22  19  15  51  65 125  28 464]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [70][   10/  207]    Overall Loss 0.034936    Objective Loss 0.034936                                        LR 0.000125    Time 0.125002    
Epoch: [70][   20/  207]    Overall Loss 0.032213    Objective Loss 0.032213                                        LR 0.000125    Time 0.102266    
Epoch: [70][   30/  207]    Overall Loss 0.033045    Objective Loss 0.033045                                        LR 0.000125    Time 0.094236    
Epoch: [70][   40/  207]    Overall Loss 0.032977    Objective Loss 0.032977                                        LR 0.000125    Time 0.090497    
Epoch: [70][   50/  207]    Overall Loss 0.032027    Objective Loss 0.032027                                        LR 0.000125    Time 0.088516    
Epoch: [70][   60/  207]    Overall Loss 0.031717    Objective Loss 0.031717                                        LR 0.000125    Time 0.087096    
Epoch: [70][   70/  207]    Overall Loss 0.031892    Objective Loss 0.031892                                        LR 0.000125    Time 0.085168    
Epoch: [70][   80/  207]    Overall Loss 0.031184    Objective Loss 0.031184                                        LR 0.000125    Time 0.084174    
Epoch: [70][   90/  207]    Overall Loss 0.030820    Objective Loss 0.030820                                        LR 0.000125    Time 0.083204    
Epoch: [70][  100/  207]    Overall Loss 0.030876    Objective Loss 0.030876                                        LR 0.000125    Time 0.082391    
Epoch: [70][  110/  207]    Overall Loss 0.031176    Objective Loss 0.031176                                        LR 0.000125    Time 0.081998    
Epoch: [70][  120/  207]    Overall Loss 0.031286    Objective Loss 0.031286                                        LR 0.000125    Time 0.081436    
Epoch: [70][  130/  207]    Overall Loss 0.031122    Objective Loss 0.031122                                        LR 0.000125    Time 0.081144    
Epoch: [70][  140/  207]    Overall Loss 0.031080    Objective Loss 0.031080                                        LR 0.000125    Time 0.081266    
Epoch: [70][  150/  207]    Overall Loss 0.031427    Objective Loss 0.031427                                        LR 0.000125    Time 0.081368    
Epoch: [70][  160/  207]    Overall Loss 0.031599    Objective Loss 0.031599                                        LR 0.000125    Time 0.081234    
Epoch: [70][  170/  207]    Overall Loss 0.031677    Objective Loss 0.031677                                        LR 0.000125    Time 0.081114    
Epoch: [70][  180/  207]    Overall Loss 0.031750    Objective Loss 0.031750                                        LR 0.000125    Time 0.081094    
Epoch: [70][  190/  207]    Overall Loss 0.031879    Objective Loss 0.031879                                        LR 0.000125    Time 0.080975    
Epoch: [70][  200/  207]    Overall Loss 0.032073    Objective Loss 0.032073                                        LR 0.000125    Time 0.080750    
Epoch: [70][  207/  207]    Overall Loss 0.032123    Objective Loss 0.032123    Top1 97.848245    Top5 99.886750    LR 0.000125    Time 0.080397    
--- validate (epoch=70)-----------
5136 samples (512 per mini-batch)
Epoch: [70][   10/   11]    Loss 0.567666    Top1 83.964844    Top5 99.746094    
Epoch: [70][   11/   11]    Loss 0.550662    Top1 83.975857    Top5 99.746885    
==> Top1: 83.976    Top5: 99.747    Loss: 0.551

==> Confusion:
[[278   2   1   1   0   1   3  14]
 [  6 261  22   1   0   1   0   9]
 [  3  20 261   1   0   5   0  10]
 [  0   4   0 735  59   6   9  24]
 [  2   0   0  33 789   8  19  28]
 [ 10   5   8  16  25 764   9  57]
 [  2   0   0   6  30  12 749  38]
 [ 25  18  12  49  77 112  20 476]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [71][   10/  207]    Overall Loss 0.028837    Objective Loss 0.028837                                        LR 0.000125    Time 0.137479    
Epoch: [71][   20/  207]    Overall Loss 0.030335    Objective Loss 0.030335                                        LR 0.000125    Time 0.105400    
Epoch: [71][   30/  207]    Overall Loss 0.029876    Objective Loss 0.029876                                        LR 0.000125    Time 0.094893    
Epoch: [71][   40/  207]    Overall Loss 0.030384    Objective Loss 0.030384                                        LR 0.000125    Time 0.089730    
Epoch: [71][   50/  207]    Overall Loss 0.030762    Objective Loss 0.030762                                        LR 0.000125    Time 0.086539    
Epoch: [71][   60/  207]    Overall Loss 0.031216    Objective Loss 0.031216                                        LR 0.000125    Time 0.084440    
Epoch: [71][   70/  207]    Overall Loss 0.031461    Objective Loss 0.031461                                        LR 0.000125    Time 0.082933    
Epoch: [71][   80/  207]    Overall Loss 0.031397    Objective Loss 0.031397                                        LR 0.000125    Time 0.081894    
Epoch: [71][   90/  207]    Overall Loss 0.031282    Objective Loss 0.031282                                        LR 0.000125    Time 0.081037    
Epoch: [71][  100/  207]    Overall Loss 0.031514    Objective Loss 0.031514                                        LR 0.000125    Time 0.080628    
Epoch: [71][  110/  207]    Overall Loss 0.031427    Objective Loss 0.031427                                        LR 0.000125    Time 0.079826    
Epoch: [71][  120/  207]    Overall Loss 0.031396    Objective Loss 0.031396                                        LR 0.000125    Time 0.079377    
Epoch: [71][  130/  207]    Overall Loss 0.031392    Objective Loss 0.031392                                        LR 0.000125    Time 0.078967    
Epoch: [71][  140/  207]    Overall Loss 0.031401    Objective Loss 0.031401                                        LR 0.000125    Time 0.078616    
Epoch: [71][  150/  207]    Overall Loss 0.031365    Objective Loss 0.031365                                        LR 0.000125    Time 0.078286    
Epoch: [71][  160/  207]    Overall Loss 0.031472    Objective Loss 0.031472                                        LR 0.000125    Time 0.078151    
Epoch: [71][  170/  207]    Overall Loss 0.031442    Objective Loss 0.031442                                        LR 0.000125    Time 0.077841    
Epoch: [71][  180/  207]    Overall Loss 0.031629    Objective Loss 0.031629                                        LR 0.000125    Time 0.078550    
Epoch: [71][  190/  207]    Overall Loss 0.031862    Objective Loss 0.031862                                        LR 0.000125    Time 0.078809    
Epoch: [71][  200/  207]    Overall Loss 0.031891    Objective Loss 0.031891                                        LR 0.000125    Time 0.078778    
Epoch: [71][  207/  207]    Overall Loss 0.032215    Objective Loss 0.032215    Top1 97.055493    Top5 100.000000    LR 0.000125    Time 0.078510    
--- validate (epoch=71)-----------
5136 samples (512 per mini-batch)
Epoch: [71][   10/   11]    Loss 0.585788    Top1 83.808594    Top5 99.687500    
Epoch: [71][   11/   11]    Loss 0.545571    Top1 83.839564    Top5 99.688474    
==> Top1: 83.840    Top5: 99.688    Loss: 0.546

==> Confusion:
[[270   2   2   1   2   2   3  18]
 [  2 255  28   1   0   1   0  13]
 [  1  17 262   1   0   6   0  13]
 [  0   4   0 732  63  10   7  21]
 [  1   0   0  34 787   7  24  26]
 [  7   3  11  14  22 772  18  47]
 [  2   0   0   4  30  10 764  27]
 [ 23  15  14  47  73 122  31 464]]

==> Best [Top1: 84.248   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 63]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [72][   10/  207]    Overall Loss 0.031186    Objective Loss 0.031186                                        LR 0.000125    Time 0.135938    
Epoch: [72][   20/  207]    Overall Loss 0.031847    Objective Loss 0.031847                                        LR 0.000125    Time 0.105059    
Epoch: [72][   30/  207]    Overall Loss 0.031059    Objective Loss 0.031059                                        LR 0.000125    Time 0.094601    
Epoch: [72][   40/  207]    Overall Loss 0.031209    Objective Loss 0.031209                                        LR 0.000125    Time 0.091089    
Epoch: [72][   50/  207]    Overall Loss 0.031246    Objective Loss 0.031246                                        LR 0.000125    Time 0.088746    
Epoch: [72][   60/  207]    Overall Loss 0.031511    Objective Loss 0.031511                                        LR 0.000125    Time 0.086616    
Epoch: [72][   70/  207]    Overall Loss 0.031628    Objective Loss 0.031628                                        LR 0.000125    Time 0.086227    
Epoch: [72][   80/  207]    Overall Loss 0.031381    Objective Loss 0.031381                                        LR 0.000125    Time 0.085125    
Epoch: [72][   90/  207]    Overall Loss 0.031646    Objective Loss 0.031646                                        LR 0.000125    Time 0.084370    
Epoch: [72][  100/  207]    Overall Loss 0.031360    Objective Loss 0.031360                                        LR 0.000125    Time 0.084001    
Epoch: [72][  110/  207]    Overall Loss 0.031619    Objective Loss 0.031619                                        LR 0.000125    Time 0.083398    
Epoch: [72][  120/  207]    Overall Loss 0.031758    Objective Loss 0.031758                                        LR 0.000125    Time 0.082779    
Epoch: [72][  130/  207]    Overall Loss 0.031669    Objective Loss 0.031669                                        LR 0.000125    Time 0.082300    
Epoch: [72][  140/  207]    Overall Loss 0.031567    Objective Loss 0.031567                                        LR 0.000125    Time 0.081949    
Epoch: [72][  150/  207]    Overall Loss 0.031373    Objective Loss 0.031373                                        LR 0.000125    Time 0.081479    
Epoch: [72][  160/  207]    Overall Loss 0.031567    Objective Loss 0.031567                                        LR 0.000125    Time 0.081132    
Epoch: [72][  170/  207]    Overall Loss 0.031432    Objective Loss 0.031432                                        LR 0.000125    Time 0.080707    
Epoch: [72][  180/  207]    Overall Loss 0.031789    Objective Loss 0.031789                                        LR 0.000125    Time 0.080363    
Epoch: [72][  190/  207]    Overall Loss 0.031759    Objective Loss 0.031759                                        LR 0.000125    Time 0.079979    
Epoch: [72][  200/  207]    Overall Loss 0.031867    Objective Loss 0.031867                                        LR 0.000125    Time 0.079691    
Epoch: [72][  207/  207]    Overall Loss 0.031861    Objective Loss 0.031861    Top1 96.828992    Top5 100.000000    LR 0.000125    Time 0.079358    
--- validate (epoch=72)-----------
5136 samples (512 per mini-batch)
Epoch: [72][   10/   11]    Loss 0.605774    Top1 84.257812    Top5 99.765625    
Epoch: [72][   11/   11]    Loss 0.628774    Top1 84.248442    Top5 99.766355    
==> Top1: 84.248    Top5: 99.766    Loss: 0.629

==> Confusion:
[[262   4   3   1   2   3   3  22]
 [  3 260  21   1   0   1   0  14]
 [  1  15 268   1   0   3   0  12]
 [  0   4   0 729  54  11  12  27]
 [  2   0   0  41 777   8  20  31]
 [  7   2  11  10  17 778   9  60]
 [  2   0   0   5  26  12 757  35]
 [ 19  16  15  39  72 110  22 496]]

==> Best [Top1: 84.248   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 72]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [73][   10/  207]    Overall Loss 0.028765    Objective Loss 0.028765                                        LR 0.000125    Time 0.133037    
Epoch: [73][   20/  207]    Overall Loss 0.030176    Objective Loss 0.030176                                        LR 0.000125    Time 0.103549    
Epoch: [73][   30/  207]    Overall Loss 0.030750    Objective Loss 0.030750                                        LR 0.000125    Time 0.093588    
Epoch: [73][   40/  207]    Overall Loss 0.029804    Objective Loss 0.029804                                        LR 0.000125    Time 0.088571    
Epoch: [73][   50/  207]    Overall Loss 0.029530    Objective Loss 0.029530                                        LR 0.000125    Time 0.085544    
Epoch: [73][   60/  207]    Overall Loss 0.029455    Objective Loss 0.029455                                        LR 0.000125    Time 0.083555    
Epoch: [73][   70/  207]    Overall Loss 0.030020    Objective Loss 0.030020                                        LR 0.000125    Time 0.082411    
Epoch: [73][   80/  207]    Overall Loss 0.030830    Objective Loss 0.030830                                        LR 0.000125    Time 0.081421    
Epoch: [73][   90/  207]    Overall Loss 0.031352    Objective Loss 0.031352                                        LR 0.000125    Time 0.080770    
Epoch: [73][  100/  207]    Overall Loss 0.031382    Objective Loss 0.031382                                        LR 0.000125    Time 0.080292    
Epoch: [73][  110/  207]    Overall Loss 0.031186    Objective Loss 0.031186                                        LR 0.000125    Time 0.079743    
Epoch: [73][  120/  207]    Overall Loss 0.031192    Objective Loss 0.031192                                        LR 0.000125    Time 0.079366    
Epoch: [73][  130/  207]    Overall Loss 0.030955    Objective Loss 0.030955                                        LR 0.000125    Time 0.079161    
Epoch: [73][  140/  207]    Overall Loss 0.030839    Objective Loss 0.030839                                        LR 0.000125    Time 0.079183    
Epoch: [73][  150/  207]    Overall Loss 0.030930    Objective Loss 0.030930                                        LR 0.000125    Time 0.079366    
Epoch: [73][  160/  207]    Overall Loss 0.031132    Objective Loss 0.031132                                        LR 0.000125    Time 0.079585    
Epoch: [73][  170/  207]    Overall Loss 0.031298    Objective Loss 0.031298                                        LR 0.000125    Time 0.079548    
Epoch: [73][  180/  207]    Overall Loss 0.031526    Objective Loss 0.031526                                        LR 0.000125    Time 0.079628    
Epoch: [73][  190/  207]    Overall Loss 0.031759    Objective Loss 0.031759                                        LR 0.000125    Time 0.079523    
Epoch: [73][  200/  207]    Overall Loss 0.031646    Objective Loss 0.031646                                        LR 0.000125    Time 0.079239    
Epoch: [73][  207/  207]    Overall Loss 0.031634    Objective Loss 0.031634    Top1 97.961495    Top5 100.000000    LR 0.000125    Time 0.078930    
--- validate (epoch=73)-----------
5136 samples (512 per mini-batch)
Epoch: [73][   10/   11]    Loss 0.600059    Top1 84.042969    Top5 99.687500    
Epoch: [73][   11/   11]    Loss 0.588706    Top1 84.034268    Top5 99.688474    
==> Top1: 84.034    Top5: 99.688    Loss: 0.589

==> Confusion:
[[268   2   3   1   1   1   3  21]
 [  2 257  25   2   0   1   0  13]
 [  1  14 265   1   0   5   0  14]
 [  0   4   0 735  58   7  13  20]
 [  2   0   0  37 780   6  29  25]
 [  6   1  10   9  21 777  11  59]
 [  2   0   0   6  28  12 762  27]
 [ 18  13  15  47  82 113  29 472]]

==> Best [Top1: 84.248   Top5: 99.766   Sparsity:0.00   Params: 117200 on epoch: 72]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [74][   10/  207]    Overall Loss 0.029567    Objective Loss 0.029567                                        LR 0.000125    Time 0.138082    
Epoch: [74][   20/  207]    Overall Loss 0.027415    Objective Loss 0.027415                                        LR 0.000125    Time 0.107203    
Epoch: [74][   30/  207]    Overall Loss 0.027810    Objective Loss 0.027810                                        LR 0.000125    Time 0.096932    
Epoch: [74][   40/  207]    Overall Loss 0.028462    Objective Loss 0.028462                                        LR 0.000125    Time 0.092992    
Epoch: [74][   50/  207]    Overall Loss 0.028264    Objective Loss 0.028264                                        LR 0.000125    Time 0.091029    
Epoch: [74][   60/  207]    Overall Loss 0.028606    Objective Loss 0.028606                                        LR 0.000125    Time 0.091654    
Epoch: [74][   70/  207]    Overall Loss 0.028714    Objective Loss 0.028714                                        LR 0.000125    Time 0.091829    
Epoch: [74][   80/  207]    Overall Loss 0.029311    Objective Loss 0.029311                                        LR 0.000125    Time 0.092074    
Epoch: [74][   90/  207]    Overall Loss 0.029293    Objective Loss 0.029293                                        LR 0.000125    Time 0.090695    
Epoch: [74][  100/  207]    Overall Loss 0.029446    Objective Loss 0.029446                                        LR 0.000125    Time 0.089239    
Epoch: [74][  110/  207]    Overall Loss 0.029605    Objective Loss 0.029605                                        LR 0.000125    Time 0.087869    
Epoch: [74][  120/  207]    Overall Loss 0.029782    Objective Loss 0.029782                                        LR 0.000125    Time 0.086844    
Epoch: [74][  130/  207]    Overall Loss 0.029824    Objective Loss 0.029824                                        LR 0.000125    Time 0.085936    
Epoch: [74][  140/  207]    Overall Loss 0.029947    Objective Loss 0.029947                                        LR 0.000125    Time 0.085215    
Epoch: [74][  150/  207]    Overall Loss 0.029805    Objective Loss 0.029805                                        LR 0.000125    Time 0.084409    
Epoch: [74][  160/  207]    Overall Loss 0.029870    Objective Loss 0.029870                                        LR 0.000125    Time 0.083917    
Epoch: [74][  170/  207]    Overall Loss 0.030098    Objective Loss 0.030098                                        LR 0.000125    Time 0.083465    
Epoch: [74][  180/  207]    Overall Loss 0.030194    Objective Loss 0.030194                                        LR 0.000125    Time 0.083020    
Epoch: [74][  190/  207]    Overall Loss 0.030349    Objective Loss 0.030349                                        LR 0.000125    Time 0.082524    
Epoch: [74][  200/  207]    Overall Loss 0.030596    Objective Loss 0.030596                                        LR 0.000125    Time 0.082091    
Epoch: [74][  207/  207]    Overall Loss 0.030861    Objective Loss 0.030861    Top1 97.734994    Top5 100.000000    LR 0.000125    Time 0.081689    
--- validate (epoch=74)-----------
5136 samples (512 per mini-batch)
Epoch: [74][   10/   11]    Loss 0.567790    Top1 84.609375    Top5 99.746094    
Epoch: [74][   11/   11]    Loss 0.517880    Top1 84.657321    Top5 99.746885    
==> Top1: 84.657    Top5: 99.747    Loss: 0.518

==> Confusion:
[[274   2   2   1   1   1   3  16]
 [  3 263  22   0   0   1   0  11]
 [  2  19 264   1   0   5   0   9]
 [  0   4   0 746  42   9   8  28]
 [  2   0   0  40 778   7  24  28]
 [  8   4  10  10  16 782  10  54]
 [  2   0   0   4  25  14 766  26]
 [ 24  20  15  47  64 118  27 474]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [75][   10/  207]    Overall Loss 0.032784    Objective Loss 0.032784                                        LR 0.000063    Time 0.142944    
Epoch: [75][   20/  207]    Overall Loss 0.029884    Objective Loss 0.029884                                        LR 0.000063    Time 0.110516    
Epoch: [75][   30/  207]    Overall Loss 0.028461    Objective Loss 0.028461                                        LR 0.000063    Time 0.099747    
Epoch: [75][   40/  207]    Overall Loss 0.028164    Objective Loss 0.028164                                        LR 0.000063    Time 0.094869    
Epoch: [75][   50/  207]    Overall Loss 0.027440    Objective Loss 0.027440                                        LR 0.000063    Time 0.091766    
Epoch: [75][   60/  207]    Overall Loss 0.027601    Objective Loss 0.027601                                        LR 0.000063    Time 0.089065    
Epoch: [75][   70/  207]    Overall Loss 0.028444    Objective Loss 0.028444                                        LR 0.000063    Time 0.086984    
Epoch: [75][   80/  207]    Overall Loss 0.028074    Objective Loss 0.028074                                        LR 0.000063    Time 0.085308    
Epoch: [75][   90/  207]    Overall Loss 0.028658    Objective Loss 0.028658                                        LR 0.000063    Time 0.084024    
Epoch: [75][  100/  207]    Overall Loss 0.028513    Objective Loss 0.028513                                        LR 0.000063    Time 0.083313    
Epoch: [75][  110/  207]    Overall Loss 0.028589    Objective Loss 0.028589                                        LR 0.000063    Time 0.082285    
Epoch: [75][  120/  207]    Overall Loss 0.028416    Objective Loss 0.028416                                        LR 0.000063    Time 0.081686    
Epoch: [75][  130/  207]    Overall Loss 0.028644    Objective Loss 0.028644                                        LR 0.000063    Time 0.081436    
Epoch: [75][  140/  207]    Overall Loss 0.028429    Objective Loss 0.028429                                        LR 0.000063    Time 0.081393    
Epoch: [75][  150/  207]    Overall Loss 0.028408    Objective Loss 0.028408                                        LR 0.000063    Time 0.081264    
Epoch: [75][  160/  207]    Overall Loss 0.028620    Objective Loss 0.028620                                        LR 0.000063    Time 0.081251    
Epoch: [75][  170/  207]    Overall Loss 0.028632    Objective Loss 0.028632                                        LR 0.000063    Time 0.081153    
Epoch: [75][  180/  207]    Overall Loss 0.028701    Objective Loss 0.028701                                        LR 0.000063    Time 0.081109    
Epoch: [75][  190/  207]    Overall Loss 0.028720    Objective Loss 0.028720                                        LR 0.000063    Time 0.081044    
Epoch: [75][  200/  207]    Overall Loss 0.028893    Objective Loss 0.028893                                        LR 0.000063    Time 0.080667    
Epoch: [75][  207/  207]    Overall Loss 0.028910    Objective Loss 0.028910    Top1 97.621744    Top5 100.000000    LR 0.000063    Time 0.080325    
--- validate (epoch=75)-----------
5136 samples (512 per mini-batch)
Epoch: [75][   10/   11]    Loss 0.583112    Top1 84.375000    Top5 99.707031    
Epoch: [75][   11/   11]    Loss 0.546988    Top1 84.404206    Top5 99.707944    
==> Top1: 84.404    Top5: 99.708    Loss: 0.547

==> Confusion:
[[270   2   2   1   2   1   3  19]
 [  2 263  22   1   0   1   0  11]
 [  2  19 263   1   0   4   0  11]
 [  0   4   0 740  48   9  13  23]
 [  2   0   0  41 776   6  28  26]
 [  9   5   8  10  19 774  14  55]
 [  2   0   0   4  25  10 772  24]
 [ 19  19  13  49  71 112  29 477]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [76][   10/  207]    Overall Loss 0.027255    Objective Loss 0.027255                                        LR 0.000063    Time 0.139316    
Epoch: [76][   20/  207]    Overall Loss 0.028545    Objective Loss 0.028545                                        LR 0.000063    Time 0.106250    
Epoch: [76][   30/  207]    Overall Loss 0.027953    Objective Loss 0.027953                                        LR 0.000063    Time 0.095948    
Epoch: [76][   40/  207]    Overall Loss 0.028603    Objective Loss 0.028603                                        LR 0.000063    Time 0.092050    
Epoch: [76][   50/  207]    Overall Loss 0.028861    Objective Loss 0.028861                                        LR 0.000063    Time 0.089751    
Epoch: [76][   60/  207]    Overall Loss 0.029193    Objective Loss 0.029193                                        LR 0.000063    Time 0.088341    
Epoch: [76][   70/  207]    Overall Loss 0.028588    Objective Loss 0.028588                                        LR 0.000063    Time 0.087281    
Epoch: [76][   80/  207]    Overall Loss 0.028291    Objective Loss 0.028291                                        LR 0.000063    Time 0.086489    
Epoch: [76][   90/  207]    Overall Loss 0.028487    Objective Loss 0.028487                                        LR 0.000063    Time 0.085645    
Epoch: [76][  100/  207]    Overall Loss 0.028115    Objective Loss 0.028115                                        LR 0.000063    Time 0.084723    
Epoch: [76][  110/  207]    Overall Loss 0.027808    Objective Loss 0.027808                                        LR 0.000063    Time 0.083735    
Epoch: [76][  120/  207]    Overall Loss 0.028144    Objective Loss 0.028144                                        LR 0.000063    Time 0.082954    
Epoch: [76][  130/  207]    Overall Loss 0.028101    Objective Loss 0.028101                                        LR 0.000063    Time 0.082340    
Epoch: [76][  140/  207]    Overall Loss 0.027860    Objective Loss 0.027860                                        LR 0.000063    Time 0.081673    
Epoch: [76][  150/  207]    Overall Loss 0.027973    Objective Loss 0.027973                                        LR 0.000063    Time 0.081658    
Epoch: [76][  160/  207]    Overall Loss 0.027995    Objective Loss 0.027995                                        LR 0.000063    Time 0.081323    
Epoch: [76][  170/  207]    Overall Loss 0.028138    Objective Loss 0.028138                                        LR 0.000063    Time 0.081671    
Epoch: [76][  180/  207]    Overall Loss 0.028329    Objective Loss 0.028329                                        LR 0.000063    Time 0.082298    
Epoch: [76][  190/  207]    Overall Loss 0.028269    Objective Loss 0.028269                                        LR 0.000063    Time 0.082820    
Epoch: [76][  200/  207]    Overall Loss 0.028193    Objective Loss 0.028193                                        LR 0.000063    Time 0.083024    
Epoch: [76][  207/  207]    Overall Loss 0.028179    Objective Loss 0.028179    Top1 97.621744    Top5 100.000000    LR 0.000063    Time 0.082753    
--- validate (epoch=76)-----------
5136 samples (512 per mini-batch)
Epoch: [76][   10/   11]    Loss 0.596198    Top1 84.257812    Top5 99.707031    
Epoch: [76][   11/   11]    Loss 0.571411    Top1 84.267913    Top5 99.707944    
==> Top1: 84.268    Top5: 99.708    Loss: 0.571

==> Confusion:
[[271   2   2   1   1   1   3  19]
 [  3 257  25   1   0   1   0  13]
 [  2  14 265   1   0   5   0  13]
 [  0   4   0 743  50   7  11  22]
 [  2   0   0  42 775   6  27  27]
 [  7   5  10  12  21 776  13  50]
 [  2   0   0   4  26  13 760  32]
 [ 18  15  14  51  71 112  26 482]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [77][   10/  207]    Overall Loss 0.027329    Objective Loss 0.027329                                        LR 0.000063    Time 0.140666    
Epoch: [77][   20/  207]    Overall Loss 0.028249    Objective Loss 0.028249                                        LR 0.000063    Time 0.108126    
Epoch: [77][   30/  207]    Overall Loss 0.027229    Objective Loss 0.027229                                        LR 0.000063    Time 0.096693    
Epoch: [77][   40/  207]    Overall Loss 0.027132    Objective Loss 0.027132                                        LR 0.000063    Time 0.090710    
Epoch: [77][   50/  207]    Overall Loss 0.027444    Objective Loss 0.027444                                        LR 0.000063    Time 0.087461    
Epoch: [77][   60/  207]    Overall Loss 0.027163    Objective Loss 0.027163                                        LR 0.000063    Time 0.085225    
Epoch: [77][   70/  207]    Overall Loss 0.027196    Objective Loss 0.027196                                        LR 0.000063    Time 0.083421    
Epoch: [77][   80/  207]    Overall Loss 0.027376    Objective Loss 0.027376                                        LR 0.000063    Time 0.083303    
Epoch: [77][   90/  207]    Overall Loss 0.027105    Objective Loss 0.027105                                        LR 0.000063    Time 0.083260    
Epoch: [77][  100/  207]    Overall Loss 0.027218    Objective Loss 0.027218                                        LR 0.000063    Time 0.082906    
Epoch: [77][  110/  207]    Overall Loss 0.027667    Objective Loss 0.027667                                        LR 0.000063    Time 0.082602    
Epoch: [77][  120/  207]    Overall Loss 0.027759    Objective Loss 0.027759                                        LR 0.000063    Time 0.082300    
Epoch: [77][  130/  207]    Overall Loss 0.027770    Objective Loss 0.027770                                        LR 0.000063    Time 0.082153    
Epoch: [77][  140/  207]    Overall Loss 0.027818    Objective Loss 0.027818                                        LR 0.000063    Time 0.081861    
Epoch: [77][  150/  207]    Overall Loss 0.027854    Objective Loss 0.027854                                        LR 0.000063    Time 0.081348    
Epoch: [77][  160/  207]    Overall Loss 0.027916    Objective Loss 0.027916                                        LR 0.000063    Time 0.080870    
Epoch: [77][  170/  207]    Overall Loss 0.027944    Objective Loss 0.027944                                        LR 0.000063    Time 0.080486    
Epoch: [77][  180/  207]    Overall Loss 0.028013    Objective Loss 0.028013                                        LR 0.000063    Time 0.080099    
Epoch: [77][  190/  207]    Overall Loss 0.027998    Objective Loss 0.027998                                        LR 0.000063    Time 0.079851    
Epoch: [77][  200/  207]    Overall Loss 0.028098    Objective Loss 0.028098                                        LR 0.000063    Time 0.079639    
Epoch: [77][  207/  207]    Overall Loss 0.028178    Objective Loss 0.028178    Top1 97.508494    Top5 100.000000    LR 0.000063    Time 0.079551    
--- validate (epoch=77)-----------
5136 samples (512 per mini-batch)
Epoch: [77][   10/   11]    Loss 0.580010    Top1 84.042969    Top5 99.726562    
Epoch: [77][   11/   11]    Loss 0.656013    Top1 84.034268    Top5 99.727414    
==> Top1: 84.034    Top5: 99.727    Loss: 0.656

==> Confusion:
[[273   2   2   1   1   1   3  17]
 [  6 258  25   2   0   1   0   8]
 [  1  17 261   1   0   6   0  14]
 [  0   4   0 729  55  11  11  27]
 [  2   0   0  40 775   6  29  27]
 [ 10   4  10  10  21 783  11  45]
 [  2   0   0   4  26  10 767  28]
 [ 23  17  14  49  67 122  27 470]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [78][   10/  207]    Overall Loss 0.026305    Objective Loss 0.026305                                        LR 0.000063    Time 0.149437    
Epoch: [78][   20/  207]    Overall Loss 0.026484    Objective Loss 0.026484                                        LR 0.000063    Time 0.114055    
Epoch: [78][   30/  207]    Overall Loss 0.025924    Objective Loss 0.025924                                        LR 0.000063    Time 0.102679    
Epoch: [78][   40/  207]    Overall Loss 0.026091    Objective Loss 0.026091                                        LR 0.000063    Time 0.096187    
Epoch: [78][   50/  207]    Overall Loss 0.026952    Objective Loss 0.026952                                        LR 0.000063    Time 0.091772    
Epoch: [78][   60/  207]    Overall Loss 0.026924    Objective Loss 0.026924                                        LR 0.000063    Time 0.088404    
Epoch: [78][   70/  207]    Overall Loss 0.026785    Objective Loss 0.026785                                        LR 0.000063    Time 0.086233    
Epoch: [78][   80/  207]    Overall Loss 0.027164    Objective Loss 0.027164                                        LR 0.000063    Time 0.084893    
Epoch: [78][   90/  207]    Overall Loss 0.027381    Objective Loss 0.027381                                        LR 0.000063    Time 0.083739    
Epoch: [78][  100/  207]    Overall Loss 0.027589    Objective Loss 0.027589                                        LR 0.000063    Time 0.083023    
Epoch: [78][  110/  207]    Overall Loss 0.027954    Objective Loss 0.027954                                        LR 0.000063    Time 0.082456    
Epoch: [78][  120/  207]    Overall Loss 0.027865    Objective Loss 0.027865                                        LR 0.000063    Time 0.082164    
Epoch: [78][  130/  207]    Overall Loss 0.027945    Objective Loss 0.027945                                        LR 0.000063    Time 0.081993    
Epoch: [78][  140/  207]    Overall Loss 0.027825    Objective Loss 0.027825                                        LR 0.000063    Time 0.081775    
Epoch: [78][  150/  207]    Overall Loss 0.028022    Objective Loss 0.028022                                        LR 0.000063    Time 0.081665    
Epoch: [78][  160/  207]    Overall Loss 0.027875    Objective Loss 0.027875                                        LR 0.000063    Time 0.081512    
Epoch: [78][  170/  207]    Overall Loss 0.027854    Objective Loss 0.027854                                        LR 0.000063    Time 0.081264    
Epoch: [78][  180/  207]    Overall Loss 0.027766    Objective Loss 0.027766                                        LR 0.000063    Time 0.080877    
Epoch: [78][  190/  207]    Overall Loss 0.028063    Objective Loss 0.028063                                        LR 0.000063    Time 0.080500    
Epoch: [78][  200/  207]    Overall Loss 0.027902    Objective Loss 0.027902                                        LR 0.000063    Time 0.080246    
Epoch: [78][  207/  207]    Overall Loss 0.027960    Objective Loss 0.027960    Top1 97.395243    Top5 100.000000    LR 0.000063    Time 0.079889    
--- validate (epoch=78)-----------
5136 samples (512 per mini-batch)
Epoch: [78][   10/   11]    Loss 0.578879    Top1 84.511719    Top5 99.667969    
Epoch: [78][   11/   11]    Loss 0.560165    Top1 84.521028    Top5 99.669003    
==> Top1: 84.521    Top5: 99.669    Loss: 0.560

==> Confusion:
[[273   2   1   1   1   1   3  18]
 [  2 258  26   0   0   1   0  13]
 [  3  20 262   1   0   4   0  10]
 [  0   4   0 741  54   6  13  19]
 [  2   0   0  39 778   6  29  25]
 [  8   3  10  15  20 776  15  47]
 [  2   0   0   4  26  10 773  22]
 [ 21  14  15  49  69 111  30 480]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [79][   10/  207]    Overall Loss 0.023921    Objective Loss 0.023921                                        LR 0.000063    Time 0.115452    
Epoch: [79][   20/  207]    Overall Loss 0.027362    Objective Loss 0.027362                                        LR 0.000063    Time 0.095898    
Epoch: [79][   30/  207]    Overall Loss 0.027627    Objective Loss 0.027627                                        LR 0.000063    Time 0.090213    
Epoch: [79][   40/  207]    Overall Loss 0.027685    Objective Loss 0.027685                                        LR 0.000063    Time 0.087479    
Epoch: [79][   50/  207]    Overall Loss 0.027076    Objective Loss 0.027076                                        LR 0.000063    Time 0.085872    
Epoch: [79][   60/  207]    Overall Loss 0.026476    Objective Loss 0.026476                                        LR 0.000063    Time 0.084991    
Epoch: [79][   70/  207]    Overall Loss 0.026093    Objective Loss 0.026093                                        LR 0.000063    Time 0.083957    
Epoch: [79][   80/  207]    Overall Loss 0.026295    Objective Loss 0.026295                                        LR 0.000063    Time 0.083033    
Epoch: [79][   90/  207]    Overall Loss 0.026066    Objective Loss 0.026066                                        LR 0.000063    Time 0.082013    
Epoch: [79][  100/  207]    Overall Loss 0.026078    Objective Loss 0.026078                                        LR 0.000063    Time 0.081162    
Epoch: [79][  110/  207]    Overall Loss 0.026333    Objective Loss 0.026333                                        LR 0.000063    Time 0.080503    
Epoch: [79][  120/  207]    Overall Loss 0.026177    Objective Loss 0.026177                                        LR 0.000063    Time 0.079950    
Epoch: [79][  130/  207]    Overall Loss 0.026730    Objective Loss 0.026730                                        LR 0.000063    Time 0.079467    
Epoch: [79][  140/  207]    Overall Loss 0.026689    Objective Loss 0.026689                                        LR 0.000063    Time 0.079012    
Epoch: [79][  150/  207]    Overall Loss 0.026868    Objective Loss 0.026868                                        LR 0.000063    Time 0.078804    
Epoch: [79][  160/  207]    Overall Loss 0.027015    Objective Loss 0.027015                                        LR 0.000063    Time 0.078801    
Epoch: [79][  170/  207]    Overall Loss 0.027329    Objective Loss 0.027329                                        LR 0.000063    Time 0.078979    
Epoch: [79][  180/  207]    Overall Loss 0.027312    Objective Loss 0.027312                                        LR 0.000063    Time 0.079190    
Epoch: [79][  190/  207]    Overall Loss 0.027388    Objective Loss 0.027388                                        LR 0.000063    Time 0.079256    
Epoch: [79][  200/  207]    Overall Loss 0.027572    Objective Loss 0.027572                                        LR 0.000063    Time 0.079359    
Epoch: [79][  207/  207]    Overall Loss 0.027734    Objective Loss 0.027734    Top1 96.828992    Top5 100.000000    LR 0.000063    Time 0.079181    
--- validate (epoch=79)-----------
5136 samples (512 per mini-batch)
Epoch: [79][   10/   11]    Loss 0.583713    Top1 84.218750    Top5 99.746094    
Epoch: [79][   11/   11]    Loss 0.582687    Top1 84.190031    Top5 99.727414    
==> Top1: 84.190    Top5: 99.727    Loss: 0.583

==> Confusion:
[[271   3   1   1   0   2   3  19]
 [  4 260  22   1   0   2   0  11]
 [  1  18 264   1   0   5   0  11]
 [  0   4   0 745  49   9  10  20]
 [  2   0   0  41 776   6  25  29]
 [  8   6   9  10  19 790  10  42]
 [  2   0   0   6  28  15 753  33]
 [ 18  18  15  51  70 130  22 465]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [80][   10/  207]    Overall Loss 0.026697    Objective Loss 0.026697                                        LR 0.000031    Time 0.134707    
Epoch: [80][   20/  207]    Overall Loss 0.028317    Objective Loss 0.028317                                        LR 0.000031    Time 0.104280    
Epoch: [80][   30/  207]    Overall Loss 0.026927    Objective Loss 0.026927                                        LR 0.000031    Time 0.094471    
Epoch: [80][   40/  207]    Overall Loss 0.026663    Objective Loss 0.026663                                        LR 0.000031    Time 0.089115    
Epoch: [80][   50/  207]    Overall Loss 0.025681    Objective Loss 0.025681                                        LR 0.000031    Time 0.085810    
Epoch: [80][   60/  207]    Overall Loss 0.026000    Objective Loss 0.026000                                        LR 0.000031    Time 0.084515    
Epoch: [80][   70/  207]    Overall Loss 0.025970    Objective Loss 0.025970                                        LR 0.000031    Time 0.084124    
Epoch: [80][   80/  207]    Overall Loss 0.026211    Objective Loss 0.026211                                        LR 0.000031    Time 0.083675    
Epoch: [80][   90/  207]    Overall Loss 0.026773    Objective Loss 0.026773                                        LR 0.000031    Time 0.083286    
Epoch: [80][  100/  207]    Overall Loss 0.026479    Objective Loss 0.026479                                        LR 0.000031    Time 0.084095    
Epoch: [80][  110/  207]    Overall Loss 0.026412    Objective Loss 0.026412                                        LR 0.000031    Time 0.085186    
Epoch: [80][  120/  207]    Overall Loss 0.026587    Objective Loss 0.026587                                        LR 0.000031    Time 0.084751    
Epoch: [80][  130/  207]    Overall Loss 0.026523    Objective Loss 0.026523                                        LR 0.000031    Time 0.084077    
Epoch: [80][  140/  207]    Overall Loss 0.026452    Objective Loss 0.026452                                        LR 0.000031    Time 0.083438    
Epoch: [80][  150/  207]    Overall Loss 0.026437    Objective Loss 0.026437                                        LR 0.000031    Time 0.082789    
Epoch: [80][  160/  207]    Overall Loss 0.026447    Objective Loss 0.026447                                        LR 0.000031    Time 0.082366    
Epoch: [80][  170/  207]    Overall Loss 0.026548    Objective Loss 0.026548                                        LR 0.000031    Time 0.081951    
Epoch: [80][  180/  207]    Overall Loss 0.026685    Objective Loss 0.026685                                        LR 0.000031    Time 0.081594    
Epoch: [80][  190/  207]    Overall Loss 0.026657    Objective Loss 0.026657                                        LR 0.000031    Time 0.082033    
Epoch: [80][  200/  207]    Overall Loss 0.026689    Objective Loss 0.026689                                        LR 0.000031    Time 0.082151    
Epoch: [80][  207/  207]    Overall Loss 0.026745    Objective Loss 0.026745    Top1 98.867497    Top5 100.000000    LR 0.000031    Time 0.081891    
--- validate (epoch=80)-----------
5136 samples (512 per mini-batch)
Epoch: [80][   10/   11]    Loss 0.587882    Top1 84.394531    Top5 99.707031    
Epoch: [80][   11/   11]    Loss 0.683571    Top1 84.384735    Top5 99.707944    
==> Top1: 84.385    Top5: 99.708    Loss: 0.684

==> Confusion:
[[270   2   2   2   2   1   3  18]
 [  4 261  21   1   0   2   0  11]
 [  2  15 263   1   0   5   0  14]
 [  0   4   0 736  56   8  10  23]
 [  2   0   0  33 784   7  27  26]
 [  9   4  10  11  20 782  13  45]
 [  2   0   0   4  28  11 763  29]
 [ 22  13  13  50  70 115  31 475]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [81][   10/  207]    Overall Loss 0.025842    Objective Loss 0.025842                                        LR 0.000031    Time 0.165540    
Epoch: [81][   20/  207]    Overall Loss 0.025935    Objective Loss 0.025935                                        LR 0.000031    Time 0.125260    
Epoch: [81][   30/  207]    Overall Loss 0.025716    Objective Loss 0.025716                                        LR 0.000031    Time 0.109675    
Epoch: [81][   40/  207]    Overall Loss 0.025994    Objective Loss 0.025994                                        LR 0.000031    Time 0.102374    
Epoch: [81][   50/  207]    Overall Loss 0.026288    Objective Loss 0.026288                                        LR 0.000031    Time 0.097827    
Epoch: [81][   60/  207]    Overall Loss 0.026332    Objective Loss 0.026332                                        LR 0.000031    Time 0.094286    
Epoch: [81][   70/  207]    Overall Loss 0.025911    Objective Loss 0.025911                                        LR 0.000031    Time 0.091982    
Epoch: [81][   80/  207]    Overall Loss 0.025937    Objective Loss 0.025937                                        LR 0.000031    Time 0.089740    
Epoch: [81][   90/  207]    Overall Loss 0.026189    Objective Loss 0.026189                                        LR 0.000031    Time 0.088093    
Epoch: [81][  100/  207]    Overall Loss 0.026196    Objective Loss 0.026196                                        LR 0.000031    Time 0.086697    
Epoch: [81][  110/  207]    Overall Loss 0.026043    Objective Loss 0.026043                                        LR 0.000031    Time 0.085447    
Epoch: [81][  120/  207]    Overall Loss 0.025955    Objective Loss 0.025955                                        LR 0.000031    Time 0.084556    
Epoch: [81][  130/  207]    Overall Loss 0.026001    Objective Loss 0.026001                                        LR 0.000031    Time 0.083971    
Epoch: [81][  140/  207]    Overall Loss 0.026230    Objective Loss 0.026230                                        LR 0.000031    Time 0.083281    
Epoch: [81][  150/  207]    Overall Loss 0.026307    Objective Loss 0.026307                                        LR 0.000031    Time 0.082726    
Epoch: [81][  160/  207]    Overall Loss 0.026324    Objective Loss 0.026324                                        LR 0.000031    Time 0.082103    
Epoch: [81][  170/  207]    Overall Loss 0.026293    Objective Loss 0.026293                                        LR 0.000031    Time 0.081628    
Epoch: [81][  180/  207]    Overall Loss 0.026569    Objective Loss 0.026569                                        LR 0.000031    Time 0.081078    
Epoch: [81][  190/  207]    Overall Loss 0.026375    Objective Loss 0.026375                                        LR 0.000031    Time 0.080914    
Epoch: [81][  200/  207]    Overall Loss 0.026478    Objective Loss 0.026478                                        LR 0.000031    Time 0.080723    
Epoch: [81][  207/  207]    Overall Loss 0.026585    Objective Loss 0.026585    Top1 96.828992    Top5 99.886750    LR 0.000031    Time 0.080400    
--- validate (epoch=81)-----------
5136 samples (512 per mini-batch)
Epoch: [81][   10/   11]    Loss 0.588843    Top1 84.375000    Top5 99.707031    
Epoch: [81][   11/   11]    Loss 0.769518    Top1 84.326324    Top5 99.707944    
==> Top1: 84.326    Top5: 99.708    Loss: 0.770

==> Confusion:
[[267   3   2   2   2   3   3  18]
 [  2 261  22   1   0   1   0  13]
 [  1  17 263   1   0   5   0  13]
 [  0   4   0 741  53   9  10  20]
 [  2   0   0  38 780   7  26  26]
 [  7   5   8  12  21 784  11  46]
 [  2   0   0   4  26  12 763  30]
 [ 17  14  14  57  64 122  29 472]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [82][   10/  207]    Overall Loss 0.026628    Objective Loss 0.026628                                        LR 0.000031    Time 0.161593    
Epoch: [82][   20/  207]    Overall Loss 0.026155    Objective Loss 0.026155                                        LR 0.000031    Time 0.127426    
Epoch: [82][   30/  207]    Overall Loss 0.025408    Objective Loss 0.025408                                        LR 0.000031    Time 0.116692    
Epoch: [82][   40/  207]    Overall Loss 0.026325    Objective Loss 0.026325                                        LR 0.000031    Time 0.111480    
Epoch: [82][   50/  207]    Overall Loss 0.026307    Objective Loss 0.026307                                        LR 0.000031    Time 0.108406    
Epoch: [82][   60/  207]    Overall Loss 0.026950    Objective Loss 0.026950                                        LR 0.000031    Time 0.103902    
Epoch: [82][   70/  207]    Overall Loss 0.026764    Objective Loss 0.026764                                        LR 0.000031    Time 0.100062    
Epoch: [82][   80/  207]    Overall Loss 0.026628    Objective Loss 0.026628                                        LR 0.000031    Time 0.097075    
Epoch: [82][   90/  207]    Overall Loss 0.026491    Objective Loss 0.026491                                        LR 0.000031    Time 0.094632    
Epoch: [82][  100/  207]    Overall Loss 0.026322    Objective Loss 0.026322                                        LR 0.000031    Time 0.092944    
Epoch: [82][  110/  207]    Overall Loss 0.026370    Objective Loss 0.026370                                        LR 0.000031    Time 0.091686    
Epoch: [82][  120/  207]    Overall Loss 0.026402    Objective Loss 0.026402                                        LR 0.000031    Time 0.090552    
Epoch: [82][  130/  207]    Overall Loss 0.026354    Objective Loss 0.026354                                        LR 0.000031    Time 0.089558    
Epoch: [82][  140/  207]    Overall Loss 0.026599    Objective Loss 0.026599                                        LR 0.000031    Time 0.088422    
Epoch: [82][  150/  207]    Overall Loss 0.026564    Objective Loss 0.026564                                        LR 0.000031    Time 0.087462    
Epoch: [82][  160/  207]    Overall Loss 0.026502    Objective Loss 0.026502                                        LR 0.000031    Time 0.086573    
Epoch: [82][  170/  207]    Overall Loss 0.026406    Objective Loss 0.026406                                        LR 0.000031    Time 0.085910    
Epoch: [82][  180/  207]    Overall Loss 0.026269    Objective Loss 0.026269                                        LR 0.000031    Time 0.085463    
Epoch: [82][  190/  207]    Overall Loss 0.026301    Objective Loss 0.026301                                        LR 0.000031    Time 0.085079    
Epoch: [82][  200/  207]    Overall Loss 0.026275    Objective Loss 0.026275                                        LR 0.000031    Time 0.084584    
Epoch: [82][  207/  207]    Overall Loss 0.026326    Objective Loss 0.026326    Top1 97.734994    Top5 100.000000    LR 0.000031    Time 0.084151    
--- validate (epoch=82)-----------
5136 samples (512 per mini-batch)
Epoch: [82][   10/   11]    Loss 0.578017    Top1 84.609375    Top5 99.707031    
Epoch: [82][   11/   11]    Loss 0.627321    Top1 84.598910    Top5 99.707944    
==> Top1: 84.599    Top5: 99.708    Loss: 0.627

==> Confusion:
[[273   2   2   1   2   1   3  16]
 [  1 262  22   1   0   2   0  12]
 [  1  17 264   1   0   5   0  12]
 [  0   4   0 743  48  10  10  22]
 [  2   0   0  42 776   6  26  27]
 [  7   6  11  10  19 785  11  45]
 [  2   0   0   4  26  13 767  25]
 [ 19  15  15  50  69 117  29 475]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [83][   10/  207]    Overall Loss 0.026815    Objective Loss 0.026815                                        LR 0.000031    Time 0.138211    
Epoch: [83][   20/  207]    Overall Loss 0.026499    Objective Loss 0.026499                                        LR 0.000031    Time 0.107284    
Epoch: [83][   30/  207]    Overall Loss 0.025655    Objective Loss 0.025655                                        LR 0.000031    Time 0.096954    
Epoch: [83][   40/  207]    Overall Loss 0.024773    Objective Loss 0.024773                                        LR 0.000031    Time 0.091616    
Epoch: [83][   50/  207]    Overall Loss 0.024726    Objective Loss 0.024726                                        LR 0.000031    Time 0.089018    
Epoch: [83][   60/  207]    Overall Loss 0.024927    Objective Loss 0.024927                                        LR 0.000031    Time 0.087089    
Epoch: [83][   70/  207]    Overall Loss 0.025450    Objective Loss 0.025450                                        LR 0.000031    Time 0.085706    
Epoch: [83][   80/  207]    Overall Loss 0.025686    Objective Loss 0.025686                                        LR 0.000031    Time 0.084131    
Epoch: [83][   90/  207]    Overall Loss 0.026013    Objective Loss 0.026013                                        LR 0.000031    Time 0.082841    
Epoch: [83][  100/  207]    Overall Loss 0.025941    Objective Loss 0.025941                                        LR 0.000031    Time 0.081812    
Epoch: [83][  110/  207]    Overall Loss 0.025986    Objective Loss 0.025986                                        LR 0.000031    Time 0.081291    
Epoch: [83][  120/  207]    Overall Loss 0.025940    Objective Loss 0.025940                                        LR 0.000031    Time 0.080872    
Epoch: [83][  130/  207]    Overall Loss 0.026175    Objective Loss 0.026175                                        LR 0.000031    Time 0.080530    
Epoch: [83][  140/  207]    Overall Loss 0.026278    Objective Loss 0.026278                                        LR 0.000031    Time 0.080354    
Epoch: [83][  150/  207]    Overall Loss 0.026265    Objective Loss 0.026265                                        LR 0.000031    Time 0.080391    
Epoch: [83][  160/  207]    Overall Loss 0.026414    Objective Loss 0.026414                                        LR 0.000031    Time 0.080241    
Epoch: [83][  170/  207]    Overall Loss 0.026628    Objective Loss 0.026628                                        LR 0.000031    Time 0.081044    
Epoch: [83][  180/  207]    Overall Loss 0.026697    Objective Loss 0.026697                                        LR 0.000031    Time 0.081601    
Epoch: [83][  190/  207]    Overall Loss 0.026481    Objective Loss 0.026481                                        LR 0.000031    Time 0.082085    
Epoch: [83][  200/  207]    Overall Loss 0.026483    Objective Loss 0.026483                                        LR 0.000031    Time 0.081799    
Epoch: [83][  207/  207]    Overall Loss 0.026533    Objective Loss 0.026533    Top1 97.734994    Top5 100.000000    LR 0.000031    Time 0.081424    
--- validate (epoch=83)-----------
5136 samples (512 per mini-batch)
Epoch: [83][   10/   11]    Loss 0.592463    Top1 84.433594    Top5 99.687500    
Epoch: [83][   11/   11]    Loss 0.550687    Top1 84.462617    Top5 99.688474    
==> Top1: 84.463    Top5: 99.688    Loss: 0.551

==> Confusion:
[[269   3   2   2   1   2   3  18]
 [  3 260  22   1   0   1   1  12]
 [  1  16 261   1   0   6   0  15]
 [  0   4   0 739  51   9  13  21]
 [  2   0   0  39 778   6  29  25]
 [  9   6   7  12  17 789  11  43]
 [  2   0   0   4  24  14 770  23]
 [ 17  15  14  51  62 126  32 472]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [84][   10/  207]    Overall Loss 0.025224    Objective Loss 0.025224                                        LR 0.000031    Time 0.138742    
Epoch: [84][   20/  207]    Overall Loss 0.025730    Objective Loss 0.025730                                        LR 0.000031    Time 0.106830    
Epoch: [84][   30/  207]    Overall Loss 0.024611    Objective Loss 0.024611                                        LR 0.000031    Time 0.098339    
Epoch: [84][   40/  207]    Overall Loss 0.025727    Objective Loss 0.025727                                        LR 0.000031    Time 0.095766    
Epoch: [84][   50/  207]    Overall Loss 0.025933    Objective Loss 0.025933                                        LR 0.000031    Time 0.095274    
Epoch: [84][   60/  207]    Overall Loss 0.025953    Objective Loss 0.025953                                        LR 0.000031    Time 0.094836    
Epoch: [84][   70/  207]    Overall Loss 0.026298    Objective Loss 0.026298                                        LR 0.000031    Time 0.095267    
Epoch: [84][   80/  207]    Overall Loss 0.026409    Objective Loss 0.026409                                        LR 0.000031    Time 0.094038    
Epoch: [84][   90/  207]    Overall Loss 0.026367    Objective Loss 0.026367                                        LR 0.000031    Time 0.092329    
Epoch: [84][  100/  207]    Overall Loss 0.025988    Objective Loss 0.025988                                        LR 0.000031    Time 0.090309    
Epoch: [84][  110/  207]    Overall Loss 0.026084    Objective Loss 0.026084                                        LR 0.000031    Time 0.088812    
Epoch: [84][  120/  207]    Overall Loss 0.025884    Objective Loss 0.025884                                        LR 0.000031    Time 0.087468    
Epoch: [84][  130/  207]    Overall Loss 0.026046    Objective Loss 0.026046                                        LR 0.000031    Time 0.086356    
Epoch: [84][  140/  207]    Overall Loss 0.026114    Objective Loss 0.026114                                        LR 0.000031    Time 0.085437    
Epoch: [84][  150/  207]    Overall Loss 0.026122    Objective Loss 0.026122                                        LR 0.000031    Time 0.084875    
Epoch: [84][  160/  207]    Overall Loss 0.026128    Objective Loss 0.026128                                        LR 0.000031    Time 0.084092    
Epoch: [84][  170/  207]    Overall Loss 0.026422    Objective Loss 0.026422                                        LR 0.000031    Time 0.083990    
Epoch: [84][  180/  207]    Overall Loss 0.026335    Objective Loss 0.026335                                        LR 0.000031    Time 0.083732    
Epoch: [84][  190/  207]    Overall Loss 0.026573    Objective Loss 0.026573                                        LR 0.000031    Time 0.083579    
Epoch: [84][  200/  207]    Overall Loss 0.026298    Objective Loss 0.026298                                        LR 0.000031    Time 0.083267    
Epoch: [84][  207/  207]    Overall Loss 0.026232    Objective Loss 0.026232    Top1 98.640997    Top5 100.000000    LR 0.000031    Time 0.082948    
--- validate (epoch=84)-----------
5136 samples (512 per mini-batch)
Epoch: [84][   10/   11]    Loss 0.580967    Top1 84.394531    Top5 99.707031    
Epoch: [84][   11/   11]    Loss 0.535306    Top1 84.423676    Top5 99.707944    
==> Top1: 84.424    Top5: 99.708    Loss: 0.535

==> Confusion:
[[271   3   2   2   2   1   3  16]
 [  3 259  25   1   0   1   0  11]
 [  1  20 263   1   0   5   0  10]
 [  0   4   0 742  52  12   9  18]
 [  2   0   0  37 780   8  25  27]
 [  8   7   8  10  20 792  10  39]
 [  2   0   0   4  29  16 758  28]
 [ 20  16  16  51  68 125  22 471]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [85][   10/  207]    Overall Loss 0.027268    Objective Loss 0.027268                                        LR 0.000016    Time 0.136821    
Epoch: [85][   20/  207]    Overall Loss 0.026767    Objective Loss 0.026767                                        LR 0.000016    Time 0.104225    
Epoch: [85][   30/  207]    Overall Loss 0.026641    Objective Loss 0.026641                                        LR 0.000016    Time 0.094024    
Epoch: [85][   40/  207]    Overall Loss 0.025626    Objective Loss 0.025626                                        LR 0.000016    Time 0.089024    
Epoch: [85][   50/  207]    Overall Loss 0.024852    Objective Loss 0.024852                                        LR 0.000016    Time 0.086194    
Epoch: [85][   60/  207]    Overall Loss 0.024973    Objective Loss 0.024973                                        LR 0.000016    Time 0.084695    
Epoch: [85][   70/  207]    Overall Loss 0.024737    Objective Loss 0.024737                                        LR 0.000016    Time 0.083343    
Epoch: [85][   80/  207]    Overall Loss 0.024872    Objective Loss 0.024872                                        LR 0.000016    Time 0.082997    
Epoch: [85][   90/  207]    Overall Loss 0.024599    Objective Loss 0.024599                                        LR 0.000016    Time 0.083329    
Epoch: [85][  100/  207]    Overall Loss 0.024662    Objective Loss 0.024662                                        LR 0.000016    Time 0.084337    
Epoch: [85][  110/  207]    Overall Loss 0.025123    Objective Loss 0.025123                                        LR 0.000016    Time 0.085109    
Epoch: [85][  120/  207]    Overall Loss 0.025432    Objective Loss 0.025432                                        LR 0.000016    Time 0.085695    
Epoch: [85][  130/  207]    Overall Loss 0.025520    Objective Loss 0.025520                                        LR 0.000016    Time 0.085819    
Epoch: [85][  140/  207]    Overall Loss 0.025672    Objective Loss 0.025672                                        LR 0.000016    Time 0.085084    
Epoch: [85][  150/  207]    Overall Loss 0.025562    Objective Loss 0.025562                                        LR 0.000016    Time 0.084435    
Epoch: [85][  160/  207]    Overall Loss 0.025758    Objective Loss 0.025758                                        LR 0.000016    Time 0.083741    
Epoch: [85][  170/  207]    Overall Loss 0.025836    Objective Loss 0.025836                                        LR 0.000016    Time 0.083277    
Epoch: [85][  180/  207]    Overall Loss 0.025910    Objective Loss 0.025910                                        LR 0.000016    Time 0.082623    
Epoch: [85][  190/  207]    Overall Loss 0.025848    Objective Loss 0.025848                                        LR 0.000016    Time 0.081998    
Epoch: [85][  200/  207]    Overall Loss 0.025780    Objective Loss 0.025780                                        LR 0.000016    Time 0.081509    
Epoch: [85][  207/  207]    Overall Loss 0.025663    Objective Loss 0.025663    Top1 98.074745    Top5 99.886750    LR 0.000016    Time 0.081049    
--- validate (epoch=85)-----------
5136 samples (512 per mini-batch)
Epoch: [85][   10/   11]    Loss 0.598455    Top1 84.335938    Top5 99.707031    
Epoch: [85][   11/   11]    Loss 0.641327    Top1 84.345794    Top5 99.707944    
==> Top1: 84.346    Top5: 99.708    Loss: 0.641

==> Confusion:
[[269   2   2   1   2   3   3  18]
 [  3 260  21   1   0   2   1  12]
 [  1  16 262   1   0   6   0  14]
 [  0   4   0 738  51  10  12  22]
 [  2   0   0  38 779   8  25  27]
 [  8   4   9  12  19 787  12  43]
 [  2   0   0   4  24  16 765  26]
 [ 17  15  14  48  71 126  26 472]]

==> Best [Top1: 84.657   Top5: 99.747   Sparsity:0.00   Params: 117200 on epoch: 74]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [86][   10/  207]    Overall Loss 0.024986    Objective Loss 0.024986                                        LR 0.000016    Time 0.136614    
Epoch: [86][   20/  207]    Overall Loss 0.025098    Objective Loss 0.025098                                        LR 0.000016    Time 0.107877    
Epoch: [86][   30/  207]    Overall Loss 0.024726    Objective Loss 0.024726                                        LR 0.000016    Time 0.098729    
Epoch: [86][   40/  207]    Overall Loss 0.025467    Objective Loss 0.025467                                        LR 0.000016    Time 0.094153    
Epoch: [86][   50/  207]    Overall Loss 0.025580    Objective Loss 0.025580                                        LR 0.000016    Time 0.091389    
Epoch: [86][   60/  207]    Overall Loss 0.025735    Objective Loss 0.025735                                        LR 0.000016    Time 0.089754    
Epoch: [86][   70/  207]    Overall Loss 0.025825    Objective Loss 0.025825                                        LR 0.000016    Time 0.088149    
Epoch: [86][   80/  207]    Overall Loss 0.025432    Objective Loss 0.025432                                        LR 0.000016    Time 0.086875    
Epoch: [86][   90/  207]    Overall Loss 0.025548    Objective Loss 0.025548                                        LR 0.000016    Time 0.085823    
Epoch: [86][  100/  207]    Overall Loss 0.025413    Objective Loss 0.025413                                        LR 0.000016    Time 0.085029    
Epoch: [86][  110/  207]    Overall Loss 0.025173    Objective Loss 0.025173                                        LR 0.000016    Time 0.084073    
Epoch: [86][  120/  207]    Overall Loss 0.025411    Objective Loss 0.025411                                        LR 0.000016    Time 0.083133    
Epoch: [86][  130/  207]    Overall Loss 0.025285    Objective Loss 0.025285                                        LR 0.000016    Time 0.082535    
Epoch: [86][  140/  207]    Overall Loss 0.025317    Objective Loss 0.025317                                        LR 0.000016    Time 0.082119    
Epoch: [86][  150/  207]    Overall Loss 0.025249    Objective Loss 0.025249                                        LR 0.000016    Time 0.081568    
Epoch: [86][  160/  207]    Overall Loss 0.025227    Objective Loss 0.025227                                        LR 0.000016    Time 0.081195    
Epoch: [86][  170/  207]    Overall Loss 0.025224    Objective Loss 0.025224                                        LR 0.000016    Time 0.080742    
Epoch: [86][  180/  207]    Overall Loss 0.025299    Objective Loss 0.025299                                        LR 0.000016    Time 0.080776    
Epoch: [86][  190/  207]    Overall Loss 0.025226    Objective Loss 0.025226                                        LR 0.000016    Time 0.080815    
Epoch: [86][  200/  207]    Overall Loss 0.025441    Objective Loss 0.025441                                        LR 0.000016    Time 0.080685    
Epoch: [86][  207/  207]    Overall Loss 0.025586    Objective Loss 0.025586    Top1 96.942242    Top5 100.000000    LR 0.000016    Time 0.080497    
--- validate (epoch=86)-----------
5136 samples (512 per mini-batch)
Epoch: [86][   10/   11]    Loss 0.583552    Top1 84.765625    Top5 99.687500    
Epoch: [86][   11/   11]    Loss 0.588283    Top1 84.754673    Top5 99.688474    
==> Top1: 84.755    Top5: 99.688    Loss: 0.588

==> Confusion:
[[272   3   2   2   2   1   3  15]
 [  2 263  22   1   0   1   0  11]
 [  1  19 261   1   0   5   0  13]
 [  0   4   0 743  48  11  11  20]
 [  2   0   0  38 780   6  27  26]
 [  8   5   9  10  21 787  10  44]
 [  2   0   0   4  23  15 768  25]
 [ 18  16  14  49  65 120  28 479]]

==> Best [Top1: 84.755   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 86]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [87][   10/  207]    Overall Loss 0.021816    Objective Loss 0.021816                                        LR 0.000008    Time 0.143421    
Epoch: [87][   20/  207]    Overall Loss 0.024840    Objective Loss 0.024840                                        LR 0.000008    Time 0.107799    
Epoch: [87][   30/  207]    Overall Loss 0.025908    Objective Loss 0.025908                                        LR 0.000008    Time 0.096795    
Epoch: [87][   40/  207]    Overall Loss 0.026272    Objective Loss 0.026272                                        LR 0.000008    Time 0.090909    
Epoch: [87][   50/  207]    Overall Loss 0.025801    Objective Loss 0.025801                                        LR 0.000008    Time 0.087570    
Epoch: [87][   60/  207]    Overall Loss 0.025877    Objective Loss 0.025877                                        LR 0.000008    Time 0.085578    
Epoch: [87][   70/  207]    Overall Loss 0.025558    Objective Loss 0.025558                                        LR 0.000008    Time 0.084050    
Epoch: [87][   80/  207]    Overall Loss 0.025208    Objective Loss 0.025208                                        LR 0.000008    Time 0.083211    
Epoch: [87][   90/  207]    Overall Loss 0.025599    Objective Loss 0.025599                                        LR 0.000008    Time 0.082670    
Epoch: [87][  100/  207]    Overall Loss 0.025828    Objective Loss 0.025828                                        LR 0.000008    Time 0.082334    
Epoch: [87][  110/  207]    Overall Loss 0.025834    Objective Loss 0.025834                                        LR 0.000008    Time 0.082121    
Epoch: [87][  120/  207]    Overall Loss 0.025852    Objective Loss 0.025852                                        LR 0.000008    Time 0.081779    
Epoch: [87][  130/  207]    Overall Loss 0.026010    Objective Loss 0.026010                                        LR 0.000008    Time 0.081402    
Epoch: [87][  140/  207]    Overall Loss 0.025554    Objective Loss 0.025554                                        LR 0.000008    Time 0.081280    
Epoch: [87][  150/  207]    Overall Loss 0.025612    Objective Loss 0.025612                                        LR 0.000008    Time 0.080868    
Epoch: [87][  160/  207]    Overall Loss 0.025352    Objective Loss 0.025352                                        LR 0.000008    Time 0.080445    
Epoch: [87][  170/  207]    Overall Loss 0.025188    Objective Loss 0.025188                                        LR 0.000008    Time 0.080051    
Epoch: [87][  180/  207]    Overall Loss 0.025116    Objective Loss 0.025116                                        LR 0.000008    Time 0.079658    
Epoch: [87][  190/  207]    Overall Loss 0.025254    Objective Loss 0.025254                                        LR 0.000008    Time 0.079295    
Epoch: [87][  200/  207]    Overall Loss 0.025381    Objective Loss 0.025381                                        LR 0.000008    Time 0.079144    
Epoch: [87][  207/  207]    Overall Loss 0.025323    Objective Loss 0.025323    Top1 97.508494    Top5 100.000000    LR 0.000008    Time 0.078819    
--- validate (epoch=87)-----------
5136 samples (512 per mini-batch)
Epoch: [87][   10/   11]    Loss 0.595215    Top1 84.355469    Top5 99.687500    
Epoch: [87][   11/   11]    Loss 0.556442    Top1 84.365265    Top5 99.688474    
==> Top1: 84.365    Top5: 99.688    Loss: 0.556

==> Confusion:
[[270   2   3   2   2   2   3  16]
 [  1 262  22   1   0   1   0  13]
 [  1  17 261   1   0   5   0  15]
 [  0   4   0 743  49  10  10  21]
 [  2   0   0  42 776   7  26  26]
 [  9   4   8  13  21 778  11  50]
 [  2   0   0   5  26  13 764  27]
 [ 18  14  13  52  67 115  31 479]]

==> Best [Top1: 84.755   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 86]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [88][   10/  207]    Overall Loss 0.023881    Objective Loss 0.023881                                        LR 0.000008    Time 0.154686    
Epoch: [88][   20/  207]    Overall Loss 0.024535    Objective Loss 0.024535                                        LR 0.000008    Time 0.116232    
Epoch: [88][   30/  207]    Overall Loss 0.024916    Objective Loss 0.024916                                        LR 0.000008    Time 0.104779    
Epoch: [88][   40/  207]    Overall Loss 0.024470    Objective Loss 0.024470                                        LR 0.000008    Time 0.098254    
Epoch: [88][   50/  207]    Overall Loss 0.024691    Objective Loss 0.024691                                        LR 0.000008    Time 0.093866    
Epoch: [88][   60/  207]    Overall Loss 0.024961    Objective Loss 0.024961                                        LR 0.000008    Time 0.090452    
Epoch: [88][   70/  207]    Overall Loss 0.025391    Objective Loss 0.025391                                        LR 0.000008    Time 0.088298    
Epoch: [88][   80/  207]    Overall Loss 0.025179    Objective Loss 0.025179                                        LR 0.000008    Time 0.086537    
Epoch: [88][   90/  207]    Overall Loss 0.025271    Objective Loss 0.025271                                        LR 0.000008    Time 0.085234    
Epoch: [88][  100/  207]    Overall Loss 0.025307    Objective Loss 0.025307                                        LR 0.000008    Time 0.084193    
Epoch: [88][  110/  207]    Overall Loss 0.025442    Objective Loss 0.025442                                        LR 0.000008    Time 0.083139    
Epoch: [88][  120/  207]    Overall Loss 0.025483    Objective Loss 0.025483                                        LR 0.000008    Time 0.082753    
Epoch: [88][  130/  207]    Overall Loss 0.025292    Objective Loss 0.025292                                        LR 0.000008    Time 0.082401    
Epoch: [88][  140/  207]    Overall Loss 0.025172    Objective Loss 0.025172                                        LR 0.000008    Time 0.082190    
Epoch: [88][  150/  207]    Overall Loss 0.025115    Objective Loss 0.025115                                        LR 0.000008    Time 0.082001    
Epoch: [88][  160/  207]    Overall Loss 0.025248    Objective Loss 0.025248                                        LR 0.000008    Time 0.082100    
Epoch: [88][  170/  207]    Overall Loss 0.025272    Objective Loss 0.025272                                        LR 0.000008    Time 0.081868    
Epoch: [88][  180/  207]    Overall Loss 0.025053    Objective Loss 0.025053                                        LR 0.000008    Time 0.081600    
Epoch: [88][  190/  207]    Overall Loss 0.024943    Objective Loss 0.024943                                        LR 0.000008    Time 0.081183    
Epoch: [88][  200/  207]    Overall Loss 0.025171    Objective Loss 0.025171                                        LR 0.000008    Time 0.080799    
Epoch: [88][  207/  207]    Overall Loss 0.025171    Objective Loss 0.025171    Top1 97.848245    Top5 99.886750    LR 0.000008    Time 0.080399    
--- validate (epoch=88)-----------
5136 samples (512 per mini-batch)
Epoch: [88][   10/   11]    Loss 0.587006    Top1 84.472656    Top5 99.707031    
Epoch: [88][   11/   11]    Loss 0.557058    Top1 84.443146    Top5 99.707944    
==> Top1: 84.443    Top5: 99.708    Loss: 0.557

==> Confusion:
[[273   1   3   1   2   1   3  16]
 [  2 262  23   1   0   1   0  11]
 [  2  17 266   1   0   4   0  10]
 [  0   4   0 740  50  10  11  22]
 [  2   0   0  42 775   8  25  27]
 [ 10   5   9  10  19 785  10  46]
 [  2   0   0   5  26  14 764  26]
 [ 20  14  14  50  67 120  32 472]]

==> Best [Top1: 84.755   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 86]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar


Training epoch: 105843 samples (512 per mini-batch)
Epoch: [89][   10/  207]    Overall Loss 0.025458    Objective Loss 0.025458                                        LR 0.000008    Time 0.116204    
Epoch: [89][   20/  207]    Overall Loss 0.025354    Objective Loss 0.025354                                        LR 0.000008    Time 0.094704    
Epoch: [89][   30/  207]    Overall Loss 0.025177    Objective Loss 0.025177                                        LR 0.000008    Time 0.088636    
Epoch: [89][   40/  207]    Overall Loss 0.025512    Objective Loss 0.025512                                        LR 0.000008    Time 0.086542    
Epoch: [89][   50/  207]    Overall Loss 0.026218    Objective Loss 0.026218                                        LR 0.000008    Time 0.085626    
Epoch: [89][   60/  207]    Overall Loss 0.025744    Objective Loss 0.025744                                        LR 0.000008    Time 0.084365    
Epoch: [89][   70/  207]    Overall Loss 0.025015    Objective Loss 0.025015                                        LR 0.000008    Time 0.083550    
Epoch: [89][   80/  207]    Overall Loss 0.025075    Objective Loss 0.025075                                        LR 0.000008    Time 0.082936    
Epoch: [89][   90/  207]    Overall Loss 0.025258    Objective Loss 0.025258                                        LR 0.000008    Time 0.082254    
Epoch: [89][  100/  207]    Overall Loss 0.025757    Objective Loss 0.025757                                        LR 0.000008    Time 0.081360    
Epoch: [89][  110/  207]    Overall Loss 0.025403    Objective Loss 0.025403                                        LR 0.000008    Time 0.080681    
Epoch: [89][  120/  207]    Overall Loss 0.025248    Objective Loss 0.025248                                        LR 0.000008    Time 0.079944    
Epoch: [89][  130/  207]    Overall Loss 0.025224    Objective Loss 0.025224                                        LR 0.000008    Time 0.079422    
Epoch: [89][  140/  207]    Overall Loss 0.025541    Objective Loss 0.025541                                        LR 0.000008    Time 0.078919    
Epoch: [89][  150/  207]    Overall Loss 0.025339    Objective Loss 0.025339                                        LR 0.000008    Time 0.078582    
Epoch: [89][  160/  207]    Overall Loss 0.025390    Objective Loss 0.025390                                        LR 0.000008    Time 0.078373    
Epoch: [89][  170/  207]    Overall Loss 0.025341    Objective Loss 0.025341                                        LR 0.000008    Time 0.078413    
Epoch: [89][  180/  207]    Overall Loss 0.025377    Objective Loss 0.025377                                        LR 0.000008    Time 0.078490    
Epoch: [89][  190/  207]    Overall Loss 0.025340    Objective Loss 0.025340                                        LR 0.000008    Time 0.078471    
Epoch: [89][  200/  207]    Overall Loss 0.025249    Objective Loss 0.025249                                        LR 0.000008    Time 0.078503    
Epoch: [89][  207/  207]    Overall Loss 0.025270    Objective Loss 0.025270    Top1 97.508494    Top5 99.886750    LR 0.000008    Time 0.078351    
--- validate (epoch=89)-----------
5136 samples (512 per mini-batch)
Epoch: [89][   10/   11]    Loss 0.586288    Top1 84.375000    Top5 99.687500    
Epoch: [89][   11/   11]    Loss 0.599651    Top1 84.384735    Top5 99.688474    
==> Top1: 84.385    Top5: 99.688    Loss: 0.600

==> Confusion:
[[272   2   2   2   2   1   3  16]
 [  3 262  23   0   0   1   1  10]
 [  1  19 262   1   0   6   0  11]
 [  0   4   0 740  52  10  10  21]
 [  2   0   0  39 778   8  24  28]
 [  8   5   8  11  20 785  11  46]
 [  2   0   0   5  27  14 759  30]
 [ 19  14  14  46  67 122  31 476]]

==> Best [Top1: 84.755   Top5: 99.688   Sparsity:0.00   Params: 117200 on epoch: 86]
Saving checkpoint to: logs/2024.01.15-145725/qat_checkpoint.pth.tar
--- test ---------------------
5631 samples (512 per mini-batch)
Test: [   10/   11]    Loss 0.576787    Top1 84.843750    Top5 99.667969    
Test: [   11/   11]    Loss 0.578146    Top1 84.816196    Top5 99.680341    
==> Top1: 84.816    Top5: 99.680    Loss: 0.578

==> Confusion:
[[276   2   1   0   0   2   2  17]
 [  3 251  30   1   0   0   0  15]
 [  1  24 263   0   1   3   0   8]
 [  1   2   1 822  72  13  11  20]
 [  5   0   0  40 843  15  20  46]
 [  6   1  13   4   5 803  25  46]
 [  0   0   0   2  26  36 887  27]
 [ 14  20  13  40  50 131  40 631]]


Log file for this run: /mnt/c/Users/Daniel/Github/ai8x-training/logs/2024.01.15-145725/2024.01.15-145725.log
